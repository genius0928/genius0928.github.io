<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://genius0928.github.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://genius0928.github.io//" rel="alternate" type="text/html" /><updated>2023-11-27T22:42:34+09:00</updated><id>https://genius0928.github.io//feed.xml</id><title type="html">천각블로그</title><subtitle>천각블로그에서 발표한 게시글을 소개합니다.</subtitle><author><name>천각</name></author><entry><title type="html">Node.js와 MongoDB의 프로젝트팁</title><link href="https://genius0928.github.io//papers/%EB%B8%94%EB%A1%9C%EA%B7%B8" rel="alternate" type="text/html" title="Node.js와 MongoDB의 프로젝트팁" /><published>2023-11-27T00:00:00+09:00</published><updated>2023-11-27T00:00:00+09:00</updated><id>https://genius0928.github.io//papers/%EB%B8%94%EB%A1%9C%EA%B7%B8</id><content type="html" xml:base="https://genius0928.github.io//papers/%EB%B8%94%EB%A1%9C%EA%B7%B8"><![CDATA[<h1 id="서론">서론</h1>

<p>MongoDB와 Node.js를 이용해서 서비스를 만들 때 코드리팩토링은 반드시 필요하다. 그렇다면 어떻게 해야할까?  요새 주먹구구식으로 만들다가 열심히 리팩토링 중인데 죽겠다 ㅠㅠ<br />
<img src="/assets/img/001.png" alt="Alt text" /></p>
<h1 id="본론">본론</h1>
<p>오류의 연속.. ㅠ</p>
<blockquote>
  <ol>
    <li>코드 모듈화
모델, 컨트롤러, 유틸 등 폴더기반 모듈화를 해야 함.  라우터 분리</li>
  </ol>
</blockquote>]]></content><author><name>주홍철:어비스</name></author><category term="papers" /><summary type="html"><![CDATA[서론]]></summary></entry><entry><title type="html">텍스트 요약 모델 성능 평가를 위한 새로운 척도, RDASS를 소개합니다.</title><link href="https://genius0928.github.io//deepdive/210729" rel="alternate" type="text/html" title="텍스트 요약 모델 성능 평가를 위한 새로운 척도, RDASS를 소개합니다." /><published>2021-07-29T00:00:00+09:00</published><updated>2021-07-29T00:00:00+09:00</updated><id>https://genius0928.github.io//deepdive/210729</id><content type="html" xml:base="https://genius0928.github.io//deepdive/210729"><![CDATA[<p>인터넷 뉴스, 블로그, 카페, SNS 등을 통해 매일같이 홍수처럼 쏟아지는 정보로 인해 현대인은 자신이 원하는 정보를 찾는 데 큰 어려움을 느끼고 있습니다. 이에 따라 선별된 양질의 정보에 대한 사용자 요구가 점차 늘어나면서, 이를 충족하기 위한 다양한 큐레이션 서비스가 전보다 더 크게 주목받고 있습니다. IT업계 실무 담당자가 읽은 양질의 콘텐츠를 요약 정리하는 플랫폼인 <a href="https://careerly.co.kr/">커리어리</a>나 디지털 콘텐츠와 콘텐츠 비즈니스 업계에 관한 최신의 뉴스와 도서 내용을 큐레이션하는 뉴스레터 <a href="https://page.stibee.com/subscriptions/50103">프로젝트 썸원</a> 등이 대표적이죠.</p>

<p>하지만 새롭게 생성되는 콘텐츠의 양은 사람이 통제할 수 없을 정도로 매우 방대합니다. 대단히 많은 문서를 빠르고 효과적으로 처리하기 위한 목적으로 본문의 핵심 내용을 자동으로 요약해주는 기술의 중요성이 점차 강조되는 건 바로 이 때문입니다. 관련 자연어처리 연구 과제로는 원본 문서를 참고해 짧은 분량의 텍스트를 생성하는 텍스트 요약<sup>text summarization</sup>이 있습니다.</p>

<p>텍스트 요약 모델의 작동 방식은 크게 2가지로 나눠볼 수 있습니다. 1)문서에서 뽑은 단어를 조합해 문장을 생성하는 추출요약(extraction)과 2)의미가 바뀌지 않은 선에서 문서에서 쓰이지 않은 단어 또는 표현을 이용해 문장을 만들어내는 생성요약(abstraction), 이렇게 말이죠<a href="#6725:rf:1" class="reference" id="6725:rf-back:1">[1]</a>. 추출요약 방식으로 선별된 단어가 문서를 충분히 대표하지 못하거나, 선별된 단어 간의 호응이 떨어지는 문제를 해결하기 위해 나온 생성요약 쪽의 연구가 현재는 더 활발하게 이뤄지고 있습니다.</p>

<p>하지만 최신의 요약 모델은 과거 추출요약의 정확도조차 달성하지 못하고 있습니다. 더 나은 성능의 요약 모델을 만들려면 모델로부터 자동으로 생성된 요약문을 어느 정도로 신뢰할 수 있는지 판별하기 위한 적절한 평가 방법이 있어야 합니다. 문제는 가장 보편적으로 쓰이는 성능 평가 척도가 모델의 성능을 제대로 평가하지 못한다는 거죠. 모델의 성능을 제대로 평가할 수 있어야 모델 성능 개선을 위한 다양한 시도를 할 수 있다는 점에서 새로운 평가 척도를 만들려는 연구 또한 자연어처리 분야에서 중요하게 다뤄지고 있습니다.</p>

<p>이러한 연구 흐름에 발맞춰 카카오(이동엽)와 카카오엔터프라이즈 (신명철, 조승우, 고병일, 이다니엘, 김응균), 고려대학교 (황태선), 한신대학교(조재춘)는 텍스트 요약 모델의 성능을 평가할 새로운 척도인 RDASS<sup>Reference and Document Aware Semantic Score</sup><a href="#6725:rf:2" class="reference" id="6725:rf-back:2">[2]</a>를 공동 제안했습니다. 이 성과를 정리한 논문이 COLING<sup>International Conference on Computational Linguistics</sup><sup><a href="#6725:fn:1" class="footnote" id="6725:fn-back:1">1</a></sup>에 게재 승인되기도 했습니다.</p>

<p>이번 글에서는 기존 성능 평가 척도인 ROUGE의 한계와 새롭게 제안한 RDASS의 계산 과정, RDASS의 타당성을 검증하는 실험 결과에 대해 상세히 다뤄보고자 합니다. 논문 제1저자인 카카오 소속의 이동엽 연구원을 만나서 자세한 이야기를 들어봤습니다.</p>

<p><br /></p>

<p class="notice">※카카오엔터프라이즈는 인공지능 기술의 공동 연구를 위해 공동체(카카오의 계열사 및 자회사를 이르는 말)와 다자간 업무 협약(MOU)을 체결한 바 있습니다. 이 연구는 이런 긴밀한 협력하에 이뤄진 카카오엔터프라이즈와 카카오 양사의 공동 연구임을 다시 한번 밝힙니다.
</p>

<p class="notice">※신명철 연구원은 기존 연구의 문제점 고찰과 이를 해결할 새로운 아이디어의 고안을 함께했습니다. 이후 실험 및 논문 작성은 이동엽 연구원이 맡았습니다. 황태선 연구원, 조승우 연구원은 논문에 삽입되는 표, 그림, 성능 평가 작업을 지원했습니다. 고병일 연구원은 S-BERT를 훈련했으며, 이다니엘 연구원, 김응균 연구원, 조재춘 교수가 논문을 검토했습니다.
</p>

<p><br /></p>

<p class="dot-line">∙  ∙  ∙  ∙  ∙  ∙  ∙</p>

<p><br /></p>

<h1 id="선행-연구">선행 연구</h1>

<p>자연어처리 모델의 성능은 사람이 문장을 직접 읽고 산출한 점수(manual)나 알고리즘으로 계산한 값(automatic)으로 비교해볼 수 있습니다. 많은 연구에서는 상대적으로 저렴한 비용에 빠르게 성능을 비교할 수 있다는 이유로 자동화 방식을 선호합니다. 문서 요약 분야에서도 관련 전문가가 일일이 검토하는 대신, 자동화 방식으로 모델의 성능을 평가하는 방법이 제안됐습니다.</p>

<p>자동화 방식 중 가장 널리 쓰이는 ROUGE<a href="#6725:rf:3" class="reference" id="6725:rf-back:3">[3]</a>는 전문가가 만들어 놓은 정답 요약문과 모델이 생성한 문장의 유사도를 비교합니다. 이를 위해 두 문장 간에 n-gram<sup><a href="#6725:fn:2" class="footnote" id="6725:fn-back:2">2</a></sup>이 <a href="http://www.ccs.neu.edu/home/vip/teach/DMcourse/5_topicmodel_summ/notes_slides/What-is-ROUGE.pdf">얼마나 많이 겹치는지를 계산</a>합니다. 일반적으로 0~1 사이의 숫자나, 여기에 100을 곱해준 값을 이용해 유사도를 나타내며, 이 숫자는 두 문장의 철자의 일치하는 정도를 나타냅니다. 따라서 숫자가 클수록 모델의 성능이 좋다고 보면 됩니다.</p>

<p>하지만 인간이 실제로 문장을 요약하는 방식을 생각해 본다면 ROGUE의 한계는 너무나 명확합니다. 요약의 핵심은 1)문서에서 핵심 내용을 잘 선택했는가(content selection), 2)문서에 쓰이지 않은 다른 표현이지만 같은 의미를 잘 나타내는가(paraphrase)입니다. 하지만 ROGUE는 주어진 두 문장의 형태학적 유사도만 높다면 의미적 유사도가 낮더라도 더 높은 점수를, 반대의 상황에서는 더 낮은 점수를 부여하며 위 2가지 핵심을 모두 놓칩니다.</p>

<p>좀 더 쉽게 설명해보겠습니다. 축구선수 리오넬 메시의 생일날 일정을 소개한 기사를 요약한다고 했을 때([그림 1]), ROGUE는 모델이 생성한 문장이 정답 문장과 유사도가 매우 낮다고 평가합니다. 두 문장에서 하늘색으로 강조 표시한 부분의 의미적 유사성은 매우 높지만, 철자의 일치도가 낮아 이런 점수가 나왔습니다. 심지어 모델이 생성한 문장이 문서에 등장한 표현(하늘색)과 의미적, 형태학적 유사도가 상대적으로 더 높으나 ROUGE는 이 부분을 간과합니다. 이는 생성 문장이 문서의 내용을 얼마나 잘 표현했는지를 계산하는 부분이 반영돼 있지 않아서 발생하는 문제입니다.</p>

<p><img src="/assets/img/2021-07-29-210729/001.png" width="85%" align="" /></p>

<p><em class="center">[ 그림 1 ] ROGUE는 두 대상 간의 형태학적 유사성만을 평가 기준으로 삼는다.</em></p>

<p>ROGUE의 한계는 한국어 문서 요약 태스크에서 더 도드라져 보입니다. 어근<sup><a href="#6725:fn:3" class="footnote" id="6725:fn-back:3">3</a></sup>에 붙은 다양한 접사<sup><a href="#6725:fn:4" class="footnote" id="6725:fn-back:4">4</a></sup>가 단어의 역할(의미, 문법적 기능)을 결정하는 언어적 특성을 갖춘 한국어에서는 복잡한 단어 변형이 빈번하게 일어나기 때문이죠. [그림 2]는 드라마 ‘슬기로운 의사생활’ 기사를 요약한 정답 문장과 모델이 생성한 요약 문장을 비교한 예시입니다. ROGUE는 정답 문장과 비교해 철자가 겨우 3개만 다른 오답 문장에 더 높은 점수를 부여합니다.</p>

<p><img src="/assets/img/2021-07-29-210729/002.png" width="95%" align="" /></p>

<p><em class="center">[ 그림 2 ] ROGUE는 특히 한국어 요약 모델 성능 평가에 취약하다.</em></p>

<p>이런 한계를 극복하기 위해 ParaEval<a href="#6725:rf:4" class="reference" id="6725:rf-back:4">[4]</a>, <a href="https://github.com/ng-j-p/rouge-we">ROUGE-WE</a><a href="#6725:rf:5" class="reference" id="6725:rf-back:5">[5]</a>, <a href="https://github.com/kavgan/ROUGE-2.0">ROGUE2.0</a><a href="#6725:rf:6" class="reference" id="6725:rf-back:6">[6]</a> 등의 척도가 새롭게 제안됐습니다. 하지만 이 방법론을 적용하려면 사람이 직접 방대한 크기의 동의어(유의어) 사전을 따로 구축해야 한다는 제약이 따릅니다.</p>

<p>또 한편, 공동 연구팀은 사람마다 요약 방식이 달라 하나의 정답 요약문만을 기준으로 요약 모델을 평가하는 데에는 한계가 있다고 판단했습니다. 이에 요약 모델이 문서의 내용을 얼마나 잘 반영하는지를 참고해 성능을 평가할 필요가 있다고 봤습니다.</p>

<p>공동 연구팀은 1)의미적 유사도는 높으나 형태학적 유사도가 낮은 문장을 생성하는 모델의 성능이 낮다고 판단하는 문제를 해결하기 위해 정답 문장과 생성 문장의 유사도, 문서와 생성 문장 간의 유사도를 모두 참고하며, 2)방대한 유의어 사전을 따로 구축할 필요가 없는 새로운 평가 척도인 RDASS를 고안했습니다.</p>

<p><br /></p>

<h1 id="rdass의-작동-과정">RDASS의 작동 과정</h1>

<h2 id="1문장-임베딩-모델의-사전학습과-미세조정">1.문장 임베딩 모델의 사전학습과 미세조정</h2>

<p>기계가 생성한 문장과 정답 문장의 유사도 계산을 위해서는, 먼저 각 문장을 고정된 크기의 벡터로 전환해야 합니다. 이 전환 기법을 문장 임베딩<sup>sentence embedding</sup>이라고 부릅니다. 문장을 구성하는 각 단어를 BERT로 출력한 결과값의 평균, 또는 모든 단어의 의미를 응축한 특별 분류 토큰[CLS<sup>special classification token</sup>]의 출력값 문장 임베딩으로 활용하는 방식이 보편적입니다.</p>

<p>하지만 이렇게 생성한 문장 임베딩이 요약 모델이 안정적인 성능을 내는 데 항상 유효하지는 않습니다. 입력 질의<sup>query</sup>와 유사한 문서를 찾아주는 검색<sup>retrieval</sup>과 같은 태스크에서 GLOVE<a href="#6725:rf:7" class="reference" id="6725:rf-back:7">[7]</a><sup><a href="#6725:fn:5" class="footnote" id="6725:fn-back:5">5</a></sup>로 생성한 단어별 임베딩 벡터의 평균값보다 되려 더 낮은 성능을 낼 수 있다는 연구 결과가 이를 뒷받침하죠<sup><a href="#6725:fn:6" class="footnote" id="6725:fn-back:6">6</a></sup>. 이에 현재는 BERT보다 문장의 의미를 더 잘 추출하는 Sentence-BERT(이하 SBERT)<a href="#6725:rf:8" class="reference" id="6725:rf-back:8">[8]</a>의 출력값을 문장 임베딩으로 많이 활용합니다.</p>

<p>공동 연구팀은 병렬 말뭉치로 구성된 NLI<sup>Natural language Inference</sup><sup><a href="#6725:fn:7" class="footnote" id="6725:fn-back:7">7</a></sup>와 STS<sup>Semantic Textual Similarity</sup><sup><a href="#6725:fn:8" class="footnote" id="6725:fn-back:8">8</a></sup> 벤치마크 데이터셋을 이용해 SBERT를 사전훈련했습니다. 그러고 나서, 정답 문장과 생성 문장의 유사도와 문서와 생성 문장 간의 유사도를 모두 고려할 수 있도록 사전훈련된 SBERT(이하 P-SBERT)와 요약 모델을 동시에 미세조정<sup>fine-tuning</sup>했습니다. 학습에 이용되는 트리플렛 손실 함수<sup>triplet loss function</sup>는 범주가 같은 두 문장 벡터와 범주가 다른 두 문장 벡터 간의 상대적인 관계를 고려하는데, 이 과정을 좀 더 자세히 설명하면 다음과 같습니다.</p>

<ol>
  <li>요약 모델이 생성한 문장의 벡터(H<sub>p</sub>), P-SBERT로 추출한 정답 문장 벡터(V<sub>p</sub><sup>r</sup>), P-SBERT로 추출한 임의의 오답 문장<sup><a href="#6725:fn:9" class="footnote" id="6725:fn-back:9">9</a></sup> 벡터(V<sub>n</sub><sup>r</sup>)로 구성된 트리플렛을 구성합니다.</li>
  <li>모델이 생성한 요약 문장과 정답 문장간의 거리인 d(H<sub>p</sub>,V<sub>p</sub><sup>r</sup>)와 모델이 요약한 문장과 오답 문장의 거리 d(H<sub>p</sub>,V<sub>n</sub><sup>r</sup>)를 계산합니다.</li>
  <li>트리플렛 손실 함수는 d(H<sub>p</sub>,V<sub>p</sub><sup>r</sup>)가 0에 수렴하도록, d(H<sub>p</sub>,V<sub>n</sub><sup>r</sup>)는 1에 수렴하도록, 동시에 그 상대적인 거리 차(d(H<sub>p</sub>,V<sub>n</sub><sup>r</sup>)-d(H<sub>p</sub>,V<sub>p</sub><sup>r</sup>))가 임의로 정한 마진값보다는 커지도록 합니다.</li>
</ol>

<p>결과적으로 P-SBERT와 요약 모델은 미세조정에 쓰이는 트리플렛 손실 함수값을 최소화하는 방향으로 훈련되는 거라 볼 수 있겠습니다. 정답 문장과 관련이 있는 문서와 그렇지 않은 문서를 S-BERT로 추출한 벡터로도 마찬가지의 학습을 진행합니다. 생성 요약모델과 SBERT를 함께 미세조정한 효과는 뒤의 성능 비교 실험 결과에서 이어서 설명하겠습니다.</p>

<h2 id="2문서와-정답-문장-생성-문장-모두의-관계를-고려한-유사도-계산">2.문서와 정답 문장, 생성 문장 모두의 관계를 고려한 유사도 계산</h2>

<p>RDASS는 &lt;본문, 정답 요약 문장, 예측 요약 문장&gt; 세 개의 관계를 동시에 고려하도록 고안됐습니다. RDASS도 0~1 사이의 숫자 값을 가지며 숫자가 높을수록 더 높은 성능을 가리킵니다. 계산 과정([그림 3])은 다음과 같습니다.</p>

<p>사전훈련과 미세조정을 모두 마친 버전인 FWA-SBERT를 이용해 문서(d), 정답 요약 문장(r), 모델이 생성한 요약 문장(p) 각각의 벡터 V<sub>d</sub>, V<sub>p</sub>, V<sub>r</sub>를 추출합니다. 그런 뒤, &lt; V<sub>p</sub>, V<sub>r</sub> &gt;와 &lt; V<sub>p</sub>, V<sub>d</sub> &gt; 각각의 코사인 유사도<sup>cosine similarity</sup><sup><a href="#6725:fn:10" class="footnote" id="6725:fn-back:10">10</a></sup>인 s(p, r), s(p, d)를 계산합니다. 마지막으로, 두 유사도의 평균값을 냅니다.</p>

<p><img src="/assets/img/2021-07-29-210729/003.png" width="50%" align="" /></p>

<p><em class="center">[ 그림 3 ] &lt;본문, 정답 문장, 예측 문장&gt; 세 개의 관계를 동시에 고려하는 평가 척도 RDASS의 계산 과정</em></p>

<p><br /></p>

<h1 id="실험-환경">실험 환경</h1>

<p>공동 연구팀은 사전학습된 BERT를 인코더로, Transformer를 디코더로 구현하고 여러 태스크<sup><a href="#6725:fn:11" class="footnote" id="6725:fn-back:11">11</a></sup>에서 SOTA(현재 최고 수준의) 성능을 달성한 BERTSUMABS<sup><a href="#6725:fn:12" class="footnote" id="6725:fn-back:12">12</a></sup>를 생성 요약 모델로 사용했습니다.</p>

<p>문장 임베딩 모델로 채택한 SBERT의 훈련을 위해서는 BERT의 사전학습이 필요했습니다. SBERT가 BERT의 출력값을 활용하는 구조로 이뤄져 있기 때문이죠. 다만 한글 데이터셋으로 사전학습된 BERT를 시중에서 구할 수 없었던 공동 연구팀은 160만 개의 문서에서 추출한 2,300만 개 문장의 한국어 데이터셋으로 BERT를 직접 사전학습했습니다. 그러고 나서 카카오브레인에서 최근 공개한 한국어 NLI 및 STS 데이터셋<a href="#6725:rf:9" class="reference" id="6725:rf-back:9">[9]</a><sup><a href="#6725:fn:13" class="footnote" id="6725:fn-back:13">13</a></sup>으로 SBERT를 따로 사전훈련했습니다. P-SBERT의 미세조정과 검증에는 카카오엔터프라이즈가 자체 수집한 정치, 경제, 문화 등 10개 주제와 관련된 뉴스(문서)와 각 뉴스를 요약한 문장을 한 쌍으로 구성해 만든 300만 개의 병렬 데이터셋을 이용했습니다.</p>

<p>공동 연구팀은 &lt;문서, 정답 요약 문장, 예측 요약 문장&gt;으로 구성된 200개의 데이터를 대상으로 ROGUE와 RDASS의 평가 정확도를 비교하는 실험을 진행했습니다. 이들 평가 척도의 객관성을 확보하고자 사람 평가와 얼마나 상관성이 있는지도 측정했습니다. 사람 평가에서는 예측된 요약문이 본문과 얼마나 관련도가 높은지(relevance), 사실 정보에 입각하는지(consistency)<sup><a href="#6725:fn:14" class="footnote" id="6725:fn-back:14">14</a></sup>, 문장의 완성도가 높은지(fluency) 등 세 가지 항목을 두고, 모든 사람의 점수(1~5점) 평균값을 각 항목의 점수로 활용했습니다.</p>

<p>인간 평가와 자동 평가의 상관성<sup>correlation</sup> 측정에는 보편적인 기법인 피어슨<sup>Pearson</sup> 상관계수, 켄달<sup>Kendall</sup> (순위) 상관계수를 이용했습니다. 두 변수(RDASS, 사람 평가)의 상관관계를 수치로 계량화한 두 상관계수는 1일수록 양의 상관관계<sup><a href="#6725:fn:15" class="footnote" id="6725:fn-back:15">15</a></sup>를, -1일수록 음의 상관관계<sup><a href="#6725:fn:16" class="footnote" id="6725:fn-back:16">16</a></sup>를, 0이면 상관관계가 없음을 나타냅니다.</p>

<p><br /></p>

<h1 id="연구-결과">연구 결과</h1>

<p>실험 결과, 공동 연구팀은 새롭게 제안한 평가 척도인 RDASS가 사람 평가와의 상관성이 ROGUE보다 더 크다는 점을 실험적으로 증명했습니다. 이는 RDASS가 요약 모델이 생성한 문장과 실제 정답 문장 간 유사성은 물론, 문서와의 유사성도 함께 파악한 구조 덕분으로 분석됩니다. 사람의 평가 결과와 유사한 품질의 평가 척도인지를 알기 위해서는 좀 더 추가적인 실험 진행되어야 할 거로 보입니다.</p>

<p><img src="/assets/img/2021-07-29-210729/004.png" width="70%" align="" /></p>

<p><em class="center">[ 그림 4 ] RDASS와 사람 평가 방식의 상관관계 측정<em></em></em></p>

<p><img src="/assets/img/2021-07-29-210729/005.png" width="" align="" /></p>

<p><em class="center">[ 그림 5 ] RDASS가 ROGUE보다 사람 평가와 상관성이 더 크다는 결과가 나왔다.</em></p>

<p>생성요약 모델과 SBERT를 함께 미세조정한 효과는 [표 1]에서 확인할 수 있습니다. P-SBERT만을 이용해 RDASS를 계산한 결과보다는, FWA-SBERT를 이용해 RDASS를 계산한 결과값이 사람 평가와 일치도가 더 높습니다.</p>

<p><img src="/assets/img/2021-07-29-210729/006.png" width="80%" align="" /></p>

<p><em class="center">[ 표 1 ]생성요약 모델과 SBERT를 함께 미세조정하면 RDASS와 사람 평가의 일치도(유사성)가 상대적으로 더 높다.</em></p>

<p><br /></p>

<h1 id="향후-계획">향후 계획</h1>

<p>요즘에는 기계가 생성한 요약문의 참 또는 거짓을 판단하는 요소를 반영하는 연구 분야에서 성과가 나고 있습니다<sup><a href="#6725:fn:17" class="footnote" id="6725:fn-back:17">17</a></sup>. 지문의 개체<sup>entity</sup>, 전치사 등의 항목에 오류를 주고 이를 복구하는 방법을 배우는 사후 교정 방식<sup>post-editing</sup>의 요약 교정 모델<a href="#6725:rf:10" class="reference" id="6725:rf-back:10">[10]</a>과 모델이 예측한 요약문으로 질문지를 생성한 뒤 정답을 예측 요약문에서 찾은 점수와 본문에서 찾은 점수를 비교해서 생성된 요약문의 사실성을 판단하는 평가 척도<a href="#6725:rf:11" class="reference" id="6725:rf-back:11">[11]</a>가 새롭게 제안되기도 했습니다. 공동 연구팀 또한 최근 연구 변화 흐름에 따라 참/거짓 오류를 줄여서 사실에 입각한 요약 문장을 생성하는 모델을 연구∙개발할 계획입니다.</p>

<p>이동엽 카카오 연구원은 “모델의 성능 평가 척도는 더 나은 요약 모델 개발에 매우 중요한 요소 중 하나”라며 “영어 텍스트 요약 태스크에서의 추가 검증 등 RDASS의 상대적 효용성을 입증하는 실험도 추가할 예정”이라고 말했습니다.</p>

<p><br /></p>

<h1 id="참고문헌">참고문헌</h1>

<p><a id="6725:rf:1" class="referencebody"><a href="#6725:rf-back:1" class="backlink">[1]</a>  <a href="http://koreascience.or.kr/article/JAKO202014264110274.pdf">한국어 기술문서 분석을 위한 BERT 기반의 분류모델</a> (2020) by 황상흠, 김도현, in 한국전자거래학회지 제25권 제1호</a><br /></p>

<p><a id="6725:rf:2" class="referencebody"><a href="#6725:rf-back:2" class="backlink">[2]</a>  <a href="https://www.aclweb.org/anthology/2020.coling-main.491.pdf">Reference and Document Aware Semantic Evaluation Methods for Korean Language Summarization</a> (2020) by Dongyub Lee, Myeong Cheol Shin, Taesun Whang, Seungwoo Cho, Byeongil Ko, Daniel Lee, EungGyun Kim, Jaechoon Jo in COLING</a><br /></p>

<p><a id="6725:rf:3" class="referencebody"><a href="#6725:rf-back:3" class="backlink">[3]</a>  <a href="https://www.aclweb.org/anthology/W04-1013.pdf">ROUGE: A Package for Automatic Evaluation of Summaries</a> (2004) by Chin-Yew Lin in ACL</a><br /></p>

<p><a id="6725:rf:4" class="referencebody"><a href="#6725:rf-back:4" class="backlink">[4]</a>  <a href="https://www.aclweb.org/anthology/N06-1057.pdf">ParaEval: Using Paraphrases to Evaluate Summaries Automatically</a> (2006) by Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu, Eduard Hovy in ACL</a><br /></p>

<p><a id="6725:rf:5" class="referencebody"><a href="#6725:rf-back:5" class="backlink">[5]</a>  <a href="https://www.aclweb.org/anthology/D15-1222.pdf">Better Summarization Evaluation with Word Embeddings for ROUGE</a> (2015) by Jun-Ping Ng, Viktoria Abrecht in ACL</a><br /></p>

<p><a id="6725:rf:6" class="referencebody"><a href="#6725:rf-back:6" class="backlink">[6]</a>  <a href="https://arxiv.org/pdf/1803.01937.pdf">ROUGE 2.0: Updated and Improved Measures for Evaluation of Summarization Tasks</a> (2018) by Kavita Ganesan in ACL</a><br /></p>

<p><a id="6725:rf:7" class="referencebody"><a href="#6725:rf-back:7" class="backlink">[7]</a>  <a href="https://www.aclweb.org/anthology/D14-1162.pdf">GloVe: Global Vectors for Word Representation</a> (2014) by Jeffrey Pennington, Richard Socher, Christopher D. Manning in EMNLP</a><br /></p>

<p><a id="6725:rf:8" class="referencebody"><a href="#6725:rf-back:8" class="backlink">[8]</a>  <a href="https://www.aclweb.org/anthology/D19-1410.pdf">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a> (2019) by Nils Reimers,  Iryna Gurevych in EMNLP</a><br /></p>

<p><a id="6725:rf:9" class="referencebody"><a href="#6725:rf-back:9" class="backlink">[9]</a>  <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.39/">KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding</a> (2020) by Jiyeon Ham, Yo Joong Choe, Kyubyong Park, Ilji Choi, Hyungjoon Soh in EMNLP</a><br /></p>

<p><a id="6725:rf:10" class="referencebody"><a href="#6725:rf-back:10" class="backlink">[10]</a>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.506.pdf">Factual Error Correction for Abstractive Summarization Models</a> (2020) by Meng Cao, Yue Dong, Jiapeng Wu, Jackie Chi Kit Cheung in EMNLP</a><br /></p>

<p><a id="6725:rf:11" class="referencebody"><a href="#6725:rf-back:11" class="backlink">[11]</a>  <a href="https://www.aclweb.org/anthology/2020.acl-main.450.pdf">Asking and Answering Questions to Evaluate the Factual Consistency of Summaries</a> (2020) by Alex Wang, Kyunghyun Cho, Mike Lewis in ACL</a><br /></p>

<p><a id="6725:rf:12" class="referencebody"><a href="#6725:rf-back:12" class="backlink">[12]</a>  <a href="https://www.aclweb.org/anthology/D19-1387/">Text Summarization with Pretrained Encoders</a> (2019) by Yang Liu, Mirella Lapata in EMNLP</a><br /></p>

<p><br /></p>

<h1 id="각주">각주</h1>

<ol class="footnotelist">
<li id="6725:fn:1" class="footnotebody" value="1"><p> 자연어처리 및 언어학을 대표하는 국제 학회로, 2020년에는 제출된 2,319편의 논문 중 644개의 논문이 통과됐다. <a href="#6725:fn-back:1" class="backlink"> ↩</a></p></li>
<li id="6725:fn:2" class="footnotebody" value="2"><p>  문장을 특정 단위를 기준으로 N개씩 분할해서 얻은 결과물을 가리킨다. 이 단위는 형태소, 또는 어절(띄어쓰기 기준) 등이 될 수 있다. <a href="#6725:fn-back:2" class="backlink"> ↩</a></p></li>
<li id="6725:fn:3" class="footnotebody" value="3"><p> 중심이 되는 의미를 갖춘 실질 형태소를 가리킨다. <a href="#6725:fn-back:3" class="backlink"> ↩</a></p></li>
<li id="6725:fn:4" class="footnotebody" value="4"><p> 어근과 결합해 그 의미나 기능을 더해주는 형식 형태소를 가리킨다. 어근 앞에 붙으면 접두사, 어근 뒤에 붙으면 접미사라고 부른다. <a href="#6725:fn-back:4" class="backlink"> ↩</a></p></li>
<li id="6725:fn:5" class="footnotebody" value="5"><p> 2014년 미국 스탠포드대학교 연구팀이 개발한 단어 임베딩 기법 <a href="#6725:fn-back:5" class="backlink"> ↩</a></p></li>
<li id="6725:fn:6" class="footnotebody" value="6"><p> Sentence-BERT 논문에서 “this common practice yields rather bad sentence embeddings, often worse than averaging GloVe embeddings.”라고 기술한 바 있다. <a href="#6725:fn-back:6" class="backlink"> ↩</a></p></li>
<li id="6725:fn:7" class="footnotebody" value="7"><p> 주어진 두 문장의 관계를 '함의한다<sup>entailment</sup>', '모순적이다<sup>contradiction</sup>', '모른다<sup>neutral</sup>'  중 하나로 분류하는 태스크 <a href="#6725:fn-back:7" class="backlink"> ↩</a></p></li>
<li id="6725:fn:8" class="footnotebody" value="8"><p>  주어진 두 문장의 의미의 유사 여부를 판별하는 태스크 <a href="#6725:fn-back:8" class="backlink"> ↩</a></p></li>
<li id="6725:fn:9" class="footnotebody" value="9"><p>  &lt;문서, 요약 문장&gt;으로 구성된 N개의 데이터셋이 있다고 가정해보자. n(1~N)번째 데이터셋으로 미세조정한다고 했을 때, 임의의 오답 문장은 n번째 데이터셋을 제외한 임의의 데이터셋의 문장을 선택한다. <a href="#6725:fn-back:9" class="backlink"> ↩</a></p></li>
<li id="6725:fn:10" class="footnotebody" value="10"><p> 두 문장의 벡터 유사도를 구하는 방법에는 유클리디언 거리<sup>euclidean distance</sup>와 코사인 유사도가 있다. 유클리디안 거리는 N차원의 공간에서 두 점의 직선 거리를 의미하며, 코사인 유사도는 두 벡터간의 사이각 계산 방법을 의미한다. <a href="#6725:fn-back:10" class="backlink"> ↩</a></p></li>
<li id="6725:fn:11" class="footnotebody" value="11"><p>  Cable News Network/DailyMail, New York Times, XSum 데이터셋 기준 <a href="#6725:fn-back:11" class="backlink"> ↩</a></p></li>
<li id="6725:fn:12" class="footnotebody" value="12"><p>  파이썬<sup>python</sup>으로 구현한 코드인 <a href="https://github.com/nlpyang/PreSumm">PreSumm</a><a href="#6725:rf:12" class="reference" id="6725:rf-back:12">[12]</a>를 사용했다. <a href="#6725:fn-back:12" class="backlink"> ↩</a></p></li>
<li id="6725:fn:13" class="footnotebody" value="13"><p>  카카오 i 번역 엔진을 이용해 SBERT 훈련에 사용된 NLI 및 STS 데이터셋을 구성하는 영어 문장을 한국어 문장으로 번역했다. <a href="#6725:fn-back:13" class="backlink"> ↩</a></p></li>
<li id="6725:fn:14" class="footnotebody" value="14"><p>  여기서 말하는 사실성은 문서에 서술된 내용을 의미한다. <a href="#6725:fn-back:14" class="backlink"> ↩</a></p></li>
<li id="6725:fn:15" class="footnotebody" value="15"><p>  한쪽이 증가할 때 다른 한쪽도 증가함을 의미한다. <a href="#6725:fn-back:15" class="backlink"> ↩</a></p></li>
<li id="6725:fn:16" class="footnotebody" value="16"><p>  한쪽이 증가할 때 다른 쪽은 감소함을 의미한다. <a href="#6725:fn-back:16" class="backlink"> ↩</a></p></li>
<li id="6725:fn:17" class="footnotebody" value="17"><p>  문서 내용 또는 외부 지식을 이용해 그래프를 구축하고 이를 학습에 활용하는 연구도 진행되고 있다. <a href="#6725:fn-back:17" class="backlink"> ↩</a></p></li>

</ol>]]></content><author><name>samantha:작성,편집,디자인</name></author><category term="deepdive" /><category term="Text Summarization" /><summary type="html"><![CDATA[더 나은 성능의 요약 모델을 만들려면 모델로부터 자동으로 생성된 요약문을 어느 정도로 신뢰할 수 있는지 판별하기 위한 적절한 평가 방법이 있어야 합니다. 문제는 가장 보편적으로 쓰이는 성능 평가 척도가 모델의 성능을 제대로 평가하지 못한다는 거죠. 이를 해결하기 위해 카카오엔터프라이즈가 카카오, 고려대학교, 한신대학교와 연구팀을 꾸리고 관련 주제로 연구를 하게 됐습니다.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://genius0928.github.io//assets/img/2021-07-29-210729/000.png" /><media:content medium="image" url="https://genius0928.github.io//assets/img/2021-07-29-210729/000.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">텍스트 스타일을 바꾸는 딥러닝 기술</title><link href="https://genius0928.github.io//deepdive/210525" rel="alternate" type="text/html" title="텍스트 스타일을 바꾸는 딥러닝 기술" /><published>2021-05-25T00:00:00+09:00</published><updated>2021-05-25T00:00:00+09:00</updated><id>https://genius0928.github.io//deepdive/210525</id><content type="html" xml:base="https://genius0928.github.io//deepdive/210525"><![CDATA[<p>오늘날 많은 기업에서는 챗봇을 이용한 고객 응대 서비스도 제공하고 있습니다. 콜센터를 구축하는 비용보다 훨씬 더 저렴하고, 더 많은 고객을 동시에 응대할 수 있는 효용성을 갖춘 덕분입니다. 그 결과, 고객은 대기자가 많아 상담사 연결까지 한참을 기다리거나 수많은 상품 소개 메뉴를 찾아다닐 수고를 조금은 덜게 됐습니다.</p>

<p>보통 이런 챗봇은 매뉴얼에 따라 질문을 입력해야만 사용자가 원하는 답을 하도록 구현돼 있습니다. 자신이 가진 문제나 질문을 해결하려는 용도로 챗봇을 이용하는 사용자에게 ‘챗봇과의 교감적 대화’는 별로 중요한 기능이 아니기 때문입니다.</p>

<p>하지만 과중한 업무에 치이는 상담사의 완벽한 업무 파트너로 발전하기 위해서는 챗봇에 공감적 대화 능력이 요구될 거로 보입니다. 누군가와 대화를 하고 사회적인 관계를 맺는 데 핵심 역할을 하는 ‘감정’을 보여준다면 사용자는 대화할 맛을 느끼게 되겠죠. 앵무새같이 말하는 무심한 챗봇보다는, 내가 처한 상황에 적절한 감정적 반응’도’ 보이는 챗봇에 누구라도 더 큰 호의를 가질 수밖에 없으리라는 판단은 바로 이런 이유 때문입니다.</p>

<p>감정 교감형 챗봇을 만드는 첫걸음은 <a href="https://obs.yonsei.ac.kr/ibody/myhome/philosophy/ph03.htm">처지와 상황이 다른 두 제자의 똑같은 질문에 각기 다른 답을 내어준 공자</a>처럼<sup><a href="#254b:fn:1" class="footnote" id="254b:fn-back:1">1</a></sup>, 같은 질문이더라도 맥락과 상황, 또는 대화 대상자에 따라 다른 반응을 보이도록 설계하는 데 있습니다. 하지만 현재의 챗봇 시스템은 특정 문장에 항상 같은 반응을 보입니다. 모든 이에게 같은 답변을 내주는 일은 교장님 훈화 말씀처럼 소통이 쌍방향이 아닌 한방향으로만 진행된다면 사용자는 챗봇을 지루하게 느낄 수밖에 없을 겁니다.</p>

<p>이런 한계를 해결하고자 많은 곳에서는 챗봇의 답변 내용은 유지하되, 사용자의 나이나 인종, 성격 등 다양한 요소로 조합된 그룹을 대표하는 스타일의 문장을 생성하는 기술 개발에 힘쓰고 있습니다. 카카오엔터프라이즈 또한 이 주제에 관한 연구를 진행하고 있으며, 최근에는 INLG<sup>International Natural Language Generation Conference</sup>에 논문 ‘Stable Style Transformer: Delete and Generate Approach with Encoder-Decoder for Text Style Transfer<a href="#254b:rf:1" class="reference" id="254b:rf-back:1">[1]</a>’을 내기도 했습니다.</p>

<p>이번 시간에는 텍스트 스타일 변환에 관한 선행 연구와 카카오엔터프라이즈가 제안한 모델인 SST<sup>Stable Style Transformer</sup>의 작동 원리와 성능, 그리고 향후 연구 계획에 관한 내용을 상세히 다뤄보고자 합니다. 카카오엔터프라이즈 AI기술실 자연어처리팀 소속의 이주성 연구원을 만나 이야기를 들어봤습니다.</p>

<p><br /></p>

<p class="notice">※텍스트의 스타일을 변환하는 모델의 성능 평가에는 긍정문을 부정문으로, 또는 부정문을 긍정문으로 바꾸는 벤치마크 데이터셋이 주로 활용됩니다. 이에 따라 본문에서는 명료한 설명을 위해 스타일 속성값을 긍정 또는 부정으로만 바꾸는 태스크를 수행하는 모델로 한정했습니다.
</p>
<p class="notice">※반복적으로 등장하는 “스타일에 해당하는 토큰”, “내용에 해당하는 토큰”은 각각 “‘스타일’ 토큰”, “‘텍스트’ 토큰”으로 보다 간단하게 표현했습니다. 아울러 두 대상을 지칭할 때는 맥락에 따라 토큰 또는 텍스트로 적었습니다.
</p>
<p class="notice">※내용 이해를 돕기 위해 실제 모델 훈련에 쓰이는 영어 문장 대신 한글 문장을 예시로 들었으며, 모든 예문에서는 띄어쓰기를 기준으로 토큰을 나눴다고 가정했습니다.
</p>

<p><br /></p>

<p class="dot-line">∙  ∙  ∙  ∙  ∙  ∙  ∙</p>

<p><br /></p>

<h1 id="선행연구-소개">선행연구 소개</h1>

<p>텍스트 스타일 변환(TST<sup>Text Style Transfer</sup>)은 입력 문장의 내용<sup>content</sup>은 보존하고, 스타일<sup>style</sup><sup><a href="#254b:fn:2" class="footnote" id="254b:fn-back:2">2</a></sup>에 해당하는 텍스트를 바꾸는 태스크입니다. “그 레스토랑은 불친절해”라는 문장에서 내용에 해당하는 부분(‘그 레스토랑은’)은 그대로, 스타일에 해당하는 부분(불친절해)과는 반대되는 의미를 지닌 텍스트(‘친절해’)로 바꾸는 긍·부정 전환이 대표적인 예입니다. 최근에는 여러 스타일 속성을 동시에 변환하는 연구<a href="#254b:rf:2" class="reference" id="254b:rf-back:2">[2]</a>도 활발하게 진행되고 있습니다.</p>

<p><img src="/assets/img/2021-05-25-210525/001.png" width="" align="" /></p>

<p><em>[ 표 1 ] 텍스트 스타일 변환 예시 문장. (슬픔), (기쁨), (화남), (크리스마스)처럼 텍스트로 표현하는 이모지도 전환할 수 있다. (출처 : <a href="#254b:rf:2" class="reference" id="254b:rf-back:2">[2]</a>)</em></p>

<p>딥러닝 모델에 TST 태스크를 가르치기 위해서는 “그 음식 맛없어(부정)”와 “그 음식 맛있어(긍정)”처럼 한 속성에 대한 상반된 값이 쌍으로 존재하는 병렬 말뭉치<sup>parallel corpus</sup>를 이용한 감독학습<sup>supervised learning</sup>을 고려해볼 수 있습니다. 하지만 이런 형태의 병렬 말뭉치 구축에는 시간과 비용이 많이 들죠. 이에 많은 연구에서는 긍정 문장 또는 부정 문장으로만 구성된 단일 말뭉치를 이용한 비감독학습<sup>unsupervised learning</sup>을 시도합니다.</p>

<p>초기 연구에서는 적대적 학습<sup>adversarial learning</sup><sup><a href="#254b:fn:3" class="footnote" id="254b:fn-back:3">3</a></sup>을 통한 해석 가능한 표현<sup>disentangled latent representation</sup>을 생성하는 데 집중했습니다<a href="#254b:rf:3" class="reference" id="254b:rf-back:3">[3]</a>. 과정은 다음과 같습니다. 입력 문장이 긍정인지 부정인지를 구분하지 못할 때까지 학습을 반복하면, 생성 모델의 인코더<sup>encoder</sup>가 입력 문장에서 ‘스타일’ 토큰을 제외한 나머지를 보고 ‘내용’ 벡터를 효과적으로 표현하리라 가정했습니다([그림1]). 그러고 나면, 이 벡터와 타깃 스타일 속성값을 디코더<sup>decoder</sup>에 입력하면 사용자가 원하는 스타일의 문장이 생성할 수 있다고 본 거죠. 하지만 실제 실험에서는 적대적 학습을 통해 입력 문장에서 ‘내용’ 벡터만을 분리하기가 쉽지 않았습니다<a href="#254b:rf:4" class="reference" id="254b:rf-back:4">[4]</a>. 또한, 다양한 길이의 문장을 항상 같은 크기의 벡터로 변환하는 데에도 한계가 있음이 발견됐습니다.</p>

<p><img src="/assets/img/2021-05-25-210525/002.png" width="" align="" /></p>

<p><em class="center">[ 그림 1 ] 적대적 학습으로 TST 모델을 훈련하는 과정</em></p>

<p>이처럼 입력 문장을 보고 ‘내용’ 토큰만을 벡터로 표현하는 대신, ‘스타일’ 토큰을 아예 삭제하는 방법론<a href="#254b:rf:5" class="reference" id="254b:rf-back:5">[5]</a>이 제안됐습니다. 이를 위해 ‘스타일’ 토큰을 삭제하는 모듈과 나머지 문장 요소(내용)만을 가지고 타깃 스타일 속성값에 해당하는 문장을 생성하는 모듈을 각기 따로 둡니다. 이 방식은 앞서 설명한 방식보다 ‘내용’ 토큰을 더 잘 보존하고, ‘스타일’ 토큰은 더 효과적으로 바꿨습니다. 그러나 문장을 구성하는 모든 텍스트가 상호 작용하는 그 특성상, ‘스타일’ 토큰을 효과적으로 삭제하는 데 한계가 있었습니다<a href="#254b:rf:6" class="reference" id="254b:rf-back:6">[6]</a>. 이에 ‘스타일’ 토큰을 임의로 삭제하지 않고도 스타일을 변환하는 E2E<sup>end-to-end</sup> 방식이 제안되기도 했습니다.</p>

<p><br /></p>

<h1 id="sst가-문제를-해결한-방식">SST가 문제를 해결한 방식</h1>

<p>카카오엔터프라이즈는 입력 문장에서 ‘스타일’ 토큰을 명시적으로 삭제하고, ‘내용 토큰과 타깃 스타일 속성값만을 가지고 문장 벡터를 생성하는 접근 방식에서 영감을 얻었습니다. 카카오엔터프라이즈가 고안한 SST 모델 또한 1)문장에서 ‘스타일’ 토큰을 삭제한 뒤, 2)’콘텐츠’ 토큰만을 가지고 입력 문장과는 반대되는 스타일 속성값을 가진 문장을 생성합니다. 다만 기존과는 다른 방식으로 각 모듈을 구현했는데, 이는 각 단락에서 이어 하겠습니다.</p>

<p><img src="/assets/img/2021-05-25-210525/003.png" width="80%" align="" /></p>

<p><em class="center">[ 그림 2 ] 문장의 스타일 속성값을 긍정에서 부정, 또는 부정에서 긍정으로 전환해 문장을 생성하는 STT 모델의 동작 과정</em></p>

<p><br /></p>

<h2 id="1-스타일-삭제-프로세스">(1) 스타일 삭제 프로세스</h2>

<p>카카오엔터프라이즈가 제안한 스타일 삭제기의 동작 방식을 설명하는 데 앞서, 기존에 제안한 방식의 동작 원리와 그 한계에 대해 먼저 간단하게 짚어보겠습니다.</p>

<p>빈도비<sup>frequency-ratio</sup>를 활용한 방식은 삭제기에 입력된 문장의 토큰 중, 사전<sup><a href="#254b:fn:4" class="footnote" id="254b:fn-back:4">4</a></sup>에 등록된 토큰과 일치한다면 이를 ‘스타일’ 토큰으로 판단해 삭제합니다. 이 방식의 한계는 사전에 등록되지 않은 ‘스타일’ 토큰을 인식하지 못할 가능성이 매우 크다는 데 있습니다. 그렇다고 해서 이 세상에 존재하는 ‘스타일’ 토큰을 모조리 찾아 등록할 수도 없습니다. 규칙 기반으로 동작하는 그 특성상 맥락을 파악해야 어느 부분이 ‘스타일’ 토큰인지 알 수 있는 문장<sup><a href="#254b:fn:5" class="footnote" id="254b:fn-back:5">5</a></sup>에서는 잘 동작하지 않을 가능성도 높습니다.</p>

<p>이런 한계를 극복하고자 어텐션attention<sup><a href="#254b:fn:6" class="footnote" id="254b:fn-back:6">6</a></sup>을 활용해 스타일 토큰을 삭제하는 방식<a href="#254b:rf:7" class="reference" id="254b:rf-back:7">[7]</a>이 고안됐습니다. 과정은 다음과 같습니다. 먼저, BERT에 문장을 입력해 [CLS<sup>Special Classification token</sup>]<sup><a href="#254b:fn:7" class="footnote" id="254b:fn-back:7">7</a></sup>에 해당하는 벡터를 얻습니다. 그다음, 입력 문장의 각 토큰이 CLS 디코딩에 영향을 미친 정도(어텐션)를 점수로 환산합니다. 가장 높은 점수를 낸 토큰을 문장을 긍·부정으로 분류하는 데 영향을 미친 ‘스타일’ 토큰이라고 판단하고, 이를 삭제합니다.</p>

<p>하지만 어텐션 점수를 사용하는 방식도 문장 전체의 의미를 파악하는 데 한계가 있습니다. “음식이 맛있다고 하는데 나는 그닥(부정)”이라는 문장을 한 번 보겠습니다. 이 문장은 부정의 의미를 담은 주절(나는 그닥)과 긍정의 의미를 담은 종속절(음식이 맛있다)로 이뤄져 있습니다. 모델은 핵심 내용을 담고 있는 주절에 집중해서 스타일 속성값을 파악해야 합니다. 하지만 어텐션 기반 모델은 ‘맛있다’와 [CLS]와의 관계, ‘그닥’과 [CLS]와의 관계를 각각 독립적으로 파악하고서는 ‘맛있다’에 더 많은 어텐션 값을 배정, 이 문장을 ‘긍정’으로 오 분류할 가능성이 높습니다.</p>

<p>더불어 위 방식에서는 CLS 벡터를 출력하는 BERT 계열의 모델만을 분류기로 사용할 수 있다는 제약이 따릅니다.</p>

<p>이에 카카오엔터프라이즈는 ‘스타일’ 토큰을 직관적으로 구분하면서도 특정 모델의 구조에 구애받지 않는<sup>model-agnostic</sup> 삭제 프로세스를 제안했습니다. 문장을 구성하는 각 토큰을 차례대로 누락한 토큰 시퀀스를 분류기<sup>pre-trained classifier</sup><sup><a href="#254b:fn:8" class="footnote" id="254b:fn-back:8">8</a></sup>에 입력한 후, 확률값 변화에 큰 영향을 미친 토큰을 ‘스타일’로 인식하고 이를 삭제<sup><a href="#254b:fn:9" class="footnote" id="254b:fn-back:9">9</a></sup>합니다. 예를 들어 좀 더 쉽게 설명해보겠습니다.</p>

<p>앞에서 언급한 예시 문장 x(음식이 맛있다고 하는데 나는 그닥)를 다시 보겠습니다([그림 3]). 문장을 구성하는 토큰을 순차적으로 누락한 문장을 분류기에 입력합니다. 그다음, 본래 문장 x를 입력해서 얻은 값과의 차이를 계산(IS)합니다. 여기서는 확률값 변화에 많은 영향을 미친 토큰은 ‘그닥’입니다. 따라서 삭제기는 이 토큰을 스타일로 인식하고 이를 삭제한 문장인 x’(음식이 맛있다고 하는데 나는)를 출력합니다.</p>

<p><img src="/assets/img/2021-05-25-210525/004.png" width="" align="" /></p>

<p><em class="center">[ 그림 3 ] 입력 문장에서 ‘스타일’ 텍스트를 삭제하는 과정</em></p>

<p><br /></p>

<h2 id="2-스타일-전환-문장-생성-프로세스">(2) 스타일 전환 문장 생성 프로세스</h2>

<p>생성기는 RNN 기반의 모델보다 더 좋은 성능을 내는 Transformer의 인코더와 디코더로 구현했습니다. ’스타일’ 토큰을 삭제한 문장 x’ 앞에는 문장의 시작을 알려주는 [start]와 스타일 속성을 표현하는 [style]<sup><a href="#254b:fn:10" class="footnote" id="254b:fn-back:10">10</a></sup> 두 스페셜 토큰<sup>special token</sup>이 붙습니다. 생성기는 문장의 끝을 알리는 [end]가 출력될 때까지 동작합니다.</p>

<p><img src="/assets/img/2021-05-25-210525/005.png" width="" align="" /></p>

<p><em class="center">[ 그림 4 ] ‘스타일’ 토큰을 삭제한 문장과 타깃 스타일 속성값을 이용해 원하는 스타일의 문장을 만드는 생성기의 작동 과정</em></p>

<p><br /></p>

<h2 id="3-모델-훈련">(3) 모델 훈련</h2>

<p>SST 또한 선행 연구에서와 마찬가지로 긍정 또는 부정의 말뭉치로 각각 구성된 비병렬 데이터셋만을 이용해야 합니다. 다시 말해, “그 레스토랑은 맛없어(부정)”라는 문장만 주어지고, 반대 속성의 문장인 “그 레스토랑은 맛있어(긍정)”가 없어서 감독학습을 불가능한 상황입니다.</p>

<p>이에 카카오엔터프라이즈는 가장 확실하게 알고 있는 유일한 정보가 입력 문장의 스타일 속성값(부정)이라는 점을 고려해, 스타일 속성과 관련된 두 손실 함수를 정의하고 두 함수를 최소화하는 방향으로 생성기의 인코더와 디코더를 훈련<sup><a href="#254b:fn:11" class="footnote" id="254b:fn-back:11">11</a></sup>했습니다. 두 손실 함수를 이용해 학습을 마친 모델은 ‘콘텐츠’ 토큰을 활용해 타깃 스타일 속성값으로 전환된 문장을 생성할 수 있게 됩니다.</p>

<p>우선, ‘스타일’ 토큰을 삭제한 문장(“그 레스토랑은”)과 입력 문장의 스타일 속성값(부정)을 입력받은 디코더가 본래 스타일로 분류될 수 있도록 하는 학습을 진행합니다. 여기에서는 크로스 엔트로피<sup>Cross Entropy</sup>를 이용한 재구성 손실 함수<sup>reconstruction loss function</sup>을 사용했습니다. 학습을 마치고 나면 디코더는 원래 주어진 스타일 속성값인 ‘부정’으로 분류되는 문장을 생성합니다.</p>

<p>하지만 재구성 손실 함수만으로는 디코더가 스타일 속성값을 반전하는 데에는 한계가 있습니다. 모델이 입력 문장을 다시 복구하는 법만 익혔기 때문입니다. 원래 스타일 속성값과 반대되는 문장(긍정)을 생성하는 학습도 중요합니다. ‘콘텐츠’ 토큰(“그 레스토랑은”)과 원래 주어진 스타일 속성의 반대되는 값인 ‘긍정’으로 분류될 수 있도록 모델을 훈련합니다. 여기에는 스타일 손실 함수<sup>style loss function</sup>을 이용했습니다.</p>

<p><img src="/assets/img/2021-05-25-210525/006.png" width="" align="" /></p>

<p><em class="center">[ 그림 5 ] STT 모델에서 두 손실 함수를 이용한 훈련 과정</em></p>

<p><br /></p>

<h1 id="성능-평가-및-결과">성능 평가 및 결과</h1>

<p>카카오엔터프라이즈는 음식점 리뷰를 모아 놓은 옐프(Yelp)와 상품 리뷰를 모아놓은 아마존(Amazon), 두 벤치마크 데이터셋<a href="#254b:rf:5" class="reference" id="254b:rf-back:5">[5]</a>을 이용해 최신의 TTS 모델의 성능을 비교했습니다. ‘콘텐츠’ 토큰을 얼마나 잘 보존했는지(content), 스타일이 제대로 전환됐는지(style), 말이 자연스러운지(fluency), 그리고 두 문장(출력 문장, 정답 문장)의 의미가 얼마나 유사한지(semantic)를 판단하는 총 4가지의 평가 항목을 마련했습니다.</p>

<table>
  <tr class="key">
    <td>입력 문장</td>
    <td>출력 문장</td>
    <td>평가</td>
  </tr>
  <tr>
    <td>“<span style="background-color:#fff2cc;">그 음식은</span> <span style="background-color:#d9ead3;">맛없어</span>”</td>
    <td>“<span style="background-color:#fff2cc;">그 음식은</span> <span style="background-color:#d9ead3;">별로야</span>”</td>
    <td>content는 보존, style은 못바꿈,<br />fluency는 자연스러움</td>
  </tr>
  <tr>
    <td>“<span style="background-color:#fff2cc;">서비스가</span> <span style="background-color:#d9ead3;">별로고</span> <span style="background-color:#fff2cc;">가격이</span> <span style="background-color:#d9ead3;">매우 비싸</span>”</td>
    <td>“<span style="background-color:#fff2cc;">서비스가</span> <span style="background-color:#d9ead3;">매우 좋았고</span> <span style="background-color:#fff2cc;">가격이</span> <span style="background-color:#d9ead3;">매우 비싸</span>”</td>
    <td>content는 보존, style은 애매함,<br />fluency는 약간 이상함</td>
  </tr>
  <tr>
    <td>“<span style="background-color:#fff2cc;">그것의 퀄리티는</span> <span style="background-color:#d9ead3;">꽤 괜찮아 보이는데</span> ”</td>
    <td>“<span style="background-color:#fff2cc;">그 게임은</span> <span style="background-color:#d9ead3;">재미없어 보이는데</span>”</td>
    <td>content는 미보존, style은 바꿈,<br />fluency는 자연스러움</td>
  </tr>
  <tr>
    <td>“<span style="background-color:#fff2cc;">그것의 퀄리티는</span> <span style="background-color:#d9ead3;">꽤 괜찮아 보이는데</span>”</td>
    <td>“<span style="background-color:#fff2cc;">그 나무는</span> <span style="background-color:#d9ead3;">맛없던데</span>”</td>
    <td>content는 미보존, style은 바꿈,<br />fluency는 이상함</td>
  </tr>
</table>
<p><em class="center">[ 표 2 ] TST 모델의 평가 결과 예시</em></p>

<p>각 항목 평가를 위해 총 8가지 자동화 평가 척도<sup>automatic evaluation metrics</sup>([표 3])를 활용했습니다. 인간 평가<sup>human evaluation</sup>가 더 정확하나, 시간과 비용이 많이 소모됩니다. 이에 카카오엔터프라이즈는 더 저렴하면서도 빠른 자동화 평가 척도를 가지고 안정적인 텍스트 스타일 변환 여부를 판단하고자 했습니다. 자동화 평가에서 상대적으로 좋지 않은 성능을 낸 모델이 인간에게도 좋은 평가를 받지 못한 실험적 결과가 이 선택을 뒷받침합니다.</p>

<table>
  <tr>
    <td class="key" rowspan="3">Content</td>
    <td>self-BLEU</td>
    <td class="left">입력 문장과 (시스템이) 생성 문장 간의 n-gram<sup><a href="#254b:fn:12" class="footnote" id="254b:fn-back:12">12</a></sup> 중첩 정도를 이용한 평가</td>
  </tr>
  <tr>
    <td>human-BLEU</td>
    <td class="left">인간이 생성한 문장과 시스템이 생성한 문장 간의 n-gram 중첩도를 이용한 평가</td>
  </tr>
  <tr>
    <td>geometric mean-BLEU</td>
    <td class="left">self-BLEU와 human-BLEU의 기하평균<sup>geometric mean</sup><sup><a href="#254b:fn:13" class="footnote" id="254b:fn-back:13">13</a></sup></td>
  </tr>
  <tr>
    <td class="key">Style</td>
    <td>classification accuracy</td>
    <td class="left">기학습된 분류기로 평가한 성능</td>
  </tr>
  <tr>
    <td class="key" rowspan="3">Fluency</td>
    <td>d-PPL<sup>perplexity</sup><sup><a href="#254b:fn:14" class="footnote" id="254b:fn-back:14">14</a></sup></td>
    <td class="left">평가 데이터셋에서 미세조정한 언어 모델을 이용한 지표</td>
  </tr>
  <tr>
    <td>g-PPL</td>
    <td class="left">사전학습된 언어 모델인 GPT를 이용한 지표</td>
  </tr>
  <tr>
    <td>t-PPL</td>
    <td class="left">d-PPL과 g-PPL의 기하평균</td>
  </tr>
  <tr>
    <td class="key">Semantic</td>
    <td>BERTScore<sup><a href="#254b:fn:15" class="footnote" id="254b:fn-back:15">15</a></sup></td>
    <td class="left">사전학습된 BERT를 이용한 평가</td>
  </tr>
</table>

<p><em class="center">[ 표 3 ] 성능 평가에 활용한 8가지 자동화 평가 척도<sup><a href="#254b:fn:16" class="footnote" id="254b:fn-back:16">16</a></sup></em></p>

<p>카카오엔터프라이즈는 8가지 자동화된 평가 척도를 이용해 여러 TST 모델의 성능을 비교했습니다([표 4], [표 5]). 핵심은 다른 모델에 표시된 빨간색 숫자입니다. 이 숫자는 다른 모델과 비교해 해당 평가 척도에서 현저히 낮은 성능을 의미합니다. 아주 단적인 예로, 문장은 이진 분류<sup>binary classification</sup>이기 때문에 확률상 최소 50%의 성능은 나와야 합니다. 하지만 실제로는 이에 훨씬 못 미치는 성능을 내고 있죠. 이런 점에서 카카오엔터프라이즈가 고안한 STT 모델이 모든 평가 척도에서 두루 안정적인 결과를 냈다는 점에 주목할 만합니다.</p>

<p>물론 이런 안정성 판별은 상대적인 척도라서 완벽하다고 할 수는 없습니다. 특정 척도만을 이용한 평가로 어느 시스템의 절대적인 우수성을 말할 수 없을 뿐만 아니라, 자동화 평가에서 좋은 성능을 시스템을 선별해 인간 평가를 진행한다고 하더라도 좋은 결과를 기대할 수 없기 때문입니다. 향후 모델의 성능을 다각도로 측정할 수 있는 평가 척도가 나온다면, 여러 환경에 더 적합한 시스템 개발 및 평가가 한결 더 수월해질 거로 보입니다.</p>

<p><img src="/assets/img/2021-05-25-210525/007.png" width="90%" align="" /></p>

<p><em class="center">[ 표 4 ] 옐프 데이터셋에서의 성능 평가 실험</em></p>

<p><img src="/assets/img/2021-05-25-210525/008.png" width="90%" align="" /></p>

<p><em class="center">[ 표 5 ] 아마존 데이터셋에서의 성능 평가 실험</em></p>

<p>한편, 카카오엔터프라이즈는 ‘스타일’ 벡터를 부정에서 긍정 또는 긍정에서 부정으로 조금씩 바꾸면서 생성되는 문장을 확인하는 잠재 공간 탐색<sup>latent space walking</sup>에 관한 실험도 진행했습니다. 텍스트 스타일 변환 모델이 문장의 스타일 속성값을 긍정과 부정을 전환할 수 있다면, 그 중간에 중립<sup>neutral</sup> 문장도 생성되리라는 가정을 증명하려는 목적이었습니다. 하지만 아쉽게도 중립 문장이 생성됨을 확인하지는 못했습니다. 이런 측면을 고려해 모델링을 한다면 문장 스타일을 좀 더 세밀하게 조절할 수 있을 거로 보입니다.</p>

<p><br /></p>

<h1 id="향후-계획">향후 계획</h1>

<p>카카오엔터프라이즈는 이후 텍스트 스타일 변환뿐만 아니라 사람의 감정이나 대화 상황을 이해하는 공감적 대화<sup>empathetic conversation</sup>, 데이터를 문장으로 표현하기<sup>data-to-text</sup>, 페르소나 채팅<sup>persona chat</sup><sup><a href="#254b:fn:17" class="footnote" id="254b:fn-back:17">17</a></sup>처럼 원하는 조건에 따라 문장을 생성하는 자연어 생성에 관한 다양한 연구를 계속해서 진행할 계획입니다.</p>

<p><br /></p>

<h1 id="참고-문헌">참고 문헌</h1>

<p><a id="254b:rf:1" class="referencebody"><a href="#254b:rf-back:1" class="backlink">[1]</a>  <a href="https://www.aclweb.org/anthology/2020.inlg-1.25.pdf">Stable Style Transformer: Delete and Generate Approach with Encoder-Decoder for Text Style Transfer</a> (2020) by Joosung Lee in INLG</a><br />
<a id="254b:rf:2" class="referencebody"><a href="#254b:rf-back:2" class="backlink">[2]</a>  <a href="https://research.fb.com/publications/multiple-attribute-text-rewriting/">Multiple-Attribute Text Style Transfer</a> (2019) Guillaume Lample, Sandeep Subramanian, Eric Michael Smith, Ludovic Denoyer, Marc’Aurelio Ranzato, Y-Lan Boureau in ICLR</a><br />
<a id="254b:rf:3" class="referencebody"><a href="#254b:rf-back:3" class="backlink">[3]</a>  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17015/15745">Style Transfer in Text: Exploration and Evaluation</a> (2018) by Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, Rui Yan in AAAI</a><br />
<a id="254b:rf:4" class="referencebody"><a href="#254b:rf-back:4" class="backlink">[4]</a>  <a href="https://arxiv.org/abs/1811.00552">Multiple-attribute text rewriting</a> (2019) by Sandeep Subramanian, Guillaume Lample, Eric Michael Smith, Ludovic Denoyer, Marc’Aurelio Ranzato, Y-Lan Boureau in ICLR</a><br />
<a id="254b:rf:5" class="referencebody"><a href="#254b:rf-back:5" class="backlink">[5]</a>  <a href="https://www.aclweb.org/anthology/N18-1169.pdf">Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer</a> (2018) by Juncen Li, Robin Jia, He He, Percy Liang in NAACL</a><br />
<a id="254b:rf:6" class="referencebody"><a href="#254b:rf-back:6" class="backlink">[6]</a>  <a href="https://www.ijcai.org/Proceedings/2019/0711.pdf">A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer</a> (2019) by Fuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang, Xu Sun, Zhifang Sui in IJCAI</a><br />
<a id="254b:rf:7" class="referencebody"><a href="#254b:rf-back:7" class="backlink">[7]</a>  <a href="https://arxiv.org/pdf/1908.09368.pdf">Transforming Delete, Retrieve, Generate Approach for Controlled Text Style Transfer</a> (2019) by Akhilesh Sudhakar, Bhargav Upadhyay, Arjun Maheswaran in EMNLP</a><br /></p>

<p><br /></p>

<h1 id="각주">각주</h1>

<ol class="footnotelist">
<li id="254b:fn:1" class="footnotebody" value="1"><p>  논어 ‘선진' 편을 보면, 제자 자로와 유의 똑같은 질문에 공자는 서로 다른 답변을 준다. 이에 또 다른 제자 공서화가 이를 보고 왜 그런지 묻자, 공자는 유에게는 성정을 억누를 신중함을 심어주고, 자로에게는 결단력을 북돋아 줄 필요가 있다고 답했다.<a href="#254b:fn-back:1" class="backlink"> ↩</a></p></li>
<li id="254b:fn:2" class="footnotebody" value="2"><p> 성별, 감정, 나이(세대), 인종 등 화자에 따라 달라지는 발화의 스타일을 의미한다.<a href="#254b:fn-back:2" class="backlink"> ↩</a></p></li>
<li id="254b:fn:3" class="footnotebody" value="3"><p> 새로운 데이터를 생성하는 생성자<sup>generator</sup>와 이 데이터를 평가하는 구별자<sup>discriminator</sup>가 서로 대립하며 각각의 성능을 높이는 목적을 달성하는 경쟁 끝에 진짜와 같은 가상의 새로운 데이터를 만들어낸다. 적대적 학습은 바로 이 두 주체가 충돌해서 발전이 이뤄지는 <a href="https://khshim.wordpress.com/2016/09/24/generative-adversarial-networks-1/">대립쌍 프로세스</a><sup>adversarial process</sup>라는 점에 기인했다.<a href="#254b:fn-back:3" class="backlink"> ↩</a></p></li>
<li id="254b:fn:4" class="footnotebody" value="4"><p> 긍∙부정 레이블을 단 데이터셋에 자주 등장하는 스타일 토큰을 모아둔 데이터베이스<a href="#254b:fn-back:4" class="backlink"> ↩</a></p></li>
<li id="254b:fn:5" class="footnotebody" value="5"><p> “이렇게 만들면 잘도 맛있겠네요" 문장이라면, ‘맛있겠네요'에만 집중했을 때 이 문장을 자칫 긍정(맛있다)으로 인식할 수 있다. 하지만 전체 맥락을 살펴보면, ‘맛없다'를 반어적으로 표현한 문장임을 알 수 있다.<a href="#254b:fn-back:5" class="backlink"> ↩</a></p></li>
<li id="254b:fn:6" class="footnotebody" value="6"><p> 특정 시퀀스를 디코딩할 때 관련된 인코딩 결과를 참조하게 만드는 기법<a href="#254b:fn-back:6" class="backlink"> ↩</a></p></li>
<li id="254b:fn:7" class="footnotebody" value="7"><p> Special Classification Token의 약자로, 입력 문장을 구성하는 모든 토큰의 의미를 응축하고 있다.<a href="#254b:fn-back:7" class="backlink"> ↩</a></p></li>
<li id="254b:fn:8" class="footnotebody" value="8"><p> STT 모델 훈련에 쓰인 같은 데이터로 미리 훈련해준다.<a href="#254b:fn-back:8" class="backlink"> ↩</a></p></li>
<li id="254b:fn:9" class="footnotebody" value="9"><p> 삭제 프로세스는 1)부정 확률이 적정 수준으로 떨어지거나, 2)전체 문장에서 일정 비율의 토큰이 삭제됐을 때 종결된다.<a href="#254b:fn-back:9" class="backlink"> ↩</a></p></li>
<li id="254b:fn:10" class="footnotebody" value="10"><p> 학습 단계에서 어떤 손실 함수를 이용하느냐에 따라 [style]은 입력 문장의 속성값 또는 타깃 스타일 속성값 둘 다 될 수 있다. 모델 훈련 과정에서 이를 상세히 설명해두었다. 반면, 테스트 단계에서는 스타일 속성을 얼마나 잘 바꾸는지를 평가해야 하므로 타깃 스타일 속성값을 나타낸다.<a href="#254b:fn-back:10" class="backlink"> ↩</a></p></li>
<li id="254b:fn:11" class="footnotebody" value="11"><p> 두 손실 함수를 합쳐서 학습을 진행해도 성능 면에서 큰 차이가 없다. 다만 카카오엔터프라이즈는 본 연구를 진행할 때 두 손실 함수를 번갈아가며 모델을 훈련했으며, 학습률을 서로 다르게 설정해 재구성손실 함수가 모델의 가중치를 더 많이 업데이트할 수 있도록 했다.<a href="#254b:fn-back:11" class="backlink"> ↩</a></p></li>
<li id="254b:fn:12" class="footnotebody" value="12"><p> 어떤 한 입력을 처리할 때 함께 볼 N개의 입력 단위(토큰)를 뜻한다.<a href="#254b:fn-back:12" class="backlink"> ↩</a></p></li>
<li id="254b:fn:13" class="footnotebody" value="13"><p> n개의 요소를 곱한 후 그 값에 n 제곱근을 씌운 값<a href="#254b:fn-back:13" class="backlink"> ↩</a></p></li>
<li id="254b:fn:14" class="footnotebody" value="14"><p> 언어 모델의 성능을 정량적으로 평가하기 위한 지표를 의미한다.<a href="#254b:fn-back:14" class="backlink"> ↩</a></p></li>
<li id="254b:fn:15" class="footnotebody" value="15"><p> 성능 격차가 크지 않아 안 좋은 시스템이 무엇인지 고르기 애매하다. 이에 이 부분은 시스템을 가우시안 분포로 가정하고 신뢰도 95% 이내에 포함되지 않는 점수를 가지는 시스템을 불안정한 시스템으로 결정했다.<a href="#254b:fn-back:15" class="backlink"> ↩</a></p></li>
<li id="254b:fn:16" class="footnotebody" value="16"><p> PPL을 제외한 나머지 척도에서는 숫자가 클수록 모델의 성능이 좋다고 할 수 있다.<a href="#254b:fn-back:16" class="backlink"> ↩</a></p></li>
<li id="254b:fn:17" class="footnotebody" value="17"><p> 시스템 설계자가 미리 정의한 고유 페르소나(인격)를 가진 가상의 상대와 대화를 나누는 태스크를 가리킨다. 여기서 페르소나는 4~6문장으로 표현된다. 이를테면, 대화 중에 페르소나(예: 나는 딸기를 좋아합니다)와 관련된 주제가 언급되면(“넌 무슨 케이크를 좋아해?”), 모델은 관련 내용으로 응답한다(“나는 딸기 케이크를 좋아해.”).<a href="#254b:fn-back:17" class="backlink"> ↩</a></p></li>

</ol>]]></content><author><name>samantha:작성,편집</name></author><category term="deepdive" /><category term="Text Style Transfer" /><summary type="html"><![CDATA[카카오엔터프라이즈는 입력 문장에서 '스타일' 토큰을 명시적으로 삭제하고, '내용 토큰과 타깃 스타일 속성값만을 가지고 문장 벡터를 생성하는 접근 방식에서 영감을 얻었습니다. 카카오엔터프라이즈가 고안한 SST 모델 또한 1)문장에서 ‘스타일' 토큰을 삭제한 뒤, 2)’콘텐츠’ 토큰만을 가지고 입력 문장과는 반대되는 스타일 속성값을 가진 문장을 생성합니다. 다만 기존과는 다른 방식으로 각 모듈을 구현했는데, 이는 포스트에 자세히 소개돼 있습니다.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://genius0928.github.io//assets/img/2021-05-25-210525/000.png" /><media:content medium="image" url="https://genius0928.github.io//assets/img/2021-05-25-210525/000.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">EMNLP 2020 - 다국어 번역 논문 2편을 소개합니다</title><link href="https://genius0928.github.io//deepdive/201217" rel="alternate" type="text/html" title="EMNLP 2020 - 다국어 번역 논문 2편을 소개합니다" /><published>2020-12-17T00:00:00+09:00</published><updated>2020-12-17T00:00:00+09:00</updated><id>https://genius0928.github.io//deepdive/201217</id><content type="html" xml:base="https://genius0928.github.io//deepdive/201217"><![CDATA[<p>카카오엔터프라이즈 AI Lab(이하 AI Lab)이 낸 3편의 논문이 EMNLP<sup>Empirical Methods in Natural Language Processing</sup>에 게재 승인됐습니다. 자연어처리에서 경험적 방법론을 다루는 이 학회는 ACL<sup>Association for Computational Linguistics</sup>, NAACL<sup>NORTH American Chapter of the ACL</sup>과 함께 전산언어학 분야에서는 인지도가 높습니다. 올해에는 총 3,677편의 논문 중 754편이 통과됐습니다.</p>

<p>이번 글에서는 EMNLP에 게재된 자사 논문 중, 신경망 기반 다국어 기계 번역 모델(MNNT<sup>Multilingual Neural machine Translation</sup>)에 관한 최신 기법을 다룬 2편의 논문<a href="#40f4:rf:1" class="reference" id="40f4:rf-back:1">[1]</a><a href="#40f4:rf:2" class="reference" id="40f4:rf-back:2">[2]</a>을 상세히 다뤄보고자 합니다. 두 논문을 저술한 AI Lab AI기술실 컨택스트팀 소속의 류성원 연구원과 손보경 연구원을 만나봤습니다.</p>

<p><br /></p>

<p class="notice">※보다 명확하고 간결한 방식으로 대상을 지칭하기 위해 본문에서는 다국어 번역을 수행하는 모델을 MNMT로, 두 언어 간의 번역을 수행하는 모델을 NMT라고 명명했습니다.</p>

<p><br /></p>

<p class="dot-line">∙  ∙  ∙  ∙  ∙  ∙  ∙</p>

<p><br /></p>

<h1 id="다국어-번역을-위한-모델-아키텍처">다국어 번역을 위한 모델 아키텍처</h1>

<p>두 논문에서 공통적으로 언급하는 다국어 번역 아키텍처에 대한 배경부터 짚어보겠습니다.</p>

<p>인코더<sup>encoder</sup>와 디코더<sup>decoder</sup> 기반 seq2seq<sup>sequence to sequence</sup><sup><a href="#40f4:fn:1" class="footnote" id="40f4:fn-back:1">1</a></sup> 아키텍처의 NMT는 기계 번역에 큰 혁신을 안겼습니다. 서로 다른 두 언어의 문장으로 구성된 대량의 병렬 코퍼스<sup>parallel corpus</sup>로부터 단어 의미와 단어 순서, 문장 구조, 단어 간의 의존 관계 등번역에 필요한 모든 정보(문장 벡터)<a href="#40f4:rf:3" class="reference" id="40f4:rf-back:3">[3]</a>를 스스로 학습하기 때문입니다. 이로써 인간이 기계에 세세한 번역 규칙을 하나씩 가르칠 필요가 없게 됐습니다.</p>

<p>기존에는 다국어 번역(\(N\)개 언어)을 위해 \(N×(N-1)\)개의 NMT를 각기 따로 두었습니다(Single NMT, [그림 1-(1)]). 영어, 한국어, 스페인어를 번역해야 한다면 [영→한, 한→영, 한→스, 스→한, 영→스, 스→한] 이렇게 총 여섯 개의 NMT를 만드는 식이죠. 다만 문제는 언어 수가 늘어나는 만큼 훈련해야 할 모델 수, 좀 더 자세히 말하면 매개변수<sup>parameter</sup> 수가 제곱으로 커진다(\(O\)(\(N^{2}\)))는 겁니다.</p>

<p><img src="/assets/img/2020-12-17-201217/001.png" width="90%" align="" /></p>

<p><em class="center">[ 그림 1 ] 3개국 언어 간 번역 태스크를 해결하는 3가지 방식</em></p>

<p>이에 매개변수 수를 획기적으로 줄인 multi-way MNMT<a href="#40f4:rf:4" class="reference" id="40f4:rf-back:4">[4]</a>가 새로 제안됐습니다([그림 1-(3)]). 여러 NMT를 각기 따로 두는 대신, 언어별 인코더와 디코더를 두고 여러 방향에서 이를 공유하는 구조<sup><a href="#40f4:fn:2" class="footnote" id="40f4:fn-back:2">2</a></sup>를 마련했습니다. N×(N-1)개의 모델, 다시 말해 N×(N-1)개의 인코더-디코더를 훈련해야 하는 Single NMT와 비교해, multi-way MNMT는 2N개의 인코더-디코더만 훈련하기만 하면 됩니다. 관련된 언어의 인코더와 디코더만 활용할 수 있어 추론 시간과 비용도 효과적으로 낮출 수 있습니다. 또한, 새로운 언어를 추가하고 싶다면 해당하는 인코더와 디코더 쌍만 따로 추가 훈련시키면 됩니다.</p>

<p>하지만 오늘날에는 전체 매개변수 수 대비 더 나은 성능을 내는 1-1 MNMT<a href="#40f4:rf:5" class="reference" id="40f4:rf-back:5">[5]</a>에 대한 연구가 주류로 자리하고 있습니다([그림 1-(2)]). 이 모델 아키텍처의 이름은 하나의 인코더(1)와 하나의 디코더(1)로만 구성됐다는 부분을 표현하고 있죠. 이처럼 매우 간단한 방법으로 다국어 번역 문제를 해결한 1-1 MNMT는 데이터가 상대적으로 적은 방향의 태스크<sup>low-resource environment</sup>에서도 일정 수준 이상의 성능을 냅니다. 또한, 학습 데이터가 하나도 주어지지 않은 태스크인 제로샷<sup>zero-shot</sup> 번역을 최초로 선보였습니다.</p>

<p><br /></p>

<h1 id="revisiting-modularized-multilingual-nmt-to-meet-industrial-demands-류성원-손보경-양기창-배재경1">Revisiting modularized multilingual NMT to meet industrial demands (류성원, 손보경, 양기창, 배재경)<a href="#40f4:rf:1" class="reference" id="40f4:rf-back:1">[1]</a></h1>

<h2 id="1-1-mnmt가-지닌-한계">1-1 MNMT가 지닌 한계</h2>

<p>하지만 기업 환경에서는 1-1 MNMT를 사용하기가 어려운 현실적인 문제가 산재합니다<a href="#40f4:rf:6" class="reference" id="40f4:rf-back:6">[6]</a>.</p>

<p>우선, 훈련해야 할 번역 방향이 늘어날수록 성능은 되려 저하되는 병목 현상<sup>capacity bottleneck</sup>이 발생합니다. 이 문제는 1-1 모델 크기가 충분히 크지 않아서, 요구되는 성능 대비 수용 능력(모델 크기)이 충분히 크지 않아서 발생합니다. 그렇다고 모델 크기를 무한정 키울 수도 없습니다. 매개변수 수가 기하급수적으로 늘어난 만큼 계산 복잡성이 커져서 최적화<sup><a href="#40f4:fn:3" class="footnote" id="40f4:fn-back:3">3</a></sup>가 매우 어려워지기 때문입니다.</p>

<p>대규모 1-1 MNMT로 모든 방향의 (성능 저하가 없는) 번역이 가능해지더라도, 실용화는 여전히 먼 나라의 일입니다. 기업 환경에서는 사용자가 요청한 하나의 질의<sup>query</sup>를 처리하는 데 드는 계산량이 비용과 직결되는 만큼 추론에 사용하는 매개변수 수를 중시합니다. 그런데 딥러닝 특성상 특정 방향의 번역 태스크에 일부 매개변수만을 선택적으로 사용할 수 없다 보니 추론 비용이 굉장히 커지게 됩니다. 마찬가지 이유로 새로운 언어를 추가하거나 또는 기존 언어를 제거하기 위해 모델 전체를 재훈련하는 비용도 상당히 높죠.</p>

<p>이에 많은 기업에서는 비용이 상당히 저렴한 pivot NMT 방식을 좀 더 선호합니다([그림 2]). ‘회전축의 중심’이라는 본래 의미에서 착안, 다국어 번역에서 피벗<sup>pivot</sup>은 전세계적으로 가장 많이 쓰이는 언어인 영어를 매개로 하는 중간 번역을 의미합니다. 비영어→영어 NMT, 영어→비영어 NMT을 각각 N개만 훈련하면 되어서 비용을 많이 낮출 수 있습니다.</p>

<p><img src="/assets/img/2020-12-17-201217/002.png" width="" align="" /></p>

<p><em class="center">[ 그림 2 ] 한국어→중국어 번역을 위한 pivot NMT 구조</em></p>

<p>하지만 두 단계에 걸쳐 번역이 이뤄지는 구조 상 추론에 더 많은 시간이 걸립니다. 피벗에 사용되는 두 NMT의 구조가 동일하다고 가정했을 때, 전체 추론 시간은 한 방향 번역에 걸리는 시간의 2배가 되는 셈이죠. 두 NMT 중 하나라도 성능이 낮으면 전체 번역 성능도 낮아지는 점 역시 구조에 기인합니다.</p>

<p>이에 AI Lab은 1-1 MNMT와 pivot NMT, Single NMT의 단점을 극복한 실용적인 번역 모델을 새롭게 탐색하는 과정에서 multi-way MNMT가 지닌 가치를 재발견하는 연구를 진행하게 됐습니다. 그럼 이전에 제안된 서로 다른 아키텍처를 갖춘 번역 모델의 성능을 직접적으로 비교하는 실험 결과에 대해 상세히 설명해보겠습니다.</p>

<p><br /></p>

<h2 id="모델-구현">모델 구현</h2>

<p>본래 multi-way MNMT는 인코더와 디코더를 LSTM으로 구현했습니다. 하지만 LSTM과 같은 RNN 계열의 아키텍처에서는 문장이 길수록 계산 속도가 느려지고 거리가 먼 단어 간 관계를 제대로 표현하지 못하는 문제가 발생합니다. 이런 한계를 극복하고자 나온 최신 아키텍처가 바로 Transformer입니다. Transformer는 컨볼루션<sup>convolution</sup>이나 순환<sup>recurrence</sup> 기법을 사용하지 않고 어텐션<sup>attention</sup><sup><a href="#40f4:fn:4" class="footnote" id="40f4:fn-back:4">4</a></sup>만으로도 거리가 먼 단어 간의 관계를 효과적으로 표현해 높은 추론 성능을 달성했습니다. AI Lab은 이런 특장점을 갖춘 Transformer로 구현해 훈련한 버전을 M2NMT<a href="#40f4:rf:1" class="reference" id="40f4:rf-back:1">[1]</a>라 명명했습니다.</p>

<p><br /></p>

<h2 id="실험-결과">실험 결과</h2>

<p>AI Lab은 다국어 번역 태스크를 위한 세 방법론(Single NMT, 1-1 MNMT, M2MNT)의 감독학습<sup>supervised learning</sup><sup><a href="#40f4:fn:5" class="footnote" id="40f4:fn-back:5">5</a></sup> 성능을 비교하는 실험을 진행했습니다. 그 결과([그림 3]), 1-1 MNMT는 성능 병목 현상으로 인해 Single NMT보다 더 낮은 성능을 냈습니다. 반면, M2NMT는 셋 중 가장 높은 성능을 선보였습니다. 한 매개변수가 여러 방향의 번역에 공유되는 구조로 인해 다국어 학습 시너지 효과<sup>data-diversification, and regularization effect</sup>가 난 거로 분석됩니다. 한편, 앞서 설명한대로 1-1 MNMT의 크기를 키우면 성능 병목 현상이 크게 개선된다는 점도 확인할 수 있습니다([그림 4]).</p>

<p><img src="/assets/img/2020-12-17-201217/003.png" width="80%" align="" /></p>

<p><em class="center">[ 그림 3 ] 12방향의 번역 태스크에서 Single NMT, 1-1 MNMT, M2NMT의 성능</em></p>

<p><img src="/assets/img/2020-12-17-201217/004.png" width="80%" align="" /></p>

<p><em class="center">[ 그림 4 ] 모델 크기가 상대적으로 더 클수록 성능 병목 현상이 완화됨을 확인할 수 있다.</em></p>

<p>두번째 실험에서 AI Lab은 4개의 언어 간 번역(En, De, Es, Ni 기준) 태스크를 익힌 M2(4)에, 아직 학습하지 않은 새로운 언어(Fr)를 담당하는 인코더와 디코더 쌍을 점진적으로 훈련해나갈 때의 제로샷 성능을 비교했습니다<sup><a href="#40f4:fn:6" class="footnote" id="40f4:fn-back:6">6</a></sup>. 그 결과[ 표 1-(2) ], M2(4)는 제로샷  태스크에서 Single(5)보다 더 좋은 성능을 보였습니다. 아울러, M2(4)가 더 많은 *↔︎Fr 방향의 태스크를 배울수록(ID 1→2→3→4), M2(4)의 En↔︎Fr 간 번역 성능이 조금씩 늘어나며, 모든 번역 태스크를 추가 학습한 M2(4)의 성능이 M2(5)에 버금간다는 점을 볼 수 있습니다. pivot(4)와 비교해서도, M2NMT가 거의 모든 제로샷 태스크에서 pivot NMT 대비 더 좋은 성능(빨간색 글씨)을 달성했음을 ([표 1-(1)])에서 확인했습니다.</p>

<p><img src="/assets/img/2020-12-17-201217/005.png" width="" align="" /></p>

<p><em class="center">[ 표 1 ] (1) 제로샷에서 pivot NMT와 M2NMT 성능 비교 (2) 새로운 언어(Fr)에 대한 번역 태스크를 추가한 모델의 성능 비교</em></p>

<p>AI Lab은 M2NMT가 생성한 문장 벡터가 언어에 독립적인 공간<sup>interlingual space</sup>에 사상된다고 가정했습니다. 그 구조상 각 인코더는 모든 디코더가 해석할 수 있는 정보를 제공해야 하며, 각 디코더는 모든 인코더가 제공한 정보를 해석할 수 있어야 하기 때문입니다. 학습을 마친 모델에 새로 추가된 인코더와 디코더가 약간의 훈련을 거치기만 해도 각각 언어에 독립적인 표현력과 해석력을 갖췄음을 보여준 두번째 실험은 이 가정을 실험적으로 증명했습니다.</p>

<p><img src="/assets/img/2020-12-17-201217/006.png" width="62%" align="" /></p>

<p><em class="center">[ 그림 5 ] M2MNT에서 생성되는 언어에 독립적인 특징 공간</em></p>

<p><br /></p>

<h1 id="sparse-and-decorrelated-representations-for-stable-zero-shot-nmt손보경-류성원2">Sparse and Decorrelated Representations for Stable Zero-shot NMT(손보경, 류성원)<a href="#40f4:rf:2" class="reference" id="40f4:rf-back:2">[2]</a></h1>

<h2 id="1-1-mnmt의-제로샷-성능이-낮은-이유">1-1 MNMT의 제로샷 성능이 낮은 이유</h2>

<p>MNMT 성능 향상의 핵심 전략이 두 언어의 문장으로 구성된 대량의 병렬 말뭉치 확보에 있다고 해도 과언이 아닙니다. 하지만 한 언어쌍에 대해, 유명한 벤치마크 중 비교적 작은 규모인 160,000개<sup><a href="#40f4:fn:7" class="footnote" id="40f4:fn-back:7">7</a></sup> 수준의 병렬 말뭉치를 구축하는 일조차 하늘에 별따기만큼 쉽지가 않습니다. 이런 이유로 제로샷에서도 충분한 성능을 내는 모델 아키텍처에 관한 연구가 이전보다 더 활발하게 이뤄지고 있습니다.</p>

<p>서론에서는 다국어 학습으로 인한 시너지 효과를 충분히 기대할 수 있으며, 모델 규모 대비 기대 이상의 성능을 내는 1-1 MNMT를 주로 활용한다는 점을 밝힌 바 있습니다. 1-1 MNMT는 어떤 언어로 번역할지를 안내하는 메타 토큰(token)을 입력해 원하는 언어의 문장을 생성합니다.</p>

<p>하지만 1-1 MNMT를 이용한 제로샷 번역에서는 비영어 문장을 입력하면 토큰 종류와 관계없이 항상 영어 문장을 생성하려는 경향이 커지는 문제가 생깁니다. 이처럼 특정 결과만을 도출하려는 현상을 학계에서는 ‘얽힘<sup>entanglement</sup>’이라고 표현합니다. 전세계적으로 가장 많이 쓰이는 언어인 영어로 작성된 문장이 반드시 포함된 병렬 말뭉치<sup>English-centric data</sup>가 원인으로 보이지만, 이는 아직 수학적으로 증명된 바 없습니다.</p>

<p>배치 크기<sup>batch size</sup>, 드롭아웃<sup>dropout</sup>, 가중치 초기화<sup>weight Initialization</sup>와 같은 초매개변수<sup>hyperparameter</sup> 조정은 얽힘 문제 해결에 도움이 되지 않았습니다. 제로샷 번역의 성능은 초매개변수 값에 매우 민감하게 반응하기 때문입니다. 그렇다고 이 값을 고정해버리면 감독학습에서의 번역 성능이 되려 낮아질 위험도 높아집니다.</p>

<p><br /></p>

<h2 id="1-1-mnmt의-구조적-한계를-극복하려는-시도">1-1 MNMT의 구조적 한계를 극복하려는 시도</h2>

<p>이에 빔 탐색<sup>beam search</sup>으로 다음에 올 단어를 예측할 때, 목표로 하는 언어에 해당하는 어휘만 남겨두고 나머지는 걸러내<a href="#40f4:rf:7" class="reference" id="40f4:rf-back:7">[7]</a> 얽힘 문제를 해결하려는 시도가 있었습니다. 빔의 모든 탐색 단계에서 의도하는 언어의 단어를 선택해야 그 다음에도 목표로 하는 언어의 단어가 생성된다는 아이디어로부터 착안됐습니다.</p>

<p>또다른 시도는 역번역<sup>back translation</sup><a href="#40f4:rf:8" class="reference" id="40f4:rf-back:8">[8]</a><sup><a href="#40f4:fn:8" class="footnote" id="40f4:fn-back:8">8</a></sup>을 이용한 데이터 어그먼테이션<sup>augmentation</sup><sup><a href="#40f4:fn:9" class="footnote" id="40f4:fn-back:9">9</a></sup>입니다. 시중에서 쉽게 구할 수 있는 비영어 단일 말뭉치를 모델에 입력해 생성한 (비영어, 비영어’) 병렬 말뭉치로 데이터 균형을 맞춰서 모델을 재훈련한다면, 영어 문장을 내뱉는 경향을 줄일 수 있다고 본 거죠. 다만, 모든 언어쌍에 대해 합성 데이터셋을 생성하는 데 드는 계산량이 큰 만큼 높은 비용이 발생합니다.</p>

<p>최근에는 동일한 의미를 가진 영어 문장 벡터와 비영어 문장 벡터가 언어에 독립적인 벡터를 생성할 수 있도록 인코더를 정규화하는 기법<sup>regularization</sup><sup><a href="#40f4:fn:10" class="footnote" id="40f4:fn-back:10">10</a></sup>을 다룬 연구가 제안됐습니다. 이는 동일한 의미를 나타내는 비영어 문장과 영어 문장을 표현하는 두 벡터를 서로 구분할 수 없을 정도로 유사하다면, 디코더가 비영어 문장을 영어로 번역하려는 판단을 쉽게 내리지 못하리라는 가정을 전제로 합니다.</p>

<p><br /></p>

<h2 id="ai-lab의-제안-slni를-이용한-인코더-정규화">AI Lab의 제안, ‘SLNI를 이용한 인코더 정규화’</h2>

<p>지속 학습<sup>continual learning</sup><sup><a href="#40f4:fn:11" class="footnote" id="40f4:fn-back:11">11</a></sup>에서는 새로운 태스크를 가르칠 때 기존 태스크에서 배운 지식을 잊어버리는 ‘파괴적 망각<sup>catastrophic forgetting</sup>‘이라는 현상이 발생합니다. 기존 태스크에 이용된 뉴런이 활성화되며, 관련된 매개변수 값이 재조정되기 때문입니다.</p>

<p>SLNI<a href="#40f4:rf:9" class="reference" id="40f4:rf-back:9">[9]</a><sup>Sparse coding through Local Neural Inhibition</sup>은 이같은 문제를 해결하기 위한 정규화 기법입니다. 연구진은 활성 상태에 있는 뉴런이 인접한 다른 뉴런의 활성화를 억제하는 ‘측면 억제<sup>lateral inhibition</sup>’에서 아이디어를 얻었습니다. 기존 태스크에서 활성화된 중요한 뉴런값을 바꾸는 대신, 활성화되지 않은 뉴런을 새로운 태스크에 사용한다면 망각을 줄일 수 있다고 본거죠.</p>

<p>SLNI는 서로 무관하면서도 희소한 특징을 갖춘 벡터<sup>sparse and decorrelated representation</sup>를 만들면 측면 억제 효과를 얻을 수 있다고 설명합니다. 이에 SLNI는 국소적으로 인접한 두 차원의 값에 점진적으로 가우시안<sup>gaussian</sup><sup><a href="#40f4:fn:12" class="footnote" id="40f4:fn-back:12">12</a></sup> 패널티를 줍니다.</p>

<p>AI Lab은 이 SLNI가 파괴적 망각 현상의 완화 외에도 활용처가 있으리라 판단, Transformer의 인코더를 구성하는 하위층<sup>sublayer</sup>에 SLNI를 적용했습니다. 그 결과, AI Lab은 바로 위에서 언급한 특징을 갖춘 벡터를 생성하는 인코더가 훈련 조건의 변화에 강건한 제로샷 모델을 만드는 데 도움이 됐음을 발견했습니다. 다음 실험 결과 항목에서 이 내용을 보다 자세히 설명하겠습니다.</p>

<p><br /></p>

<h2 id="실험-결과-1">실험 결과</h2>

<p>AI Lab은 번역 모델로 Transformer를, 훈련 데이터셋으로는 IWSLT2017를 활용했습니다. 이 데이터셋은 한쪽이 영어, 다른 한쪽은 독일어(De), 이탈리아어(It), 네덜란드어(NI), 로마니아어(Ro)로 구성된 병렬 말뭉치입니다. SLNI를 적용한 번역 모델이 다양한 훈련 조건에도 안정적으로 문장을 합성하는지를 살펴보고자 4가지 실험 환경을 설정해 모델의 성능을 비교했습니다([표 2]).</p>

<table style="border-collapse: collapse; width: 100%;" border="1">
<tbody>
  <tr>
    <td style="width:20%; text-align:center; font-weight:bold">Default</td>
    <td style="width:80%;">최대 2400 토큰/번역 방향 당, 0.2 드롭아웃</td>
  </tr>
  <tr>
    <td style="text-align:center; font-weight:bold">AttDrop</td>
    <td>attention dropout<sup><a href="#40f4:fn:13" class="footnote" id="40f4:fn-back:13">13</a></sup> * 0.1 + activation dropout<sup><a href="#40f4:fn:14" class="footnote" id="40f4:fn-back:14">14</a></sup> * 0.1</td>
  </tr>
  <tr>
    <td style="text-align:center; font-weight:bold">LargeBatch</td>
    <td>최대 9600 토큰/번역 방향 당</td>
  </tr>
  <tr>
    <td style="text-align:center; font-weight:bold">Compound</td>
    <td>AttDrop + LargeBatch</td>
  </tr>
</tbody>
</table>
<p><em>[ 표 2 ]1-1 MNMT 번역 모델의 인코더에 SLNI를 적용하면, 여러 실험 조건에서 안정적으로 제로샷 성능을 달성하는지를 확인하는 실험을 진행했다.</em></p>

<p>AI Lab은 SLNI를 적용한 번역 모델이 감독학습에서의 성능은 거의 그대로 유지하며, 특히 다양한 훈련 조건에서 안정적인 제로샷 성능을 확보할 수 있음을 확인했습니다. 이는 데이터 후처리<sup>post-processing</sup>나 사전 훈련, 데이터 어그먼테이션 없이 간단한 조치만으로도 만들어낸 성과라 큰 의의가 있다고 평가해볼 수 있습니다.</p>

<p><img src="/assets/img/2020-12-17-201217/007.png" width="" align="" /></p>

<p><em class="center">[ 그림 6 ] 감독학습(왼쪽)과 제로샷 태스크를 수행할 때 SLNI 기법의 효과를 나타내는 그래프</em></p>

<p>AI Lab은 1-1 MNMT의 인코더를 정규화하면 모든 언어의 문장 벡터가 사상된 공간의 형태가 서로 유사한지<sup>space similarity</sup>, 동일한 의미를 가진 영어 문장 벡터와 비영어 문장 문장 벡터가 비슷한 표현을 갖추는지<sup>instance similarity</sup>를 확인하는 실험도 진행했습니다. 만약 SLNI를 적용한 번역 모델이 위 두 유사성을 갖춘다면, SLNI를 적용하지 않은 버전(naive)와 비교해 더 높은 숫자를 갖춰야 합니다. 하지만 실험 결과, 두 버전 간 숫자간 격차가 크지 않음을 확인했습니다. 즉, 이 실험은 SLNI가 기존 인코더 정규화 기법을 다룬 연구에서 전제한 내용과는 다른 원리로 얽힘 문제를 해결했음을 보여줍니다.</p>

<p><img src="/assets/img/2020-12-17-201217/008.png" width="90%" align="" /></p>

<p><em>[ 표 3 ] 1-1MNMT에서 인코더를 정규화하는 접근 방식이 언어에 독립적인 특징 벡터를 생성한다는 걸 전제로 하지 않음을 실험적으로 증명했다.</em></p>

<p><br /></p>

<h1 id="향후-계획">향후 계획</h1>

<p>AI Lab은 M2NMT가 생성한 언어에 독립적인 특징 공간의 여러 효용을 검증하는 연구는 물론, 제로샷 태스크에서 1-1 MNMT가 훈련 조건 변화에 취약한 원인과 새로 제안한 기법의 문제 해결 원리에 관한 새로운 탐색을 해나갈 계획입니다. 그 뿐만 아니라 도메인에 특화된 번역 모델과 지속 학습에서 안정적으로 성능을 내는 기법에 대한 연구도 진행할 계획입니다. 앞으로도 자사 기술력을 집약한 카카오 i 번역 엔진에 대한 많은 애정과 관심 부탁드립니다.</p>

<p><br /></p>

<h1 id="참고-문헌">참고 문헌</h1>

<p><a id="40f4:rf:1" class="referencebody"><a href="#40f4:rf-back:1" class="backlink">[1]</a>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.476/">Revisiting Modularized Multilingual NMT to Meet Industrial Demands</a> (2020) by Sungwon Lyu, Bokyung Son, Kichang Yang, Jaekyoung Bae in EMNLP </a><br />
<a id="40f4:rf:2" class="referencebody"><a href="#40f4:rf-back:2" class="backlink">[2]</a>  <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.205/">Sparse and Decorrelated Representations for Stable Zero-shot NMT</a> (2020) by Bokyung Son, Sungwon Lyu in EMNLP </a><br />
<a id="40f4:rf:3" class="referencebody"><a href="#40f4:rf-back:3" class="backlink">[3]</a>  <a href="http://kiss.kstudy.com/public/public2-article.asp?key=50905527">일상생활 속으로 들어온 기계 번역</a> (2017) by 김준석 in 국립국어원 </a><br />
<a id="40f4:rf:4" class="referencebody"><a href="#40f4:rf-back:4" class="backlink">[4]</a>  <a href="https://www.aclweb.org/anthology/N16-1101/">Multi-way, multilingual neural machine translation with a shared attention mechanism</a> (2016) by Orhan Firat, Kyunghyun Cho, Yoshua Bengio in ACL </a><br />
<a id="40f4:rf:5" class="referencebody"><a href="#40f4:rf-back:5" class="backlink">[5]</a>  <a href="https://www.aclweb.org/anthology/Q17-1024/">Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation Melvin Johnson</a> (2017) by Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, Jeffrey Dean in ACL </a><br />
<a id="40f4:rf:6" class="referencebody"><a href="#40f4:rf-back:6" class="backlink">[6]</a>  <a href="https://www.aclweb.org/anthology/2020.acl-main.148/">Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation</a> (2020) by Biao Zhang, Philip Williams, Ivan Titov, Rico Sennrich in ACL </a><br />
<a id="40f4:rf:7" class="referencebody"><a href="#40f4:rf-back:7" class="backlink">[7]</a>  <a href="https://arxiv.org/abs/1711.07893">Effective strategies in zero-shot neural machine translation</a> (2017) by Thanh-Le Ha, Jan Niehues, Alexander Waibel in Arxiv </a><br />
<a id="40f4:rf:8" class="referencebody"><a href="#40f4:rf-back:8" class="backlink">[8]</a>  <a href="https://www.aclweb.org/anthology/P19-1121/">Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations</a> (2019) by Jiatao Gu, Yong Wang, Kyunghyun Cho, Victor O.K. Li in ACL </a><br />
<a id="40f4:rf:9" class="referencebody"><a href="#40f4:rf-back:9" class="backlink">[9]</a>  <a href="https://research.fb.com/publications/selfless-sequential-learning/">Selfless Sequential Learning</a> (2019) by Rahaf Aljundi, Marcus Rohrbach, Tinne Tuytelaars  in ICLR </a><br /></p>

<p><br /></p>

<h1 id="각주">각주</h1>

<ol class="footnotelist">
<li id="40f4:fn:1" class="footnotebody" value="1"><p> 먼저 단위정보 시퀀스를 인코더에 입력한다. 인코더는 이를 분석해 고정 길이의 벡터 표현<sup>vector representation</sup>을 추정한다. 디코더는 이 벡터를 활용해 또 다른 단위정보의 시퀀스를 생성한다.<a href="#40f4:fn-back:1" class="backlink"> ↩</a></p></li>
<li id="40f4:fn:2" class="footnotebody" value="2"><p> 이를 테면, 한국어→영어, 한국어→일본어 방향의 번역에서는 하나의 한국어 인코더를 사용한다. 반대로, 영어→한국어, 일본어→한국어 번역에서도 하나의 한국어 디코더만 사용한다.<a href="#40f4:fn-back:2" class="backlink"> ↩</a></p></li>
<li id="40f4:fn:3" class="footnotebody" value="3"><p> 모든 학습 데이터에 대해 오차를 최소화하는 가중치 값을 찾은 상태<a href="#40f4:fn-back:3" class="backlink"> ↩</a></p></li>
<li id="40f4:fn:4" class="footnotebody" value="4"><p> 기계 번역을 위한 seq2seq 구조의 모델의 RNN 셀에서 특정 시퀀스를 디코딩할 때 관련 인코딩 결과값을 참조할 수 있게 하는 보조적인 역할을 하는 데 처음 사용됐다. 이후 구글이 Transformer를 통해 어텐션만으로도 문장을 모델링하는 셀프어텐션<sup>self-attention</sup> 기법을 선보였다.<a href="#40f4:fn-back:4" class="backlink"> ↩</a></p></li>
<li id="40f4:fn:5" class="footnotebody" value="5"><p> 정답<sup>label</sup>이 주어진 데이터셋으로 훈련하는 방식<a href="#40f4:fn-back:5" class="backlink"> ↩</a></p></li>
<li id="40f4:fn:6" class="footnotebody" value="6"><p> 새로운 언어가 포함된 방향의 번역 태스크의 훈련은 기존 언어별 인코더와 디코더의 가중치를 고정한 상태에서 진행한다.<a href="#40f4:fn-back:6" class="backlink"> ↩</a></p></li>
<li id="40f4:fn:7" class="footnotebody" value="7"><p> IWSLT14(En-De)<a href="#40f4:fn-back:7" class="backlink"> ↩</a></p></li>
<li id="40f4:fn:8" class="footnotebody" value="8"><p> 기계가 생성한 문장을 다시 학습에 사용한다는 특성 때문에 역번역이라는 이름이 붙여진 거로 보인다.<a href="#40f4:fn-back:8" class="backlink"> ↩</a></p></li>
<li id="40f4:fn:9" class="footnotebody" value="9"><p> 이미지를 좌우로 뒤집거나(flipping) 자르는(cropping) 등 데이터에 인위적인 변화를 가하는 방법론<a href="#40f4:fn-back:9" class="backlink"> ↩</a></p></li>
<li id="40f4:fn:10" class="footnotebody" value="10"><p> 설명력이 높으면서도 그 구조나 표현이 간단한 상태를 이르는 말<a href="#40f4:fn-back:10" class="backlink"> ↩</a></p></li>
<li id="40f4:fn:11" class="footnotebody" value="11"><p> 학습 중간에 또는 기존 주어진 데이터셋의 학습이 끝나고 난 상황에서 새로운 데이터나 새로운 범주의 데이터가 입력해 모델을 업데이트하는 학습 방식<a href="#40f4:fn-back:11" class="backlink"> ↩</a></p></li>
<li id="40f4:fn:12" class="footnotebody" value="12"><p> 평균을 중심으로 좌우 대칭의 종모양을 갖는 확률분포를 가리킨다. 여기서는 이 가우시안 분포 함수에 근사해 생성한 패널티 함수를 가리킨다.<a href="#40f4:fn-back:12" class="backlink"> ↩</a></p></li>
<li id="40f4:fn:13" class="footnotebody" value="13"><p> multihead attention에 주는 dropout<a href="#40f4:fn-back:13" class="backlink"> ↩</a></p></li>
<li id="40f4:fn:14" class="footnotebody" value="14"><p> 2-layer feedforward 중간에 있는 relu activation 뒤의 dropout<a href="#40f4:fn-back:14" class="backlink"> ↩</a></p></li>
</ol>]]></content><author><name>samantha:작성,편집</name></author><category term="deepdive" /><category term="translation" /><category term="MNMT" /><category term="EMNLP" /><summary type="html"><![CDATA[첫번째 논문은 1-1 MNMT가 지닌 한계를 극복한 실용적인 번역 모델을 새롭게 탐색하는 과정에서 multi-way MNMT의 가치를 재발견했습니다. LSTM 대신 Transfomer로 인코더와 디코더를 구현한 버전을 M2NMT라 명명하고 실험을 진행했습니다. 두번째 논문에서는 제로샷 번역에서 1-1 MNMT의 한계와 SLNI 기법이 무엇인지, SLNI를 이용했을 때 제로샷-감독학습 번역에서 성능이 어떻게 달라지는지 실험적으로 증명했습니다.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://genius0928.github.io//assets/img/2020-12-17-201217/000.jpg" /><media:content medium="image" url="https://genius0928.github.io//assets/img/2020-12-17-201217/000.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">지식그래프에서 경로를 탐색하는 모델 AttnIO를 소개합니다</title><link href="https://genius0928.github.io//deepdive/201214" rel="alternate" type="text/html" title="지식그래프에서 경로를 탐색하는 모델 AttnIO를 소개합니다" /><published>2020-12-14T00:00:00+09:00</published><updated>2020-12-14T00:00:00+09:00</updated><id>https://genius0928.github.io//deepdive/201214</id><content type="html" xml:base="https://genius0928.github.io//deepdive/201214"><![CDATA[<p>이상형에 관한 설문조사를 보면 상위권에 오르는 유형 중 하나가 바로 ‘대화가 통하는 사람’입니다. 끊임없이 말을 주고받는 시간이 즐거워서 또 만나서 이야기를 나누고 싶다는 감정이 드는 상대를 의미하겠죠. 소개팅 자리에서 본인을 할리우드 영화감독 스티븐 스필버그(Steven Spielberg)의 팬이라 소개하며 대화 포문을 열었을 때, “그래요?”, “그렇군요…”, “아 네…”라기보다는, “저도요! 레오나르도 디카프리오가 주연을 맡은 ‘캐치 미 이프 유 캔’은 여러 번 봤을 정도로 음악도, 연기도, 연출도 모두 멋진 작품이었어요”라고 말하는 사람이 바로 여기에 해당할 겁니다.</p>

<p>사람과 대화를 나누는 기계를 만들 때도 위에서 언급한 ‘대화가 잘 통한다’는 느낌을 제공하는 게 중요합니다. 물론 지금 당장은 감정적 교감을 나누는 기계의 구현은 먼 미래의 일입니다. 이에 현재 산학계에서는 대화 맥락에 어울리는 유일한 정보를 제공하는 과제부터 순차 해결하는 데 집중하고 있습니다.</p>

<p>오늘날의 많은 대화 시스템은 언어 모델<sup>language model</sup>을 이용해 문장을 생성합니다. 특정 대상에 대한 지식을 보다 원활하게 획득하고자 최근에는 외부 지식 베이스<sup>knowledge base</sup><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>를 활용하는 방식도 고려되고 있습니다. 올해 EMNLP<sup>Empirical Methods in Natural Language Processing</sup><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>에 게재된 카카오엔터프라이즈 AI Lab(이하 AI Lab)의 논문에서는 바로 이 지식 베이스 상에서 대화 맥락에 어울리는 (답변) 경로를 효과적으로 탐색하는 모델<sup>path retrieval model</sup>인 AttnIO<a href="#51e9:rf:1" class="reference" id="51e9:rf-back:1">[1]</a>를 제안했습니다. 자연어처리에서 경험적 방법론을 다루는 올해 이 학회에서는 총 3,677개 중 754개의 논문이 통과됐습니다.</p>

<p><img src="/assets/img/2020-12-14-201214/001.png" width="" align="" /></p>

<p><em class="center">[ 그림 1 ] AttnIO가 지식 그래프에서 경로를 탐색하는 과정</em></p>

<p>이번 시간에는 바로 이 AttnIO를 자세히 소개해보고자 합니다. AI Lab AI기술실 컨텍스트팀 소속의 정재훈 연구원을 만나 기존 접근 방식의 한계, AI Lab이 제안한 AttnIO의 작동 과정과 그 성능, 그리고 향후 연구 계획에 관한 이야기를 들어봤습니다.</p>

<p><br /></p>

<p class="dot-line">∙  ∙  ∙  ∙  ∙  ∙  ∙</p>

<p><br /></p>

<h1 id="지식-그래프를-활용하는-대화-시스템">지식 그래프를 활용하는 대화 시스템</h1>

<p>산업 현장에서는 Seq2Seq<sup>sequence to sequence</sup><sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> 기반 언어 모델을 활용합니다. 이 언어 모델은 대량의 말뭉치를 사전학습하기만 해도 외부 지식<sup>external knowledge</sup> 일부를 습득할 수 있습니다. 영화에 대한 문서도 학습한 모델이라면, 스필버그 감독에 관한 지식을 내뱉을 수 있다는 의미입니다. 언어 모델 그 자체를 지식 베이스의 일종이라고 볼 수 있다고 한 ‘Language Models as Knowledge Bases?<a href="#51e9:rf:2" class="reference" id="51e9:rf-back:2">[2]</a>‘라는 논문이 이를 뒷받침합니다.</p>

<p>하지만 언어 모델은 ‘스티븐 스필버그가 판타지 영화 해리포터를 연출했다’처럼 겉으로 보기엔 그럴싸해 보이나, 사실이 아닌 문장을 생성할 수 있다는 취약점을 지니고 있습니다. 언어 모델은 단어 시퀀스의 우도<sup>likelihood</sup><sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>를 최대화하는 방식으로 주어진 단어 다음에 올 단어를 예측합니다. 이 과정에서 모델은 스스로 문장에 포함된 의미가 사실에 근거하는지 아닌지를 구분하지 못합니다. 그래서 ‘스티븐 스필버그는 SF영화 A.I를 감독했다”라는 옳은 문장만큼이나, 위의 잘못된 예시 문장이 생성될 확률이 높아집니다.</p>

<p>앞서 설명한 학습 방식으로 인해 언어 모델은 입력된 문장과 관계없는 상투적인 표현을 내뱉기도 합니다<a href="#51e9:rf:3" class="reference" id="51e9:rf-back:3">[3]</a>. 이는 훈련 데이터셋에 “안녕하세요”, “잘 몰라요”, “감사합니다”, “미안합니다”와 같은 표현의 높은 등장 빈도에 기인한 현상으로 분석됩니다. 또는 자주 함께 등장하는 주어와 동사, 또는 목적어를 포함한, 구체성이 떨어지는 문장이 생성될 수도 있습니다. 스티븐 스필버그가 누군지 묻는 말에 “아니 몰라” 또는, “스티븐 스필버그는 연출했다”, “스티븐 스필버그는 태어났다”로 대답하는 식이죠.</p>

<p>이에 외부 지식을 언어 모델에 내재화하거나, 따로 저장한 외부 지식을 학습에 활용하는 메커니즘에 대한 필요성이 커지게 됐습니다. 2019년부터는 외부에 구축해 둔 지식 베이스를 활용한 연구가 본격적으로 제안됐습니다. 특히 지식 그래프<sup>Knowledge graph</sup><sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>에서 대화 맥락에 어울리는 개체<sup>entity</sup>와 관계<sup>relation</sup>를 찾는 경로 탐색에 관한 연구가 활발하게 이뤄지고 있습니다. ‘(스티븐 스필버그)→감독하다→(A.I)→장르→(SF)’와 같은 경로를 만들어낼 수 있다면, 글 도입부 예시와 같은 적절한 응답 문장을 수월하게 생성하리라는 판단에 근거한 접근입니다.</p>

<p>페이스북(Facebook)은 ‘OpenDialKG{%rf%}<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>’ 논문에서 RNN 디코더<sup>decoder</sup>를 제안했습니다. \(n\)단계에서의 출력값을 이용해 \(n+1\)단계에서의 상태를 예측하는 재귀적 구조를 이용해, 탐색 각 단계에서 가장 최고의 답을 만드는 하나의 노드를 선택하는 탐욕적 탐색<sup>Greedy Search</sup>을 진행합니다. 바이두(Baidu)는 강화학습<sup>reinforcement learning</sup><sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup> 기반 에이전트로 탐욕적 경로 탐색을 구현했습니다{%rf%}.</p>

<p><br /></p>

<h1 id="기존-접근-방식의-한계점">기존 접근 방식의 한계점</h1>

<p>하지만 이런 기존의 접근 방식에는 다음과 같은 한계가 존재합니다.</p>

<p>우선, 인코더가 지식 그래프의 구조 정보를 반영하지 못합니다. 오픈도메인 대화 시스템에 사용되는 지식 그래프 규모만 해도 노드 11만 개, 관계 100만 개 이상일 정도로 그 규모가 상당합니다. 따라서 실제 대화에서 사용되는 극소수의 지식을 매우 효과적으로 찾기 위해서는 노드와 이웃 노드 간의 관계를 함축적으로 표현하는 인코딩 기법이 필요합니다. 하지만 기존의 그래프 임베딩<sup>graph embedding</sup><sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup>은 각 노드의 주변 정보를 효과적으로 반영하는 데 한계가 있습니다.</p>

<p>두 번째, 탐색 방식의 한계입니다. “누가 영화 A.I.를 감독했어?”라는 질문에 대한 답이 “스티븐 스필버그” 하나인 예시처럼, 정답이 정해진 닫힌 질문에서는 기존의 탐욕적 탐색 방식만으로도 적합합니다. 이 때문에 “스티븐 스필버그가 누구야?”라는 질문처럼 “미국인” 또는 “영화감독”, “케이드 캡쇼의 남편”과 같은 여러 답변이 존재하는 열린 질문에는 적합하지 않습니다. 매 탐색의 순간 가장 높은 확률값이 할당된 간선과 연결된 노드를 선택하는 방식으로 단일 경로를 생성하기 때문입니다.</p>

<p>세 번째, &lt;대화 말뭉치, 지식 경로&gt;로 구성된 대규모 병렬 코퍼스 구축에 큰 비용과 많은 시간이 필요합니다. 앞에서 계속 예시로 언급한 스티븐 스필버그와 그가 연출한 작품을 두고 대화를 나누는 모델을 만들기 위해서는 라벨링된 학습 데이터로 감독학습<sup>supervised learning</sup>을 진행해야 합니다. 이미 구축된 지식 그래프를 토대로 사람이 직접 학습에 이용할 대화 말뭉치와 지식 경로를 하나하나 만들어야 한다는 의미죠.</p>

<p><br /></p>

<h1 id="ai-lab이-제안한-attnio의-동작-방식">AI Lab이 제안한 AttnIO의 동작 방식</h1>

<p>AI Lab은 <b>1)</b>인코딩 과정에서 지식 그래프의 구조 정보를 반영하고, <b>2)</b>대화 맥락에 따라 경로를 생성하며, <b>3)</b>정답 경로의 첫 번째 노드와 마지막 노드만을 이용한 약감독학습<sup>weakly-supervised learning</sup>으로도 감독학습에 버금가는 성능을 달성한 새로운 지식 경로 탐색 모델인 AttnIO를 제안했습니다. in-flow<sup>incoming attention flow</sup>에서 각 노드는 자신과 연결된 이웃 노드와 그 관계를 표현하는 벡터를 생성하며, out-flow<sup>outgoing attention flow</sup>에서는 대화 맥락에 따른 최적의 경로를 탐색합니다. 자세한 설명은 각 단락에서 이어서 하겠습니다.</p>

<p><img src="/assets/img/2020-12-14-201214/002.png" width="" align="" /></p>

<p><em class="center">[ 그림 2 ] AttnIO의 아키텍처</em></p>

<h2 id="dialog-encoder">dialog encoder</h2>

<p>대화의 맥락과 질문의 의도를 효과적으로 반영한 컨텍스트 벡터(context vector)를 생성하는 모듈로, 사전학습을 마친 ALBERT<sup>A Lite BERT</sup><a href="#51e9:rf:4" class="reference" id="51e9:rf-back:4">[4]</a><sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>의 인코더로 구현됐습니다. AI Lab은 문장을 구성하는 모든 토큰의 응축된 의미를 가진 CLS<sup>Special Classification token</sup>에 해당하는 벡터를 컨텍스트 벡터로 활용했습니다.</p>

<h2 id="in-flow">in-flow</h2>

<p>in-flow는 그래프 신경망<sup>Graph Neural Networks</sup><sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>을 이용합니다. 우선, 각 노드와 간선에 해당하는 데이터를 벡터로 변환합니다. 그런 뒤, 특정 노드는 진입 간선<sup>incoming edge</sup>으로 연결된 이웃 노드와 이 간선을 합쳐 새로운 값으로 업데이트됩니다<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">11</a></sup>. 이 업데이트 과정을 반복하면 특정 노드는 그래프 구조상의 주변 정보도 표현할 수 있게 됩니다.</p>

<p>A에서 나가는 간선이 2개인 ‘\(C\)(학생)←이다←\(A\)(해리포터)→사랑한다→\(B\)(지니 위즐리)’를 예로 들어보겠습니다. 시작 노드가 \(A\)라고 한다면, \(C\) 노드는 “해리포터는 학생이다”, \(B\) 노드는 “해리포터는 지니 위즐리를 사랑한다”라는 정보를 표현하는 벡터를 갖게 되는 겁니다.</p>

<p>Entity-Context Fusion이라는 모듈에서 in-flow의 출력인 각 노드 벡터와 dialog encoder의 출력인 컨텍스트 벡터를 합치면, 각 노드 벡터는 대화의 맥락과 질문의 의도까지 표현할 수 있게 됩니다. 예를 들어, 스티븐 스필버그와 관련된 여러 지식 중 그가 감독한 영화 관련 지식을 노드 벡터에 더 많이 반영하게 되는 거죠.</p>

<h2 id="out-flow">out-flow</h2>

<p>대화에 따라서는 “해리포터는 지니 위즐리를 사랑한다”는 지식이 필요할 수도 있고, “해리포터는 학생이다”는 지식이 필요할 수도 있습니다. 이처럼 현재의 대화 맥락에 따라 경로의 중요도를 계산하는 게 out-flow의 역할입니다.</p>

<p>out-flow는 어텐션<sup>attention</sup>을 전파하는 방식으로 경로를 탐색합니다. 어텐션은 특정 시퀀스를 디코딩할 때 관련된 인코딩 결과를 참조하게 만드는 기법을 일컫습니다. 여기서는 특정 노드에 연결된 진출 간선<sup>outgoing edge</sup> 중 대화 맥락에 더 어울리는 쪽에 더 높은 어텐션 값을 전파하는 걸 의미합니다. 우선, 대화의 첫 발화 문장과 관련된 노드의 어텐션을 초기화<sup>initial attention computation</sup>하고 나서, 연속해서 어텐션을 전파합니다. 어텐션 전파를 완료한 이후에는 각 탐색 단계에서 어텐션이 가장 많이 전파된 노드와 간선을 선택해 최종 경로를 생성합니다.</p>

<p>위의 예시를 다시 예로 들어보겠습니다. 대화 맥락에 따라 ‘\(A\)→사랑한다→\(B\)‘라는 지식이 필요하면, out-flow는 \(A\)의 어텐션 값 대부분을 ‘사랑한다’는 간선을 통해 B로 보냅니다. 반대로 ‘\(A\)→이다→학생’이 필요한 대화에서는 ‘이다’라는 간선을 통해 \(C\)쪽 방향 쪽 위주로 어텐션 값을 보냅니다.</p>

<p><br /></p>

<h1 id="모델-성능-평가-기준">모델 성능 평가 기준</h1>

<p>AI Lab은 지난 2019년 페이스북이 ‘유익한 응답 생성을 위한 지식 그래프 탐색’ 평가를 목적으로 만들어서 배포한 OpenDialKG<a href="#51e9:rf:4" class="reference" id="51e9:rf-back:4">[4]</a>라는 벤치마크를 활용했습니다. 여기에는 약 10만 개의 대화<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">12</a></sup>와 지식 경로로 구성된 병렬 코퍼스, 100만 개 이상의 간선으로 구성된 지식 그래프가 포함돼 있습니다.</p>

<table>
<tr>
  <td class="key">질문</td>
  <td class="left">스티븐 스필버그가 누구야?</td>
</tr>
<tr>
  <td class="key">정답 경로</td>
  <td class="left">[스티븐 스필버그, 직업, 영화감독]</td>
</tr>
<tr>
  <td class="key">AttnIO가 생성한 후보 경로</td>
  <td class="left">[스티븐 스필버그, 국적, 미국] (1등)<br />
  [스티븐 스필버그, 직업, 영화감독] (2등)<br />
  [스티븐 스필버그, 감독하다, 영화] (3등)</td>
</tr>
</table>
<p><em class="center">[ 표 1 ] path@k와 tgt@k 설명을 위한 예시</em></p>

<p>[ 표 1 ]의 질문이 포함된 대화 말뭉치를 AttnIO에 입력하면, 전파된 어텐션 값에 따라 여러 지식 경로 후보군이 생성됩니다. \(path@k\)는 정답과 일치하는 경로를 기준으로, \(tgt@k\)는 정답 경로의 마지막 노드만을 기준으로 K등수 안에 포함되는지 여부를 기준으로 점수를 산정합니다. 등수 안에 들면 100점을, 포함되지 않으면 0점을 부과해 모든 데이터셋에 대한 평균 점수를 산술합니다. AttnIO가 생성한 1위 후보군이 정답 경로와 일치하는 상황이 가장 이상적입니다. 그래서 \(path@1\) 또는 \(tgt@1\)는 다른 태스크 평가에서 쓰이는 정확도와 동일한 지표라고 보면 됩니다.</p>

<p><br /></p>

<h1 id="실험-결과">실험 결과</h1>

<p>지식 경로를 라벨링한 데이터를 감독학습한 여러 모델의 성능을 비교한 결과, AttnIO-AS는 OpenDialKG에서 현재 최고 수준의(SOTA) 추론 성능을 달성했습니다. 특히 k가 작을수록 그 성능 격차가 더 크다는 부분은 눈여겨서 볼만 합니다. AttnIO가 생성한 1위 후보 경로와 경로상 마지막 노드가 정답과 일치할 확률이 다른 모델보다 높다는 점을 시사하기 때문입니다. 이는 탐색 각 단계에서 노드를 미리 선택하는 RNN 디코더를 갖춘 모델과는 달리, 대화 맥락에 따라 가장 많은 어텐션 값을 전파받은 경로를 선택하는 구조에 기인한 효과로 분석됩니다. 주변 노드와의 관계를 잘 표현하는 노드 인코딩 기법도 유효한 영향을 미쳤습니다.</p>

<p><img src="/assets/img/2020-12-14-201214/003.png" width="" align="" /></p>

<p><em class="center">[ 표 2 ] 대화 말뭉치에 지식 경로를 라벨링한 데이터를 학습한 모델의 성능 비교</em></p>

<p>정답 경로의 첫 번째 노드와 마지막 노드만을 이용한 약감독학습 버전인 AttnIO-TS도 기존 모델과 비견할 만한 수준의 성능을 보였습니다.</p>

<p>다만 \(path@k\)에서는 감독학습한 모델과 약감독학습한 모델 간 점수 차가 더 크게 벌어졌습니다. 이는 경로에 포함된 모든 노드와 관계를 다 맞춰야만 정답으로 처리하는 평가 방식의 한계입니다. 예를 들어, 사용자가 “스티븐 스필버그가 어떤 영화를 감독했어?”라고 물으면, [스티븐 스필버그, 감독하다, (모든 스필버그 영화)]가 정답이어야 합니다. 하지만 \(path@k\)는 정답 경로에서 언급되지 않은 다른 영화 제목은 모두 오답으로 처리하죠.</p>

<p>이를 보완하고자 [감독하다]와 같은 관계 정보만을 제대로 포함하는지를 평가<sup>relation path accuracy</sup>하는 실험([표 3])을 진행했습니다. 그 결과, 정답 경로를 배우지 않은 AttnIO-TS가 노드 간의 관계 정보를 배운 AttnIO-AS에 상응하는 수준의 성능을 선보였습니다. 모델이 생성한 경로의 적합성을 사람이 직접 평가하는 실험([표 4])에서는 AttnIO-TS가 좀 더 나은 점수를 받기도 했습니다. 이는 대화 맥락에 따라 어텐션을 전달하는 과정에서 경로를 스스로 탐색한 거로 보입니다.</p>

<p><img src="/assets/img/2020-12-14-201214/004.png" width="40%" align="" /></p>

<p><em class="center">[ 표 3 ] AttnIO-AS와 AttnIO-TS의 관계 경로 일치성 여부를 (자동) 평가한 결과</em></p>

<p><img src="/assets/img/2020-12-14-201214/005.png" width="90%" align="" /></p>

<p><em class="center">[ 표 4 ] AttnIO-AS와 AttnIO-TS가 생성한 경로의 적합성에 대한 인간 평가 결과</em></p>

<p>아울러 AI Lab은 AttnIO가 해석력<sup>interpretability</sup>도 갖추고 있음을 확인했습니다([그림 3]). 각 추론 단계에서 특정 노드가 어떤 간선을 통해 얼마나 많은 어텐션을 받았는지를 볼 수 있다는 의미입니다. “AttnIO가 열린 질문과 닫힌 질문 사이의 차이를 경로 탐색 과정에서 반영하는가?”를 분석한 결과, “스티븐 스필버그가 누구야?” 식의 열린 질문에는 여러 후보 노드에 어텐션 값을 골고루 전파했습니다. 반면, “누가 영화 A.I.를 감독했어?” 식의 닫힌 질문에는 정답이 될 수 있는 한, 두 가지의 개체에만 어텐션이 전파됐음을 확인할 수 있었습니다.</p>

<p><img src="/assets/img/2020-12-14-201214/006.png" width="" align="" /></p>

<p><em class="center">[ 그림 3 ] AttnIO가 추론한 어텐션 값 분포를 분석하면 AttnIO가 대화 맥락에 따라 탐색한 지식 경로를 살펴볼 수 있다.</em></p>

<p><br /></p>

<h1 id="향후-계획">향후 계획</h1>

<p>이번 글에서는 기존 대화 모델에서 발견된 한계를 극복하고자 지식 그래프를 활용하는 방법을 소개했습니다. 대화 시스템뿐만 아니라, 텍스트 생성과 관련된 거의 모든 태스크에서는 사실이 아닌 문장이나 지나치게 단순한 문장이 생성되는 현상이 꾸준히 보고되고 있습니다. AI Lab은 AttnIO의 방법론을 활용해 더 다양한 NLP 태스크에서 외부 지식을 효과적으로 이용하는 방안을 탐색하는 한편, 텍스트에서 지식을 추출하는 모델과 언어 모델의 합동훈련<sup>joint-training</sup>을 위한 일반화된 프레임워크를 연구할 계획입니다.</p>

<p>정재훈 연구원은 “외부 지식 베이스를 이용하는 언어 모델은 관계 레이블(감독하다, 사랑한다, 국적 등)을 이해만 하고 이를 생성하지 못한다”며 “특정 지식 베이스의 스키마<sup>schema</sup>에 종속되지 않고 사람처럼 개념과 개념 간의 관계를 자유롭게 구조화하는 지식 추출 모델에 관한 새로운 접근 방식을 탐색하겠다”고 말했습니다.</p>

<p><br /></p>

<h1 id="참고-문헌">참고 문헌</h1>

<p><a id="51e9:rf:1" class="referencebody"><a href="#51e9:rf-back:1" class="backlink">[1]</a>  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.280/">AttnIO : Knowledge Graph Exploration with In-and-Out Attention Flow for Knowledge-Grounded Dialogue</a> (2020) by Jaehun Jung, Bokyung Son, Sungwon Lyu in EMNLP </a><br /></p>

<p><a id="51e9:rf:2" class="referencebody"><a href="#51e9:rf-back:2" class="backlink">[2]</a>  <a href="https://www.aclweb.org/anthology/D19-1250/">Language Models as Knowledge Bases?</a> (2019) by Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, Sebastian Riedel in EMNLP </a><br /></p>

<p><a id="51e9:rf:3" class="referencebody"><a href="#51e9:rf-back:3" class="backlink">[3]</a>  <a href="https://arxiv.org/abs/1510.03055">A Diversity-Promoting Objective Function for Neural Conversation Models</a> (2016) by Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan in NAACL </a><br /></p>

<p><a id="51e9:rf:4" class="referencebody"><a href="#51e9:rf-back:4" class="backlink">[4]</a>  <a href="https://www.aclweb.org/anthology/P19-1081">OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs</a> (2019) by Seungwhan Moon, Pararth Shah, Anuj Kumar, Rajen Subba in ACL </a><br /></p>

<p><a id="51e9:rf:5" class="referencebody"><a href="#51e9:rf-back:5" class="backlink">[5]</a>  <a href="https://www.aclweb.org/anthology/D19-1187">Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs</a> (2019) by Zhibin Liu, Zheng-Yu Niu, Hua Wu, Haifeng Wang in EMNLP </a><br /></p>

<p><a id="51e9:rf:6" class="referencebody"><a href="#51e9:rf-back:6" class="backlink">[6]</a>  <a href="https://arxiv.org/abs/1909.11942">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a> (2019) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut in arXiv </a><br /></p>

<p><br /></p>

<h1 id="각주">각주</h1>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>지식을 구조화해서 표현해 놓은 자료 <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>ACL(Association for Computational Linguistics), NAACL(NORTH American Chapter of the ACL)와 함께 전산언어학 분야에서는 인지도가 높은 학회 <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>단어 시퀀스를 입력받아 임베딩 벡터를 생성하는 인코더와 이 벡터를 이용해 또 다른 단어 시퀀스를 생성하는 디코더를 갖춘 아키텍처 <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>확률변수 X가 주어졌을 때 또다른 확률변수 Y가 나올 확률 <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>그래프는 데이터를 노드를, 노드와 노드를 잇는 간선<sup>edge</sup>을 이용해 데이터 간 관계 정보를 나타낸다. 지식 그래프는 바로 이 그래프를 이용해 표현한 지식을 가리킨다. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>자연어처리 분야 최고 학회 중 하나인 ACL에서 연구의 시의성을 인정받아 Honorable Mention 상을 받기도 했다. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>환경과 에이전트가 상호작용하면서 시도와 실패를 통해 보상을 학습한다. 매 단계에서 에이전트가 행동 α를 수행하면 환경은 보상 r과 다음 상태 s를 반환한다. 에이전트는 한 에피소드<sup>episode</sup>에서 최대 보상을 얻을 수 있는 행동 또는 행동 순서를 선택한다. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>이웃 노드와의 관계를 고려해 그래프의 노드를 벡터로 변환하는 기법 <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>토큰 시퀀스가 입력되면 ALBERT는 맥락을 반영한 토큰별 임베딩 벡터를 출력한다. AI Lab은 파이썬 NLP 라이브러리를 전문적으로 만드는 기술 스타트업 허깅페이스(HuggingFace)에서 제공한 버전을 활용했다. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>임의의 구조의 그래프 G가 들어왔을 때 이를 하나의 특징 벡터로 표현하는 모델 <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p>이웃 노드와 간선을 합치는 과정에서 각 노드에 부여된 어텐션 값과 노드 벡터를 곱한다. 이처럼 어텐션 값이 직접 전파되지는 않지만 어텐션을 곱한 주변 노드 벡터가 전파된다는 점에서 incoming-attention-flow라는 이름이 붙여졌다. <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:12" role="doc-endnote">
      <p>대화 지문에는 오픈 도메인에서의 일상 대화 뿐 아니라 영화를 추천하는 목적 지향적 대화<sup>task-oriented dialog</sup>도 포함돼 있다. <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>samantha:작성,편집</name></author><category term="deepdive" /><category term="translation" /><category term="MNMT" /><category term="EMNLP" /><summary type="html"><![CDATA[산업 현장에서 많이 쓰이는 Seq2Seq 기반 대화 모델은 지식을 갖춘 문장 생성에 취약합니다. 대화 모델의 근간이 되는 대규모 말뭉치를 사전학습한 언어 모델은 스스로 문장에 포함된 의미가 사실에 근거하는지 아닌지를 구분하지 못하기 때문입니다. 아울러 입력 문장과 관계없는 상투적인 표현을 내뱉는 경향도 크죠. 이에 지난 2019년부터는 외부에 구축해 둔 지식 그래프를 활용하는 연구가 본격적으로 제안되고 있습니다. 특히 지식 경로를 효과적으로 탐색하는 방법론이 많이 다뤄지고 있습니다. 카카오엔터프라이즈 연구도 이와 궤를 같이합니다.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://genius0928.github.io//assets/img/2020-12-14-201214/000.jpg" /><media:content medium="image" url="https://genius0928.github.io//assets/img/2020-12-14-201214/000.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">무인 편의점 개발기 - Edge Device로 Object Tracking 시스템 구축하는 방법</title><link href="https://genius0928.github.io//deepdive/201117" rel="alternate" type="text/html" title="무인 편의점 개발기 - Edge Device로 Object Tracking 시스템 구축하는 방법" /><published>2020-11-17T00:00:00+09:00</published><updated>2020-11-17T00:00:00+09:00</updated><id>https://genius0928.github.io//deepdive/201117</id><content type="html" xml:base="https://genius0928.github.io//deepdive/201117"><![CDATA[<p>무인편의점 프로젝트는 ‘비전(vision) 기술을 활용해 일상에서 쉽게 접할 수 있는 제품’을 만들어보자는 취지로 시작됐습니다. 여러 아이디어 중 자사가 지난 수년간 축적해온 내부 기술을 조합한 “여러 카메라를 이용한 사람 추적 시스템”이 무인편의점이라는 컨셉에 가장 부합한다고 판단, 지난 2019년 9월부터 개발을 진행하고 있습니다.</p>

<p>현재 카카오엔터프라이즈의 사내 공간인 ‘비전룸’은 무인편의점 형태로 운영되고 있습니다. 곳곳에 설치된 여러 대의 카메라가 비전룸에 입장한 사람들의 행동(공간 내 어디에 있는지, 어떤 상품을 집었는지 등)을 실시간으로 추적합니다.</p>

<p>AI Lab이 구성한 사람 추적 시스템은 크게 4가지 모듈로 구성돼 있습니다.</p>

<ol>
  <li>먼저 추적 대상을 찾는 person detector</li>
  <li>대상의 관절 위치를 찾는 pose estimator</li>
  <li>매장 내 사람의 위치를 추정하는 multi-view geometry</li>
  <li>영상에서 사람의 위치를 실시간 추적하는 person tracker</li>
</ol>

<p>카메라 촬영으로 획득한 영상 데이터는 사실 매우 고용량의 데이터라 서버로 전송해 이를 분석하려면 많은 시간과 비용이 듭니다. 이를 해결하고자 AI Lab은 여러 카메라에서 획득한 영상을 한 대의 엣지 디바이스로 분석할 수 있도록 하는 시스템 구축에 집중했습니다.</p>

<p>이 개발 프로젝트를 공동 리드한 카카오엔터프라이즈・카카오 소속 엔지니어인 김한샘(person detection), 강동오(pose estimation), 박민호(multi-view geometry), 안지운(person tracking), 임현호(system integration)를 만나 사람 추적 시스템의 구조를 자세히 들어봤습니다.</p>

<p><br /></p>

<div class="kakaotv-wrapper">
    <iframe src="https://play-tv.kakao.com/embed/player/cliplink/414132079" allowfullscreen=""></iframe>
</div>

<p><br /></p>

<p class="dot-line">∙  ∙  ∙  ∙  ∙  ∙  ∙</p>

<p><br /></p>

<h1 id="무인-편의점-비전룸">무인 편의점 ‘비전룸’</h1>

<p>먼저, 저희가 만든 가상의 편의점은 이렇습니다. 실험을 위해 사내에 ‘비전룸’이라는 공간을 설치했고요. 이 방의 중앙에는 과자, 라면 등이 있는 매대와, 구석에는 냉동 또는 냉장 식품을 넣을 수 있는 매대가 설치되어 있습니다.</p>

<p>그리고 이 공간에서 일어나는 일을 저희의 비전 기술로 파악하기 위해 총 8대의 카메라를 사방에 설치했습니다.</p>

<p>이렇게해서 총 8대의 카메라에서 들어오는 영상을 동시에, 그리고 실시간으로 계속 분석하는데요. 이를 위해, 서버 위주로 시스템을 만들 수도 있겠죠. 즉 서버에 영상을 보내고, 서버에서 처리하는 식으로 구현할 수도 있겠습니다. 하지만 처리하게 될 동영상은 아주 고용량의 데이터이기 때문에, 이 데이터를 보내는것만 하더라도, 운영 비용에 많은 투자가 필요하게 됩니다. 이런 비용을 줄이고자 저희는 서버 대신에, 엣지 디바이스 단에서 추적을 수행하도록 설계하였습니다. 여기서 엣지 디바이스란, 입력 데이터를 가까이서 처리할 수 있는 소형 컴퓨터라고 이해하시면 되겠습니다.</p>

<p>엣지 디바이스 중에서도, 딥러닝 모델로 빠르게 추론 결과를 얻을 수 있는 것이 중요한데요. 그렇게 하기 위해서 GPU가 있는 기종이 필요합니다. 특히 Nvidia에서 출시한 Jetson 제품군을 이용한다면, 영상을 다루는 DeepStream과, GPU 추론 API인 tensorRT를 사용할 수 있는 장점이 있죠. 그 중에서도 저희는 연산량이 많으면서도, 비교적 저렴한 Jetson Xavier를 고르게 되었습니다.</p>

<p>유사 서비스를 제공하는 아마존 고를 보면요, 고가의 depth camera를 설치하고 그 카메라마다 하나의 엣지 디바이스를 연결하는 식으로 되어 있습니다. 이렇게 하면 시스템 구축 비용이 많이 들게 되는데요, 이 점이 아마존 고의 대중화를 어렵게 만드는 요인으로 잘 알려져 있습니다. 그래서 저희는 여러 대의 카메라를, 단 한 대의 엣지 디바이스에 연결하는 방식을 고려하게 되었고요, 그래서 합리적인 가격에 아마존 고와 유사한 성능을 내는, 시스템을 구축하는 데 집중했습니다.</p>

<p><br /></p>

<h1 id="object-tracking-overview">Object Tracking Overview</h1>

<p>저희가 만든 시스템의 흐름을 간략하게 표현해 보았는데요. 먼저 추적 대상을 찾는 Person detector가 있고요, 또 대상의 관절 위치를 찾는 Pose estimator, 이어서 매장 내 위치를 추정하는 multi-view geometry 모듈, 그리고 최종적으로 tracker를 거쳐서 사람을 추적하게 됩니다. 자세한 구현 방식은 각 장에서 좀 더 자세히 설명하겠습니다.</p>

<h2 id="person-detection">Person Detection</h2>

<p>무인 편의점에서, 문을 열고 어떤 사람이 들어온다면, 카메라로 그걸 보고 사람이 있다는 걸 알아차려야 되겠죠? person detection은 바로, 영상에서 사람을 찾아내는 기술입니다.</p>

<p>detector에서 중요한 것은, 여러 사람이 밀집되어 있는 경우라던지 가려져서 잘 안보이는 경우에도 잘 작동해야 한다는 것입니다. 특히 저희 실험 환경에서는, 천장에 가까운-높은 위치에 카메라를 설치하기 때문에 그런 문제가 더 심각하게 되죠. 따라서 이 실제 환경과 되도록 유사한 데이터셋으로 학습하는 것이 중요하고요, 실제로 이에 맞는 데이터를 수집하여 학습에 활용했습니다.</p>

<p>detector는 tracking이 시작되는 지점이라고 할 수 있습니다. 사람을 검출한 이후에야 다른 모듈도 수행할 수 있겠죠. 이는 detector가 성능이 아주 중요하다는 걸 의미하기도 합니다.</p>

<p>검출기는 연산에 필요한 단계에 따라서 크게 두 가지로 나뉩니다. 먼저 one stage detector가 있는데요, 단 한 번만에 객체의 위치를 찾아내는 방법이고요, 반면, two stage detector는 두 단계로 이루어져 있습니다. 먼저 객체의 대략적인 위치를 찾는 단계가 있고요, 그후에 이를 보정하는 과정이 추가됩니다.</p>

<p>보통 two stage detector가 속도는 느리지만 더 정확하기 때문에, 이 방식의 검출기를 많이 사용하게 되죠. 하지만 저희는 내부적인 실험과 성능 개선 작업을 통해 one-stage detector로 만으로도 충분히 정확한 검출을 할 수 있다는 점을 확인했습니다.</p>

<p>추론 속도를 더 빠르게하기 위해서 pruning과 quantization와 같은 추가 기법을 사용할 수 있습니다. 이를 이용해서 정확도는 거의 유지하면서도 속도는 아주 빠르게할 수 있습니다. 하지만 이것만으로는 속도가 충분하지 않을 수 있죠. 이때 사용할 수 있는 방법 중 하나를 소개해 드리자면, 여기 그래프를 보면, 여러 개로 나뉘어져 있지만 하나로 간략화 할 수 있는 연산이 있죠. 또 추론에 불필요한 연산이 남아있는 경우도 있습니다. 그런 것을 고치는 작업을 할 수 있는데, 이를 그래프 최적화라고 합니다. 저희는 이것을 이용해서 약 20% 정도 속도를 높일 수 있었습니다.</p>

<p>이렇게하면, 여러 사람이 한꺼번에 들어오는 경우에도, 빠른 속도로 정확하게 검출하는 것을 확인할 수 있죠.</p>

<h2 id="pose-estimation">Pose Estimation</h2>

<p>그리고 이어서, 사람의 신체 관절 위치를 찾는데요. 이때 사용되는 기술이 pose estimation이 되겠습니다.</p>

<p>pose를 찾는 것 역시 때때로 어려울 수 있는데요. detector와 마찬가지로 다른 사람이나 매대로 가려질 수 있고, 카메라 왜곡 또한 발생합니다. 따라서 이 부분에서도 실제 환경과 유사한 데이터를 수집하여 학습을 진행하게 되었습니다.</p>

<p>pose estimator 또한 두 가지로 분류할 수 있습니다. 우선, top-down 방식은, 영상에서 사람의 위치를 박스 형태로 찾은 다음에, 그 박스 내에서 pose를 찾는 방식이고요. 반면, bottom-up 방식은 이런 힌트 없이도 한 번에 pose을 찾는 방식입니다. detection 모듈이 따로 있기 때문에 저희는 top-down 방식의 pose estimator로 진행하였습니다.</p>

<p>그런데 문제는, 이 top-down 방식, 그러니까 검출한 박스 안에서 pose을 찾는 방식은요, 성능은 더 좋지만, 박스마다 각각 pose를 찾아야하는 단점이 있습니다. 때문에 사람이 많아지게 되면 처리 속도가 느려지게 되겠죠. 따라서 사람이 많은 경우에도 빠른 속도를 유지할 수 있도록 모델을 최대한 가볍게 만드는 게 정말 중요하게 되었습니다. 그래서 저희는 distillation 기법을 적극 이용해서 모델 크기를 대폭 줄이게 하였습니다. 이렇게 했을 때 성능은 약, 2% 정도 떨어지긴 하지만, 추론 속도는, 30% 이상, 향상시킬 수 있었습니다.</p>

<p>그렇게해서, 보시는 것과 같이, 14개의 관절 위치를 빠르게 찾는 것을 확인할 수 있습니다.</p>

<h2 id="multiple-view-geometry">Multiple view Geometry</h2>

<p>MultiView Geometry 는 3차원 공간 상에서 사람의 매장 내 현재 위치를 실시간으로 추정하는 단계입니다. 이 때, Person Detector 에서 얻은 box 정보와, Pose Estimator 에서 얻은 2차원 pose 정보를 이용합니다. 즉, 카메라의 정보를 활용해서 2차원 좌표 정보를 3차원 좌표 정보로 변환하는 단계라고 보시면 됩니다.</p>

<p>이미지는 3차원 공간에 있는 object 가, 카메라 렌즈와 센서를 통해 2차원으로 투영되면서, 깊이에 대한 차원 정보를 소실한 결과입니다. 이 말은 곧, 투영이라는 과정을 수식적으로 모델링할 수 있고, 2개 이상의 이미지를 통해 깊이를 추정한다면, 실제 3차원 공간 상의 위치 정보를 복원할 수 있다는 의미이기도 합니다.</p>

<p>투영 모델을 구성하는 값 중에서 중요한 요소는, 카메라 파라미터 K와 렌즈왜곡계수인 D입니다. 이 값은 렌즈의 종류, 이미지 센서와의 거리 및 각도 등, 카메라 내부의 기구적 요소에 따라 달라집니다. 그리고 동일한 모델명의 카메라라도, 미세한 제조 공정의 차이로 인해 그 값은 또 달라질 수 있죠. 따라서 모든 카메라마다 K와 D의 값을 정밀하게 측정하고, 이 값을 통해서 보정하는 camera calibration을 수행해야지만, 실제와 가까운 3차원 공간 정보를 획득할 수 있게 됩니다.</p>

<p>투영 모델 개발을 위해, 앞서 설명드린 K와 D 같은 카메라 하드웨어 정보 외에도, 카메라의 위치, 회전각, 카메라 간 상대적인 거리 등이 추가적으로 필요합니다. 최종 결과물의 허용 오차를 수 cm로 낮추기 위해서는, 이런 물리적 정보를 정확하게 측정하는 것 또한, 매우 중요합니다.</p>

<p>그 결과, 왼쪽 하단의 영상에서는 2D 공간에서 사람 위치를, 오른쪽 하단의 영상에서는 3D 공간에서 사람 위치를 확인할 수 있습니다. 사람의 움직임을 더 직관적으로 보여드리고자, 왼쪽 상단의 2D heatmap 과 오른쪽 상단의 이동경로도 함께 표현했습니다.</p>

<p>한편, 다른 모듈과는 달리, MultiView Geometry 는 딥러닝으로 구현하지는 않았습니다. 비전 기술만으로도 충분히 정확한 투영 모델을 만들 수 있었고, 서버에 비해 하드웨어 자원이 부족한 엣지 디바이스에서는, 계산량이 적은 구조의 모델 구현이 더 중요하기 때문입니다.</p>

<h2 id="object-tracking">Object Tracking</h2>

<p>그리고 마지막 단계인 Object Tracker 를 통해 사람을 추적하게 되는데요. 다중 카메라에서 찍은 영상을 실시간으로 분석하면서 추적 기능을 수행하면, 어떤 사람이 언제 들어왔는지, 어디 있었는지, 언제 나갔는지 등을 파악할 수 있게 됩니다.</p>

<p>Object Tracker 도 딥러닝에 기반한 추론 방식을 이용하는데요. 저희 세팅과 비슷한 종류의 트래킹 데이터를 구하는 것은 쉽지가 않았습니다. 따라서, 저희 비전룸에서 촬영된 영상을 학습 자료로 사용하였습니다. 물론, 사전에 크루원들에게 데이터 수집에 대해서는 사전 동의를 얻었습니다.</p>

<p>이 화면에서 볼 수 있듯이, 여러 카메라에 포착된 사람들을 추적해서, 이 사람이 어디에 있는지, 그 동선을 파악할 수 있게 됩니다. 그 결과, 같은 사람은 같은 색의 박스로 표시되는 것을 확인하실 수 있습니다.</p>

<h2 id="system-integration">System Integration</h2>

<p>이 프로젝트의 전체적인 구조는, Nvidia 에서 제공하는 DeepStream 을 이용해 구성했습니다. DeepStream 은 멀티미디어 프레임워크인 Gstreamer를 기반으로 구성되어 있으며, Jetson 에 특화된 다양한 영상처리 기능을 포함하고 있습니다.</p>

<p>저희는 디코딩, 전처리 등의 기능은 DeepStream 이 제공하는 플러그인을, 별도로 필요한 모듈은 Gstreamer 플러그인을 직접 구현해서 사용했습니다. 추론 과정에서는 TensorRT를 이용했는데요. Pytorch 로 구현한 각 모델을 Onnx로, 그리고 이를 다시 TensorRT 로 변환하여 사용했습니다. 여기서는 각 모듈을 통합하고 진행하는 과정을, 순서에 따라 간단한 샘플 코드와 함께 소개해보고자 합니다.</p>

<p>첫 단계인 영상 디코딩은 DeepStream plugin 을 사용하여 진행했는데요. 이 때 동일한 시점에 촬영된 이미지가 있어야, 사람의 위치를 보다 정확하게 추정할 수 있습니다. 문제는, 카메라들의 스트리밍 속도가 제각기 다를 가능성이 크다는 거죠. 이를 해결하기 위해, 여러 카메라 간의 싱크를 맞추는 작업이 필요로 합니다. 먼저 각 카메라의 NTP 서버를 동일하게 설정해주어, 각 카메라의 시간정보를 동기화합니다. 그리고 예제 코드와 같이 “nvstreammux” 라는 플러그인을 생성할 때, “live-source”라는 옵션을 설정하면, 카메라 간의 싱크를 맞출 수 있게 됩니다.</p>

<p>그 다음, DeepStream 플러그인을 이용해 디코딩된 영상 데이터를, TensorRT에서 사용할 수 있는 형태로 포맷을 변환하는 과정을 거칩니다. 추가적으로, 영상 크기를 변환하는 과정에서는, DeepStream API 또는 CUDA 코드를 이용했는데요. 화면에서는 DeepStream API를 이용해, crop and resize 를 하는 예시를 보실 수 있습니다.</p>

<p>그 다음으로 추론 과정을 거치게 되는데, 이 때 미리 변환해둔 TensorRT 모델을 사용하게 됩니다. 각 모듈별로 직접 구현한 Gstreamer 플러그인 내에서, TensorRT API 를 호출하는 방식으로 구성했습니다. 이 과정에서, 앞서 설명드렸던 각 모듈들이 실행 됩니다. 각각의 모듈의 출력은 바로 다음 모듈의 입력이 되기에, 모듈별 추론이 순차적으로 진행될 수 있도록 구성했습니다. 예제 코드에서는 TensorRT API 를 이용해, 추론하는 간단한 과정을 확인할 수 있습니다.</p>

<p>추론을 통해 나온 결과 중에서, 필요한 경우 원하는 결과를 도출할 수 있도록 후처리 과정을 거치게 되는데요. 이 때는 TensorRT에서 제공하는 플러그인을 사용하거나, CUDA 코드로 별도로 구현하게 됩니다. 예제 코드에서는 TensorRT 플러그인 중에서 “Batched NMS” 플러그인을 추가하는 과정을 확인할 수 있습니다.</p>

<p>각각의 모듈로부터 나온 결과를 DeepStream API 를 이용해 metadata 에 저장합니다. 그런 뒤, DeepStream 플러그인을 이용해 metadata 를 시각처리해서 화면에 출력할 수 있습니다. 예시코드는 DeepStream API 를 이용해 Person Detection 의 결과를 metadata 에 저장하는 과정을 보여줍니다.</p>

<h2 id="object-tracking-overview-1">Object Tracking Overview</h2>

<p>앞에서 설명 드린 각각의 모듈들을 모두 하나로 합치면 다음과 같은 형태가 됩니다. 다음 데모 영상을 보면서, 자세한 설명을 드리겠습니다.</p>

<p>가장 먼저, Person Detection 으로 사람을 박스 형태로 찾습니다. 그 다음, Pose Estimation 을 통해 pose 를 추정할 수 있게 되죠. 그 다음 단계에서는 각 카메라마다 여러 사람이 추적되는 모습을 확인해볼 수 있습니다. 그리고 우측 화면에서 Multiview Geometry 를 통해 사람의 매장 내 실시간 위치를 확인할 수 있습니다. 최종적으로, 다중 카메라 간의 사람이 추적되는 것을 볼 수 있습니다.</p>

<p>이렇게 만들어진 카카오엔터프라이즈 내 무인편의점 시스템은, 최대 7명의 움직임을 동시에 추적할 수 있습니다. 앞에서는 자세히 말씀을 드리지 못했지만, 현재 버전에서는 최대 수 kg 까지 측정이 가능하면서, 수 g 단위도 구별이 가능한 무게 센서로, 각 상품의 픽업 여부를 확인할 수 있습니다. 그 뿐만 아니라, 추적된 각 사람의 위치 정보에 기반해, 누가 물건을 들었는지도 포착하고 있죠.</p>

<p><br /></p>

<h1 id="향후-계획">향후 계획</h1>

<p>물론 더 개선해야 할 점도 있습니다. 앞에서도 설명드렸지만, 카메라 배치와 매대로 인해 가림 현상이 심한데요. 이를 대처하기 위한 노력에도 불구하고, 여전히 여러 사람이 밀착하거나 엉켜있는 경우, 그 사람들의 위치 정보를 정확하게 분류하는데 한계가 있습니다. 이 여파로 물건을 잡은 사람이 누군지 정확하게 판단하는 데 있어서도 여전히 오류가 있죠.</p>

<p>이러한 문제를 해결하기위해, 카메라를 추가로 설치하여, 하나의 카메라에서 겹쳐보이는 경우에는, 또 다른 카메라를 통해 구별할 수 있게 만들어보고자 합니다. 특히, 사물이 얼마나 멀리에 위치했는지 알 수 있는 depth camera 의 도입을 검토 중이기도 합니다.</p>

<p>그리고, 각 모듈의 지속적인 경량화 및 최적화를 진행할 예정인데요. 그렇게 되면, 하나의 엣지 디바이스에서 처리할 수 있는 카메라가 늘어나고, 동시에 추적 가능한 사람의 수가 늘어날 것 입니다. 결과적으로, 전체 시스템에 필요한 엣지 디바이스의 수가 적어지기 때문에, 시스템 구축 비용이 감소하게 될 것입니다. 이것은 사업성과도 밀접한 연관이 있어서, 지속적으로 경량화 및 최적화를 진행하고자 합니다.</p>

<p>한편, 우리는 Pose Estimation 에서 검출된 결과를 통해, 사람의 자세를 판단할 수 있습니다. 그 중에서, 물건을 잡는 자세를 선별할 수 있다면, 누가 물건을 잡았는지를 정확히 알 수 있겠죠. 하지만 사람이 물건을 잡는 자세는 매우 다양하기 때문에, 이 검출된 결과와 더불어 손 부분의 영상을 사용하여, 사물 픽업 여부 판단을 고려하고 있습니다. 현재 PoC 단계에서는 적용되지 않은 기술로, 추후 검토를 진행할 계획입니다.</p>]]></content><author><name>천각</name></author><category term="deepdive" /><category term="person detector" /><category term="pose estimator" /><category term="multi-view geometry" /><category term="person tracker" /><summary type="html"><![CDATA[저희는 '편의점'이라는 가상의 공간을 만들어서, 그 안에서 소형의 엣지 디바이스만으로 사람을 추적하는 것을 목표로 개발을 진행하였습니다. 오늘은 바로, 저희가 이 시스템을 어떻게 설계했는지에 대해서 간략하게 소개해보려 합니다.]]></summary></entry><entry><title type="html">정답 유형을 분류하는 딥러닝 기술</title><link href="https://genius0928.github.io//deepdive/200724" rel="alternate" type="text/html" title="정답 유형을 분류하는 딥러닝 기술" /><published>2020-07-24T00:00:00+09:00</published><updated>2020-07-24T00:00:00+09:00</updated><id>https://genius0928.github.io//deepdive/200724</id><content type="html" xml:base="https://genius0928.github.io//deepdive/200724"><![CDATA[<p>현대인은 자신이 원하는 정보를 찾는 데 점차 많은 어려움을 느끼고 있습니다. 언제 어디서나 경제적인 부담없이 편리하게 정보를 습득할 수 있는 인터넷이 가진 장점과는 별개로, 유용한 정보에 접근하는 데에는 물리적인 제약이 따르기 때문입니다. 모르거나 모를 수밖에 없는 정보량이 압도적으로 많이 생산되고 있어 특정 상황과 조건에 따른 답을 파악하기가 쉽지 않죠. 이런 이유로 부정확하거나 잘못된 정보를 습득할 가능성도 이전보다 더 높아짐은 물론, 검색 정보를 이해하고 활용하는 수준이 낮아서 발생하는 새로운 형태의 불평등도 야기되고 있습니다.</p>

<p>카카오엔터프라이즈 AI Lab(이하 AI Lab)은 사용자에게 도움이 되는 정보를 효율적으로 찾아주는 검색 시스템이 필요하다고 판단, 자사 인공지능 기술을 집약한 플랫폼인 카카오 i 대화 엔진<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>을 점진적으로 개선해나가고 있습니다. AI Lab은 그 중에서도 음성, 키보드와 같은 다양한 인터페이스에 입력된 사용자 질의<sup>query</sup>에 대한 답변을 포함하는 후보 문서에서 정답 부분을 추출하는 ‘질의 응답 기술’([그림 1])의 연구개발에 특히 힘쓰고 있습니다.</p>

<p><img src="/assets/img/2020-07-24-200724/001.png" width="" align="" /></p>

<p><em class="center">[그림 1] 질의 응답 시스템의 동작 과정</em></p>

<p>이번 글에서는 자사 질의 응답 기술 요소 중 하나인 딥러닝 기반 정답 유형(서술형, 단답형) 분류 모델<a href="#bde9:rf:1" class="reference" id="bde9:rf-back:1">[1]</a>을 소개해보고자 합니다. AI Lab AI기술팀 자연어처리파트의 조승우 연구원을 만나 자세한 이야기를 들어봤습니다.</p>

<p><br /></p>

<p class="notice">※이번 글은 조승우 연구원과 최동현 연구원이 2019년 7월부터 2020년 2월까지 진행한 연구를 바탕으로 내용을 구성했습니다.</p>

<p><br /></p>

<p class="tech-ground">☛ Tech Ground 데모 페이지 바로 가기 : <b><a href="https://labs.kakaoi.ai/mrc">지문분석</a></b> 데모, <b><a href="https://labs.kakaoi.ai/openqa">통합분석</a></b> 데모</p>

<p><br /></p>

<p class="dot-line">∙  ∙  ∙  ∙  ∙  ∙  ∙</p>

<p><br /></p>

<h1 id="질의-응답-시스템이란">질의 응답 시스템이란</h1>

<p>카카오 i 대화 엔진은 사용자 질의 분석을 통해 호출 의도에 따른 적합한 시스템을 가동합니다. [표 1]에서처럼 인텐트 분류 단계를 거친 사용자 발화가 답변을 요구할 때 질의 응답 시스템이 활성화된다고 보시면 됩니다. 이 내용은 ‘<a href="https://tech.kakaoenterprise.com/43">카카오미니의 명령어 분류 방법</a>’이라는 글에 서술된 대화 엔진의 동작 방법에서 좀 더 자세히 살펴볼 수 있습니다.</p>

<table>
  <tr class="key">
    <td>질의</td>
    <td>답변</td>
  </tr>
  <tr>
    <td>토사구팽이 뭐야?</td>
    <td>필요할 때 이용하다가 필요가 없어지면 버리는 일을 비유한 사자성어</td>
  </tr>
  <tr>
    <td>소방서 전화번호가 뭐야?</td>
    <td>119</td>
  </tr>
  <tr>
    <td>아시아나 온라인 체크인은 언제부터 가능해?</td>
    <td>출발 48시간 전부터</td>
  </tr>
  <tr>
    <td>이효리가 속한 그룹 이름이 뭐야?</td>
    <td>핑클</td>
  </tr>
  <tr>
    <td>올림픽 공원은 어디에 있어?</td>
    <td>서울 송파구 올림픽로 424</td>
  </tr>
  <tr>
    <td>교보문고 강남 어디에 있어?</td>
    <td>서울특별시 서초구 강남대로 465</td>
  </tr>
  <tr>
    <td>김태희 누구랑 결혼했어?	</td>
    <td>비</td>
  </tr>
  <tr>
    <td>올림픽 공원은 어디에 있어?</td>
    <td>서울 송파구 올림픽로 424</td>
  </tr>
</table>
<p><em class="center">[표 1] 답변을 요구하는 사용자 질의 예시</em></p>

<p>질의 응답 시스템의 작동 과정은 다음과 같습니다. 첫 번째, 입력 문장을 분석해 검색에 유용한 쿼리를 생성합니다. 두 번째, 추출된 검색 쿼리를 활용해 찾은 모든 문서를 기계 독해에 적합한 형태로 잘게 쪼갭니다. 세 번째, 잘게 쪼갠 텍스트에서 후보 정답을 추출합니다. 네 번째, 후보 정답 중 질문에 가장 적합한 정답을 제공합니다.</p>

<p>마지막 네 번째 단계에서 질의 응답 시스템은 질문과 가장 연관성이 높은 N개의 문서를 분석해 얻은 <b>1)</b> 서로 다른 N개의 정답을 각각 제시하거나, 또는 <b>2)</b> 여러 후보군에서 가장 적합한 단일 정답을 제시할 수 있습니다. 스마트 스피커와 같은 음성 인터페이스나 스마트폰과 같은 소형 디스플레이에서는 텍스트를 쪼갠 수만큼 늘어난 추출된 모든 정답을 사용자에게 그대로 제공하는 방식은 사용자에게 큰 불편함을 초래할 수 있습니다. 이런 이유로 하나의 정답을 제공하는 두 번째 방식이 보편적으로 활용되고 있습니다.</p>

<p>문제는 수많은 후보군 중 하나의 최종 정답을 제공하는 질의 응답 시스템에서는 하나의 알고리즘으로 서로 다른 정답 유형을 처리하기가 쉽지 않다는 데 있습니다.</p>

<table>
  <tr class="key">
    <td width="15%">질의</td>
    <td width="25%">마우스 발명자는 누구?
    </td>
    <td width="60%">컴퓨터가 뭐야?
    </td>
  </tr>
  <tr>
    <td class="key">후보 정답 1</td>
    <td>더글러스 엔젤바트</td>
    <td class="left">미리 정해진 방법에 따라 입력된 자료를 처리함으로써 문제를 해결하는 다양한 형태의 전자공학적 자동장치</td>
  </tr>
  <tr>
    <td class="key">후보 정답 2</td>
    <td>더글러스 엥겔바트</td>
    <td class="left">반도체 집적 회로를 이용해 주어진 명령을 자동으로 맡아 실행하는 정보 처리기</td>
  </tr>
  <tr>
    <td class="key">후보 정답 3</td>
    <td>더글라스 엥겔바트</td>
    <td class="left">프로그램을 이용해 정보를 처리하는 장치</td>
  </tr>
  <tr>
    <td class="key">후보 정답 4</td>
    <td>더글라스 엥겔바트</td>
    <td class="left">전자 회로를 이용한 고속 자동 계산기</td>
  </tr>
  <tr>
    <td class="key">후보 정답 5</td>
    <td>더글러스 엥겔바트</td>
    <td class="left">프로그램에 따라 작업이나 계산을 수행하는 기계</td>
  </tr>
</table>
<p><em class="center">[표 2] 단답형 정답과 서술형 정답을 요구하는 질의 예시</em></p>

<p>[표 2]에서 보듯이, “마우스 발명자는 누구”라는 질의에 따라 각 문서에서 추출한 후보군은 대부분 비슷한 문형<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>을 지니며 그 표현 범위 또한 다소 한정돼 있습니다. 그래서 그 후보를 압축하는 데 도움이 될만한 정답 사이 포함 관계나 날짜, 요일과 같은 규칙성에 따른 투표를 통해 최종 정답을 가려낼 수 있죠. 이를 단답형 정답이라고 부릅니다.</p>

<p>반면, “컴퓨터가 뭐야”라는 질의에 따라 각 문서에서 추출한 후보군은 다양한 문형을 지니며 그 표현 범위가 상당히 넓습니다. 후보 답변 사이 규칙성을 발견하기 어려워 앞서 설명한 투표 알고리즘을 사용하기가 어렵습니다. 그래서 각 정답 후보에서 추출한 중심어와 수식어에 따른 <a href="https://mk-v1.kakaocdn.net/dn/if-kakao/conf2019/conf_video_2019/2_104_03_m1.mp4">새로운 투표 알고리즘</a>이 필요합니다. 이를 서술형 정답이라고 부릅니다.</p>

<p>이처럼 질의 응답 시스템에서는 정답의 유형에 따른 적절한 알고리즘이 동작해야 한다는 점에서 정답 유형 분류는 매우 중요하다고 할 수 있겠습니다.</p>

<p><br /></p>

<h1 id="정답-유형-분류가-어려운-이유">정답 유형 분류가 어려운 이유</h1>

<p>앞서 설명한 정답 유형 인식은 ‘단답형’과 ‘서술형’이라는 이진 분류 문제라고 볼 수 있습니다. 얼핏 봐서는 질문의 길이가 짧고 간단해 답변의 유형을 둘 중 하나로 분류하기가 매우 쉽다고 느껴질 수도 있습니다. 하지만 실제로는 질의의 형태만 봐서는 정답 유형을 구분하기가 거의 불가능합니다. 같은 문형을 가진 질문임에도 그 정답의 유형이 완전히 다름을 알 수 있는 예를 한 번 들어보겠습니다.</p>

<table>
  <tr class="key">
    <td width="8%">번호</td>
    <td width="30%">질의</td>
    <td width="15%">정답 유형</td>
    <td width="47%">정답</td>
  </tr>
  <tr>
    <td>1</td>
    <td>경찰서 번호가 뭐야?</td>
    <td>단답형</td>
    <td>112</td>
  </tr>
  <tr>
    <td>2</td>
    <td>원자 번호가 뭐야?</td>
    <td>서술형</td>
    <td>서술형 주기율표에서 각 원소마다 주어진 원소 고유의 순번</td>
  </tr>
</table>
<p><em class="center">[표 3] 질의 형태가 비슷한 사례</em></p>

<p>[표 3]에서 1번과 2번 문장은 ‘~번호가 뭐야’라는 동일한 문형으로 종결됩니다. 1번은 ‘경찰서’라는 주제어의 하위 속성에 대한 질문입니다. 따라서 단답형으로 정답이 제공되어야 합니다. 원자 번호라는 주제어에 대한 정의를 질의하는 2번에서는 서술형 정답이 제공되어야 하죠.</p>

<p>특정 단어 시퀀스에 대한 정답 유형의 단답형 또는 서술형 확률을 구할 때에는 형태소<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>, 품사와 같은 정보를 고려합니다. 그런데 품사 정보는 문장을 두 범주 중 하나로 분류하는 데 크게 도움이 되지 않습니다. 즉, ‘번호가 뭐야’라는 동일한 문형을 가진 두 개의 문장에서 주제어의 형태소만 다른 상황에서는 해당 문장이 요구하는 정답이 단답형일 확률 또는 서술형일 확률이 서로 크게 다르지 않다는 의미입니다. 이런 이유로 단순한 키워드 나열이나 단문 형태를 취하는 질의 분석만으로는 예상 정답의 유형을 분류하기가 쉽지 않습니다.</p>

<p>질의 형태의 변형(경찰서 번호가 뭐야, 경찰서 번호 좀 알려줘봐)이나 입력 오류(경칠서 번호가 뭐야, 경찰서 반호가 뭐야) 등 수제 규칙이나 패턴에서 벗어난 질문에 대해서도 적절한 정답을 제공하지 못할 수도 있습니다. 질의를 제대로 분석하지 못해 정답 유형에 따른 적합한 알고리즘을 제대로 호출하지 못하게 되기 때문입니다. 이처럼 대다수의 서비스 이용자가 자신만의 스타일로 문장을 입력한다는 사실을 고려했을 때 패턴이나 규칙에서 벗어난 질의 문장 또한 제대로 분류할 수 있는 강건한 시스템이 필요합니다.</p>

<p><br /></p>

<h1 id="bert-언어-모델을-사용하지-않는-이유">BERT 언어 모델을 사용하지 않는 이유</h1>

<p>기존의 언어 모델에서는 순차 데이터<sup>sequential data</sup>를 다루기 용이한 RNN<sup>recurrent neural network</sup><sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup> 구조에 주로 기반을 두었습니다. 하지만 이로 인해 병렬 처리<sup>parallel processing</sup>가 어려워 계산 속도가 느려지고, 입력 초기의 데이터를 잊어버리는 경향으로 인해 문장 길이가 길어질수록 성능도 떨어지게 됩니다. 어텐션<sup>attention</sup><sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>을 통해 위에 언급한 문제가 일정 부분 해소되기는 했으나, 그 단점을 완전히 극복하지는 못했습니다.</p>

<p>RNN의 한계를 넘어서고자 새롭게 고안된 Transformer<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>는 거리가 먼 단어 간 관계를 효과적으로 표현하는 셀프 어텐션<sup>self-attention</sup><sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>을 이용해 학습 시간은 줄이고, 학습 성능은 효과적으로 높였습니다. 이후에 나온 BERT, GPT2를 비롯한 현재 최고 수준의(SOTA<sup>State-Of-The-Art</sup>) 언어 모델은 바로 이 Transformer 구조에 기반을 둡니다. 문맥에 따라 변하는 단어의 의미를 표현하는 데 탁월한 최신의 이 언어 모델은 다양한 NLP 과제에서 기대 이상의 성능을 선보이고 있습니다.</p>

<p>하지만 이런 낙관론과는 달리, 실제 서비스 개발 현장에서는 속도 저하를 문제로 최신 언어 모델을 도입하는 사례가 많지 않습니다. 223GB의 한국어 문장을 가지고 CNN<sup>Convolutional Neural Networks</sup><sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup> 기반 모델과 BERT 기반 모델<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>의 성능을 비교한 실험이 이를 뒷받침합니다. 성능 차는 거의 없었으나 처리 속도는 수배 이상 차이 났기 때문입니다.</p>

<p>보통은 Eigen<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>을 이용해 디코더<sup>decoder</sup>를 구현하면 속도를 최적화할 수 있습니다. 다만 모델의 층수가 많아질수록 그 이점은 줄어듭니다. 그래서 BERT처럼 층이 많고 그 구조가 복잡한 모델에서는 서비스가 가능한 수준의 속도 최적화는 매우 어려운 편에 속합니다. 실제로 BERT 기반 모델은 서버 GPU를 사용하고도 한 문장을 분류하는 데에만 9.3280ms나 걸립니다. 반면, CNN 기반 모델에서는 CPU 싱글코어만으로도 한 문장 분류에 2ms밖에 걸리지 않았죠.</p>

<p>AI Lab은 비슷한 정확도를 가진 모델인데, 훨씬 값싸고 빠르게 사용할 수 있다면 공학적인 측면에서 마다할 이유가 없다고 판단, BERT 기반 언어 모델 대신 GloVe<a href="#bde9:rf:2" class="reference" id="bde9:rf-back:2">[2]</a> 의미 벡터와 어절 임베딩 벡터를 CNN에 입력해 그 문맥을 학습할 수 있도록 했습니다<a href="#bde9:rf:1" class="reference" id="bde9:rf-back:1">[1]</a>. CNN 기반 모델을 채택한 이유는 뒤에서 자세히 설명해드리겠습니다.</p>

<p><br /></p>

<h1 id="ai-lab이-정답-유형을-분류하는-방법">AI Lab이 정답 유형을 분류하는 방법</h1>

<h2 id="1-전처리">1. 전처리</h2>

<p>한국어 어절<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">11</a></sup> 하나는 두 개 이상의 형태소를 포함하는 경우가 많습니다. 이런 이유로 기존 한국어 관련 많은 연구에서는 한국어 형태소 분석기를 이용해 입력 문장을 형태소 단위로 나누는 데이터 전처리를 실시합니다. AI Lab 또한 규칙과 통계 기반으로 동작하는 자체 한국어 형태소 분석기<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">12</a></sup>를 이용해 입력 문장에서 형태소와 품사 정보를 추출했습니다.</p>

<p>하지만 AI Lab은 형태소를 분석하는 것만으로는 오탈자 여부를 알기가 쉽지 않다고 판단했습니다. 사용자는 비슷하게 발음되는 두 단어를 혼동하거나, 키보드상에서 가까이 위치하는 키를 잘못 타이핑하거나, 단순히 띄어쓰기를 생략할 수도 있습니다. 대부분은 정상적인 표현에서 자음, 모음 한두 개만 달랐습니다. 이에 AI Lab은 자음과 모음 추출기를 이용해 어절의 자모음 정보도 획득했습니다. 이 자모음 정보는 뒤에서 설명할 통합 어절 임베딩(행렬) 생성에 활용됩니다.</p>

<p><img src="/assets/img/2020-07-24-200724/002.png" width="" align="" /></p>

<p><em class="center">[그림 2] 전처리 및 통합 어절 임베딩 생성 과정</em></p>

<p><br /></p>

<h2 id="2-통합-어절-임베딩-생성">2. 통합 어절 임베딩 생성</h2>

<p>자연어 어절을 벡터로 바꾸는 데에는 워드 임베딩<sup>word embedding</sup>을 사용합니다. 워드 임베딩은 대규모 말뭉치에 등장하는 단어의 일반적인 의미를 벡터로 표현합니다. 예를 들어, 의미가 비슷한 ‘인간’과 ‘사람’을 비슷한 방식으로 표현할 수 있습니다. 또 ‘왕-남자=왕비’와 같은 관계에서 보듯이 단어의 실제적 의미적 차이를 거리로 표현하는 것도 합니다. 대표적인 워드 임베딩 모델로 Word2Vec<a href="#bde9:rf:3" class="reference" id="bde9:rf-back:3">[3]</a>, GloVe<a href="#bde9:rf:2" class="reference" id="bde9:rf-back:2">[2]</a>, FastText<a href="#bde9:rf:4" class="reference" id="bde9:rf-back:4">[4]</a>가 있습니다.</p>

<p>AI Lab은 여타 다른 임베딩 기법보다 단어의 의미를 더 잘 담아내는 GloVe 포함한, 입력 오류에 강건한 (자모) 통합 어절 임베딩 생성기를 구축했습니다. 형태소 어휘 사전에 존재하지 않은 오탈자로 인해<sup>out of vocabulary</sup> 형태소 분석 오차<sup>loss</sup>를 최소화하기 위함입니다. 통합 어절 임베딩 생성기는 형태소와 자모음 정보에 각각 부여한 임베딩 벡터를 이용해 다시 원래 어절을 조합하는 과정을 학습합니다. 그 결과, 입력 오류가 발생하더라도 원래 어절을 유사하게 추정할 수 있습니다.</p>

<p>AI Lab은 통합 어절 임베딩 생성기 훈련 과정에서 자모 드롭아웃<sup>dropout</sup><sup id="fnref:13" role="doc-noteref"><a href="#fn:13" class="footnote" rel="footnote">13</a></sup>과 띄어쓰기 없는 문장 생성이라는 두가지 데이터 노이즈<sup>noise</sup> 생성 기법을 추가했습니다. 기존 시스템 대비 오류가 포함된 문장을 분류하는 성능이 약 23%P 향상되는 등<a href="#bde9:rf:1" class="reference" id="bde9:rf-back:1">[1]</a> 이 기법의 유효성은 AI Lab 자체 실험을 통해서도 증명이 됐습니다.</p>

<p><br /></p>

<h2 id="3-문장-임베딩-벡터-생성">3. 문장 임베딩 벡터 생성</h2>

<p>문장에 존재하는 각 어절을 표현하는 임베딩을 추출했다면, 이를 바탕으로 문맥까지 표현하는 임베딩 벡터를 생성할 차례입니다. 여기에는 CNN 기반 모델이 활용됩니다.</p>

<p><img src="/assets/img/2020-07-24-200724/003.png" width="" align="" /></p>

<p><em class="center">[그림 3] 문장 임베딩 생성 및 분류</em></p>

<h3 id="1-densenet">(1) DenseNet</h3>

<p>이미지 분류<sup>classification</sup>와 분할<sup>segmentation</sup>, 객체 감지<sup>detection</sup>과 같은 비전 문제에서 탁월한 성능을 내는 CNN은 자연어처리 문제에도 효과적입니다. 컨볼루션 연산층<sup>convolution layer</sup>의 필터<sup>filter</sup>가 문맥 파악에 중요한 부분만 도출하는 데 유리한 덕분입니다. 그 결과, CNN을 통과한 최종 벡터는 문장의 지역 정보를 보존하는 추상화 과정을 거쳐 단어나 표현의 등장 순서를 반영한 문장의 의미 정보<sup>semantic information</sup>를 표현할 수 있게 됩니다.</p>

<p>AI Lab이 문장 분류를 위해 기본으로 사용한 DenseNet<a href="#bde9:rf:5" class="reference" id="bde9:rf-back:5">[5]</a>은 CNN을 응용한 대표 알고리즘으로, 이전 층에서 생성된 특징 맵<sup>feature map</sup><sup id="fnref:14" role="doc-noteref"><a href="#fn:14" class="footnote" rel="footnote">14</a></sup>을 모두 이어붙여서<sup>concatenation</sup> 다음층에 입력합니다. 컨볼루션 층이 많아질수록 출력 층에서부터 계산된 기울기<sup>gradient</sup>가 전체 네트워크에 올바르게 전달되지 않는 기울기 소실<sup>gradient vanishing</sup><sup id="fnref:15" role="doc-noteref"><a href="#fn:15" class="footnote" rel="footnote">15</a></sup> 문제를 완화하는 이 로직이 자연어처리에서도 효과적인 방법이라고 AI Lab은 판단했습니다.</p>

<h3 id="2-깊이별-분리-컨볼루션-연산depthwise-separable-convolution">(2) 깊이별 분리 컨볼루션 연산<sup>depthwise separable convolution</sup></h3>

<p>형태소와 자모 정보를 반영한 통합 어절 임베딩을 모델에 바로 입력하면 전체 학습 시간이 지나치게 길어지는 문제가 발생할 수 있습니다. 각 임베딩별로 컨볼루션 연산이 이뤄지다 보니 매개변수<sup>parameter</sup> 수가 늘어나는 만큼 처리 시간이 비례해서 늘어나기 때문입니다. 따라서 매개변수가 늘어나는 상황에 대비해 학습 속도나 추론 속도를 높일 필요가 있습니다.</p>

<p>이를 해결하고자 AI Lab은 깊이별 분리 컨볼루션 연산<a href="#bde9:rf:6" class="reference" id="bde9:rf-back:6">[6]</a>을 이용합니다. 2D 이미지 데이터에 대한 깊이별 분리 컨볼루션은 매개변수 수 최적화를 통해 메모리 사용량은 줄이고 학습 속도를 높입니다. 채널을 기준으로 각각 [필터 높이, 필터 너비, 1]와 [1, 1, 채널 수]로 분리한 두 종류의 새로운 필터로 각각 깊이별 컨볼루션<sup>Depthwise convolution</sup>과 포인트별 컨볼루션<sup>Pointwise convolution</sup> 연산을 순차적으로 진행합니다.</p>

<p>이 연구에서 깊이별 컨볼루션은 어절 간 관계를 고려합니다. 주위 어절을 함께 고려한다는 측면에서 n-gram<sup id="fnref:16" role="doc-noteref"><a href="#fn:16" class="footnote" rel="footnote">16</a></sup> 확률을 분류에 사용하는 것과 비슷하다고 보면 됩니다. 포인트별 컨볼루션은 유효 데이터만 추려내고자 전체 채널을 하나로 압축해 컨볼루션 연산을 진행합니다. 이렇게 하면 연산 속도를 높이는 데 도움이 됩니다.</p>

<p>\(300\)차원의 통합 어절 임베딩 벡터와 \(300×3\) 크기의 필터 256개를 이용한 컨볼루션 연산<sup id="fnref:17" role="doc-noteref"><a href="#fn:17" class="footnote" rel="footnote">17</a></sup>이 있다고 가정하겠습니다. 일반적인 필터를 이용한 \(1\)회의 컨볼루션 연산에는 <b>입력 채널 수×필터 높이×필터 너비×출력 채널 수</b>만큼의 매개변수가 필요합니다. 여기서는 \(300×1×3×256=230,400\)개의 매개변수를 연산해야 하죠. 반면, 깊이별 분리 컨볼루션에서는 각각 \(300×1×3×1=900\)개와  \(300×1×1*×256=76,800\)개의 매개변수가 필요합니다. 이를 합치면 총 \(77,700\)개로, 연산 효율이 \(33.7\)% 더 좋다는 사실을 확인해볼 수 있습니다.</p>

<p><img src="/assets/img/2020-07-24-200724/004.png" width="80%" align="" /></p>

<p><em class="center">[그림 4] 기존 컨볼루션 연산과 깊이별 분리 컨볼루션에 사용하는 필터 예시</em></p>

<h3 id="3-동적-셀프-어텐션dynamic-self-attention">(3) 동적 셀프 어텐션<sup>dynamic self-attention</sup></h3>

<p>서로 연관성이 높으나 거리상 멀리 떨어진 어절이 서로 참조할 수 있게 하는 기법으로 어텐션이 있습니다. 다만 행렬의 형태가 어절 수에 제약을 받는 기존 방식은 입력 발화 길이에 제약이 없는 실제 서비스에는 적합하지 않을 수도 있습니다. 이에 AI Lab은 입력 어절 수에 관계없이 어텐션 벡터를 자유롭게 계산하고 관리하는 기법인 동적 셀프 어텐션<a href="#bde9:rf:7" class="reference" id="bde9:rf-back:7">[7]</a>을 적용했습니다.</p>

<p><img src="/assets/img/2020-07-24-200724/005.png" width="" align="" /></p>

<p><em class="center">[그림 5] 기존 어텐션 기법</em></p>

<p>과정은 다음과 같습니다. 첫 번째, 각 어절 임베딩 벡터와 셀프 어텐션을 나타내는 동적 가중치 벡터<sup>dynamic weight vector</sup>를 곱해 각 어절의 문맥 점수를 구합니다. 두 번째, 각 문맥 점수에 대한 소프트맥스<sup>softmax</sup><sup id="fnref:18" role="doc-noteref"><a href="#fn:18" class="footnote" rel="footnote">18</a></sup> 연산을 거치면 중요도 점수(소프트맥스 확률값)를 얻게 됩니다. 세 번째, 어절 임베딩 벡터에 중요도 점수를 가중치로 두고 선형 결합<sup>linear combination</sup>합니다. 네 번째, 이 가중합의 결과는 다시 동적 가중치 벡터로 재정의됩니다. 이 과정을 거치면 어절 길이에 제약을 가진 가중치 행렬을 사용하지 않으면서도 현재 보는 어절과 관련성이 높은 다른 어절의 중요도도 반영할 수 있게 됩니다.</p>

<p><img src="/assets/img/2020-07-24-200724/006.png" width="" align="" /></p>

<p><em class="center">[그림 6] 동적 셀프 어텐션 과정</em></p>

<p><br /></p>

<h2 id="4-분류">4. 분류</h2>

<p>문장 임베딩 벡터는 마지막 출력층인 FFNN<sup>Feed-Forward Neural Network</sup>과 소프트맥스 연산을 거쳐 사용자 질의문이 요구하는 답변 유형을 단답형 또는 서술형으로 분류합니다.</p>

<h1 id="향후-계획">향후 계획</h1>

<p>AI Lab은 예기치 않은 오분류 문장을 완전히 제어할 수 있어야 비로소 사용자에게 충분한 만족감을 선사하는 서비스를 제공할 수 있다고 보고 있습니다. 지금까지는 육하원칙, 개체명 인식기, 가젯티어<sup id="fnref:19" role="doc-noteref"><a href="#fn:19" class="footnote" rel="footnote">19</a></sup>와 같은 추가 특징을 딥러닝 분류 모델에 적용했을 때 유의미한 성능 개선을 확인했습니다. 정답 유형 분류에 맞게 미세조정된<sup>fine-tuning</sup>된 BERT 모델을 전문가(teacher)로, CNN을 숙련자(student)로 설정하는 실험에서는 괄목할만한 성능 개선을 이뤘으며 현재 그 결과를 정리하고 있습니다. 이처럼 AI Lab은 정답 유형 분류기의 성능을 개선하기 위한 연구를 앞으로도 계속 진행할 계획입니다.</p>

<p><br /></p>

<h1 id="참고-문헌">참고 문헌</h1>

<p><a id="bde9:rf:1" class="referencebody"><a href="#bde9:rf-back:1" class="backlink">[1]</a>  한국어 챗봇에서의 오류에 강건한 한국어 문장 분류를 위한 어절 단위 임베딩 (2019) by 최동현, 박일남, 신명철, 김응균, 신동렬 in 제 31회 한글 및 한국어 정보처리 학술대회 논문집(pp. 43-48) </a><br /></p>

<p><a id="bde9:rf:2" class="referencebody"><a href="#bde9:rf-back:2" class="backlink">[2]</a>  <a href="https://www.aclweb.org/anthology/D14-1162.pdf">GloVe: Global Vectors for Word Representation</a> by Jeffrey Pennington, Richard Socher, Christopher D. Manning, in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) </a><br /></p>

<p><a id="bde9:rf:3" class="referencebody"><a href="#bde9:rf-back:3" class="backlink">[3]</a>  <a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a> (2013) by Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean in ICLR Workshop </a><br /></p>

<p><a id="bde9:rf:4" class="referencebody"><a href="#bde9:rf-back:4" class="backlink">[4]</a>  <a href="https://arxiv.org/abs/1607.01759">Bag of Tricks for Efficient Text Classification</a> by  Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov in arXiv </a><br /></p>

<p><a id="bde9:rf:5" class="referencebody"><a href="#bde9:rf-back:5" class="backlink">[5]</a>  <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf">Densely Connected Convolutional Networks</a> (2017) by Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger in CVPR </a><br /></p>

<p><a id="bde9:rf:6" class="referencebody"><a href="#bde9:rf-back:6" class="backlink">[6]</a>  <a href="https://arxiv.org/abs/1704.04861">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a> (2017) by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam in CVPR </a><br /></p>

<p><a id="bde9:rf:7" class="referencebody"><a href="#bde9:rf-back:7" class="backlink">[7]</a>  <a href="https://arxiv.org/abs/1808.07383">Dynamic Self-Attention: Computing Attention over Words Dynamically for Sentence Embedding</a> (2018) by Deunsol Yoon, Dongbok Lee, SangKeun Lee in arXiv </a><br /></p>

<p><br /></p>

<h1 id="각주">각주</h1>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>사용자의 발화를 인식해 적절한 서비스를 연결해주는 자연어처리기술 <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>문장을 구성하는 문장성분의 배열 유형 <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>뜻을 지니는 최소의 단위 <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>N 단계에서의 값을 다시 입력값으로 사용해 N+1단계에서의 상태를 예측하는 재귀적<sup>recursive</sup>인 구조를 갖춘 신경망 <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>인코더-디코더 구조의 모델이 특정 시퀀스를 디코딩할 때 관련된 인코딩 결과값을 참조한다. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>컨볼루션<sup>convolution</sup>이나 순환<sup>recurrence</sup> 기법 대신, 모든 단어가 현재 결과에 기여하는 정도를 반영할 수 있도록 각 입력 단어가 출력 상태에 연결하는 어텐션<sup>attention</sup> 신경망 구조를 활용한 seq2seq 모델이다. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>현재 처리하는 단어와 연관성이 높은 단어를 참조하는 기법 <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>이미지의 공간 정보를 유지하면서 특징을 효과적으로 인식하고 강조하는 딥러닝 모델. 특징을 추출하는 영역은 컨볼루션 층과 풀링 층으로 구성된다. 컨볼루션 층은 필터를 사용해 공유 파라미터 수를 최소화하면서 이미지의 특징을 찾는다. 풀링 층은 특징을 강화하고 모은다. <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>형태소 분석기 대신 최신 단어 분절 알고리즘인 BPE를 사용했다. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>C++ 선형대수 연산 라이브러리 <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p>문장 성분의 최소 단위. 띄어쓰기의 단위가 된다. <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:12" role="doc-endnote">
      <p>AI lab은 뜻을 지니는 최소 단위인 형태소로 우선 안정적으로 분류 실험을 진행하고 나서, 서브워드<sup>subword</sup> 단위로 문장을 분절하는 최신 기법인 BPE를 활용한 연구도 진행하고 있다. <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:13" role="doc-endnote">
      <p>특정 자모가 포함된 임베딩 벡터값을 전부 0으로 변환하는 기법 <a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:14" role="doc-endnote">
      <p>컨볼루션 연산으로 얻은 결과 <a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:15" role="doc-endnote">
      <p>역전파 알고리즘에서는 낮은 층으로 갈수록 전파되는 오류<sup>error</sup>의 양이 적어진다. 이로 인해 미분값의 변화가 거의 없어져 학습이 제대로 일어나지 않는다. <a href="#fnref:15" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:16" role="doc-endnote">
      <p>어떤 한 입력을 처리할 때 N개의 입력 단위(토큰<sup>token</sup>)를 함께 볼지를 결정한다. 이 글에서는 통합 어절 임베딩을 생성하기에 어절을 하나의 입력 단위로 취급한다. <a href="#fnref:16" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:17" role="doc-endnote">
      <p>패딩<sup>padding</sup>은 같게, 스트라이드<sup>stride</sup>는 1인 조건에서 컨볼루션 연산 20회를 수행한다. <a href="#fnref:17" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:18" role="doc-endnote">
      <p>다범주 분류기에서는 출력값을 제대로 해석하고자 다른 출력 노드와의 상대적인 크기를 비교한다. 이를 위해 각 출력 노드를 0~1 사이로 제한해 이를 합한 값을 1로 만든다. <a href="#fnref:18" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:19" role="doc-endnote">
      <p>사람이 관리하는 사전 정보로, 보통은 CRF 같은 알고리즘에서 기계학습 성능이 더 높게 나올 수 있도록 수치값을 조정해주는 역할을 맡고 있다. <a href="#fnref:19" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>samantha:작성,편집</name></author><category term="deepdive" /><category term="QA" /><summary type="html"><![CDATA[사용자 질의에 적절한 답을 제시해주는 검색 시스템에서 후보 정답을 추출하는 방식은 정답의 유형에 따라 다릅니다. 단답형의 정답은 대부분 그 형태가 비슷하다 보니 일종의 규칙에 기반한 투표 알고리즘을 통해 최종 정답을 선별할 수 있습니다. 하지만 서술형의 정답은 제각기 표현 방식이 달라 전과는 다른 새로운 투표 알고리즘이 동작해야 합니다. 질의만 보고 그 정답의 유형(단답형, 서술형)을 분류하는 딥러닝 기술에 대한 중요성이 필요해지는 이유입니다.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://genius0928.github.io//assets/img/2020-07-24-200724/000.jpg" /><media:content medium="image" url="https://genius0928.github.io//assets/img/2020-07-24-200724/000.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">얼굴 인식 알고리즘 선행 연구를 소개합니다</title><link href="https://genius0928.github.io//deepdive/200723" rel="alternate" type="text/html" title="얼굴 인식 알고리즘 선행 연구를 소개합니다" /><published>2020-07-23T00:00:00+09:00</published><updated>2020-07-23T00:00:00+09:00</updated><id>https://genius0928.github.io//deepdive/200723</id><content type="html" xml:base="https://genius0928.github.io//deepdive/200723"><![CDATA[<p>얼굴 인식 기술<sup>face recognition</sup>은 지난 수십 년간 컴퓨터 비전<sup>computer vision</sup>의 주요 연구 분야 중 하나로 자리매김하고 있습니다. [그림 1]처럼 시스템에 입력된 두 이미지 속 인물 간의 동일인 여부를 검증<sup>verification</sup>하거나, 이미지 속 인물이 내부 데이터베이스(DB)에 미리 저장된 인물 중 누구와 가장 유사한지를 식별<sup>identification</sup>하는 데 이 기술이 널리 활용되고 있습니다.</p>

<p><img src="/assets/img/2020-07-23-200723/001.jpg" width="" align="" /></p>

<p><em class="center">[ 그림 1 ] 얼굴 인식 과정</em></p>

<p>다만 얼굴 인식 모델의 훈련 또는 추론 단계에서 사진 속 얼굴 위치가 제각기 다르거나 그 촬영 각도가 다르면 얼굴 인식 정확도가 낮아질 수 있습니다. 따라서 사진에서 얼굴 영역을 찾아 동일한 형태의 정면 얼굴을 추출하는 전처리 과정이 선행되어야 합니다. 일반적인 전처리 과정은 다음과 같습니다. 1) 시스템에 입력된 이미지에서 얼굴 영역을 찾아(얼굴 검출<sup>face detection</sup>) 눈과 코 등 얼굴의 특징을 나타내는 점을 찾습니다(얼굴 정렬<sup>face alignment</sup><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>). 3) 이 특징점을 이용해 얼굴 영역을 동일한 형태와 크기로 변경합니다(정규화<sup>normalization</sup><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>).</p>

<p><img src="/assets/img/2020-07-23-200723/002.jpeg" width="80%" align="" /></p>

<p><em class="center">[ 그림 2 ] 얼굴 이미지 전처리 과정</em></p>

<p>얼굴 인식 모델은 수만 명에 달하는 인물로부터 획득한 수백만 장의 정규화된 얼굴 이미지로부터 인물을 잘 구분하는 함축된 얼굴 표현<sup>facial representation</sup>인 특징 벡터<sup>feature vector</sup>를 학습합니다. 그 결과, 최종 얼굴 인식 모델은 입력된 이미지의 특징 벡터 간 유사도<sup>similarity</sup>를 비교하는 방식으로 검증 또는 식별을 수행할 수 있게 됩니다.</p>

<p>카카오엔터프라이즈 AI Lab(이하 AI Lab) 또한 이 얼굴 인식과 관련해 다양한 연구를 진행하고 있습니다. 최근에는 AI Lab(김용현, 노명철, 신종주)이 카카오(박원표)와 공동으로 연구한 얼굴 인식에 전문화된 아키텍처에 관한 논문(GroupFace<sup>Learning Latent Groups and Constructing Group-based Representations for Face Recognition</sup><a id="01" href="#rf01"><a href="#rf01">[1]</a></a>)이 CVPR 2020에 게재 승인되는 성과를 거두기도 했죠. 이번 글에서는 논문에서 다룬 딥러닝 기반 얼굴 인식 선행 연구를 간단하게 살펴보고자 합니다.</p>

<p class="tech-ground">☛ Tech Ground 데모 페이지 바로 가기 : <b><a href="https://labs.kakaoi.ai/facedetection">얼굴 검출</a></b> 데모</p>

<p><br /></p>

<p class="dot-line">∙  ∙  ∙  ∙  ∙  ∙  ∙</p>

<p><br /></p>

<h1 id="얼굴-인식-모델의-손실-함수-연구-트렌드">얼굴 인식 모델의 손실 함수 연구 트렌드</h1>

<p>딥러닝 모델의 훈련(가중치 업데이트) 과정은 다음과 같습니다([그림 3]). 먼저 순전파<sup>forward propagation</sup><sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> 과정에서 데이터를 입력받은 모델은 무작위로 초기화된 가중치를 이용해 예측값을 출력합니다. 그다음, 예측값과 정답 사이의 차이를 정의하는 손실 함수를 이용해 입력에 대한 손실<sup>loss</sup>을 계산합니다. 다음으로, 출력층에서 입력층으로 거슬러 올라가는 역전파<sup>backward propagation</sup><sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup> 과정에서 최적화 알고리즘은 기울기<sup>gradient</sup> 값을 이용해 앞서 구한 손실을 최소화하는 방향으로 모델의 가중치 값을 수정합니다. 딥러닝 모델은 이 과정을 여러 번 반복해서 데이터를 점진적으로 학습하게 됩니다.</p>

<p><img src="/assets/img/2020-07-23-200723/003.jpg" width="80%" align="" /></p>

<p><em class="center">[ 그림 3 ] 딥러닝 모델의 학습 과정</em></p>

<p>얼굴 인식을 학습하기 위한 손실 함수는 소프트맥스 손실 함수<sup>softmax loss function</sup>, 거리 기반 손실 함수<sup>distance-based loss function</sup>, 앵귤러 마진 기반 손실 함수<sup>angular margin based loss function</sup> 등 크게 세 종류<a id="02" href="#rf02"><a href="#rf02">[2]</a></a>로 나눠볼 수 있습니다. 이제부터 이 손실 함수를 하나씩 알아보겠습니다.</p>

<p><br /></p>

<h2 id="1-소프트맥스-손실-함수">1. 소프트맥스 손실 함수</h2>

<p>일반 객체 분류 모델인 AlexNet과 ResNet, 그리고 초창기 얼굴 인식 모델인 DeepFace, DeepID의 마지막 출력층은 이전 단계에서 추출된 특징 벡터를 N개의 범주로 분류하기 위해 배치된 완전연결층<sup>fully-connected layer</sup>과 소프트맥스<sup>softmax</sup> 함수<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>로 구성돼 있습니다. 이후 교차 엔트로피<sup>cross entropy</sup>가 소프트맥스 확률 분포와 정답 분포 사이 오차를 계산합니다. 소프트맥스 확률값을 이용한다는 점에서 이 손실 함수를 소프트맥스 손실 함수라고도 부릅니다.</p>

<p><img src="/assets/img/2020-07-23-200723/004.jpg" width="55%" align="" /></p>

<p><em class="center">[ 그림 4 ] 소프트맥스 손실 함수</em></p>

<p>소프트맥스 손실 값을 최소화하는 학습을 통해 모델은 특징 공간<sup>feature space</sup>에 동일인의 두 특징 벡터<sup>intra-class</sup>를 더 가깝게, 비동일인의 두 특징 벡터<sup>inter-class</sup>는 더 멀게 표현할 수 있게 됩니다. 하지만 단순히 소프트맥스 함수만을 사용한 손실 함수로는 수만 명에 달하는 인물의 특징 공간을 효율적으로 학습하기가 어렵습니다. 전체적인 최적화를 고려하지 못하고 국부적인 최적 지점<sup>local minimum</sup>으로 쉽게 수렴할 수 있기 때문입니다. 이로 인해 추론 단계에서 마주하는 학습하지 않은 새로운 얼굴 이미지의 인식 성능은 낮을 수 있습니다.</p>

<p><br /></p>

<h2 id="2-거리-기반-손실-함수">2. 거리 기반 손실 함수</h2>

<p>거리 기반의 손실 함수 또한 앞서 설명한 소프트맥스 손실 함수처럼 특징 공간에 동일인의 두 특징 벡터를 더 가깝게, 비동일인의 두 특징 벡터는 더 멀게 표현하는 학습에 활용됩니다. 차이점은 특징 벡터 간의 거리를 학습에 직&gt;접 활용하는 부분입니다. 대표적인 거리 기반 손실 함수인 대비<sup>contrastive</sup> 손실 함수와 트리플렛<sup>triplet</sup> 손실 함수를 통해 그 작동 원리에 대해 좀 더 자세히 소개해드리겠습니다.</p>

<p><img src="/assets/img/2020-07-23-200723/005.jpg" width="50%" align="" /></p>

<p><em class="center">[ 그림 5 ] 거리 기반 손실 함수</em></p>

<p>대비 손실 함수는 두 얼굴 이미지의 쌍을 구성해 두 특징 벡터 간의 거리를 계산합니다. 여기서 손실 값은 동일인의 두 벡터 간 거리가 멀면 커지고, 반대로 비동일인의 두 벡터간 거리가 가까우면 커집니다. 이에 모델은 이 손실 값을 최소화하는 학습을 통해 동일인에 해당하는 두 벡터를 가깝게, 비동일인의 두 벡터를 더 멀게 표현할 수 있게 됩니다.</p>

<p>하지만 이런 손실 계산 방식에는 한계가 있습니다. 동일인에 해당하는 두 벡터 거리와 비동일인의 두 벡터 거리가 개별적으로 학습되기 때문입니다. 즉, 동일인 간의 벡터 거리가 비동일인 사이 벡터 거리보다 상대적으로 가까워져야 함을 고려하지 못하는 거죠.</p>

<p>트리플렛 손실 함수는 범주가 같은 벡터 간의 거리와 범주가 다른 벡터 간의 거리의 상대적 관계를 고려하는 방식으로 이 문제를 해결합니다. 먼저, 기준이 되는 이미지 a, 동일인 이미지 p, 그리고 비동일인 이미지 n으로 구성된 트리플렛(a, p, n)을 구성합니다. 트리플렛 손실 함수는 a와 p의 벡터 간 거리를 줄이는 동시에, a와 n의 벡터 간 거리를 넓힙니다. 즉, 두 거리의 차\((\vert a-n \vert - \vert a-p \vert)\)가 개발자가 임의로 정한 마진값보다 크도록 합니다. 그 결과, 트리플렛 손실 함수로 학습된 인식 모델은 대비 손실 함수와 비교했을 때 더 높은 추론 성능을 보이는 경향이 있습니다.</p>

<p>다만 지난 2017년을 기점으로 얼굴 인식 관련 딥러닝 모델에서는 거리 기반 손실 함수를 잘 사용하지 않는 추세입니다. 트리플렛을 구성하는 방식에 따라 성능이 크게 달라지기 때문입니다. 특히 범주 수가 많을수록 효과적인 트리플렛을 구성하기도 쉽지 않습니다. 많은 방법이 고안되었음에도, 얼굴 인식처럼 분류해야 할 범주가 많은 상황에서는 효과적인 트리플렛을 구성하는 데는 여전히 어려움이 있습니다.</p>

<p>아울러 컴퓨팅 비용도 상대적으로 더 많이 소모됩니다. 일반적으로 이 손실 함수는 범주의 수가 수십~수백개 수준일 때 효과적으로 동작하나, 수천 개 이상의 범주가 존재할 때는 잘 동작하지 않습니다. 기존 대비 2~4배 이상 배치 크기<sup>batch size</sup><sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>를 키움으로써 성능 저하를 일부 극복할 수 있지만, 수만 명에 달하는 인물을 다루어야 하는 얼굴 인식 모델에서는 그 역시 쉽지 않은 일입니다.</p>

<p><br /></p>

<h2 id="3-앵귤러-마진-기반-손실-함수">3. 앵귤러 마진 기반 손실 함수</h2>

<p>SphereFace에서 CosFace와 ArcFace로 이어지는 근래의 얼굴 인식 연구는 앵귤러 마진을 추가한 소프트맥스 기반 손실 함수를 이용해 서로 다른 인물 간 거리를 충분히 넓히는 방향으로 진행되고 있습니다.</p>

<p><img src="/assets/img/2020-07-23-200723/006.jpg" width="50%" align="" /></p>

<p><em class="center">[ 그림 6 ] 앵귤러 마진 기반 손실 함수인 AdaM-소프트맥스</em></p>

<p>충분히 큰 한정된 공간이 있다고 가정해 보겠습니다. 학습 시 다른 범주를 1의 간격으로 분포하게 만들어도 분류는 완벽하게 이뤄집니다. 여기에 더 나아가, 다른 범주를 10의 간격으로 두고 데이터 분포를 훈련한다면 어떻게 될까요? 학습 난이도는 높을지라도 공간을 효율적으로 사용할 수 있어, 모델의 일반화<sup>generalization</sup><sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup> 성능이 높다고 기대해볼 수 있겠습니다. 여기서는 이러한 간격을 특징 벡터 간의 각도에 적용하였다는 점에서 앵귤러 마진이라는 이름이 붙었습니다.</p>

<p><br /></p>

<h1 id="한계점">한계점</h1>

<p>이처럼 손실 함수가 새롭게 제안되고 있음에도 불구하고, 얼굴 인식 모델에게 서로 다른 인물의 얼굴을 구분하는 최적의 특징 공간을 생성하는 방법을 가르치기는 여전히 쉽지 않습니다. AI Lab은 기존 대부분의 손실 함수에 대한 얼굴 인식 연구로는 수만 명의 얼굴을 구분하는 충분한 효과를 기대하기가 어렵다고 판단했습니다. AlexNet, VGGNet, ResNet처럼 수십에서 수천 개의 범주를 분류하는 데 최적화된 범용 모델을 그대로 사용해서는 수만 명의 얼굴을 효과적으로 처리하는 데 적합하지 않다고 본 거죠.</p>

<p>선행 연구가 최대 수만 개의 범주를 효과적으로 분류하려는 목적에서 새로운 손실 함수를 도입했듯이, AI Lab은 이를 위한 특별한 인식 모델 구조를  고민했습니다.</p>

<p>AI Lab은 인간의 얼굴에 각자 고유의 특징이 있으면서도, 동시에 공통적인 특징을 가지고 있다는 점을 면밀히 관찰했습니다. 실제로 얼굴의 공통적인 특징을 모아 보는 그룹화 개념은 현실에서도 누군가를 특정하는 데 유용한 기법으로 활용되고 있습니다. 예를 들어, 검은 머리와 붉은 수염을 가진 남자와 같이 주된 특징을 조합한 몽타주 표현처럼 말이죠.</p>

<p>최종적으로 AI Lab은 얼굴의 유사성을 그룹화해 표현하는 특징 벡터<sup>group-aware representation</sup>를 추출하는 새로운 모델 구조인 GroupFace를 제안했습니다. GroupFace는 다양한 얼굴 인식 모델에 적용 가능합니다. 얼굴 인식 분야 대표적인 9개 공개 데이터셋을 상대로 테스트를 진행해본 결과, GroupFace는 모든 데이터셋에 대해 얼굴 인식 성능을 높였습니다. 특히 IJB 벤치마크에서는 그 정확도를 최대 10%P 가까이 끌어올렸습니다.</p>

<p>AI Lab이 새로 고안한 방법론인 GroupFace의 아이디어와 아키텍처, 성능 평가에 관한 자세한 내용은 해당 논문을 확인해 주시기 바랍니다.</p>

<p><img src="/assets/img/2020-07-23-200723/007.png" width="70%" align="" /></p>

<p><em class="center">[ 그림 7 ] GroupFace</em></p>

<p><br /></p>

<h1 id="참고-문헌">참고 문헌</h1>

<p>[<a id="rf01" href="#01">1</a>] <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_GroupFace_Learning_Latent_Groups_and_Constructing_Group-Based_Representations_for_Face_CVPR_2020_paper.pdf">GroupFace : Learning Latent Groups and Constructing Group-based Representations for Face Recognition</a> (2020) by Yonghyun Kim, Wonpyo Park, Myung-Cheol Roh, Jongju Shin in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</p>

<p>[<a id="rf02" href="#02">2</a>] <a href="https://arxiv.org/abs/1804.06655">Deep Face Recognition: A Survey</a> (2018) by Mei Wang, Weihong Deng in arXiv</p>

<p><br /></p>

<h1 id="각주">각주</h1>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>얼굴 위에서 특징점<sup>keypoints</sup>의 형태를 잡아간다는 점에서 착안해 ‘정렬<sup>alignment</sup>‘이라는 표현을 쓴다. 얼굴 특징점 검출<sup>face landmark detection</sup>이라고도 부른다. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>일부 연구에서는 정규화 단계에서 사진 속 조명을 제거하거나 인물 표정을 같은 식으로 변경하기도 하고, 측면 얼굴을 정면 얼굴로 생성하기도 한다. 하지만 기본적으로 2D 영상의 정규화라고 한다면 오려낸 얼굴을 똑같은 형태로 맞추는 일이다. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>입력층-은닉층-출력층을 거쳐서 예측값을 내는 방법 <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>출력층-은닉층-입력층으로 거슬러 올라가며 가중치를 업데이트하는 방법 <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>다범주 분류기에서는 출력값을 제대로 해석하고자 다른 출력 노드와의 상대적인 크기를 비교한다. 이를 위해 각 출력 노드를 0~1 사이로 제한해 이를 합한 값을 1로 만든다. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>한차례의 훈련, 즉 가중치를 한 번 업데이트하는 데 사용하는 훈련 집합의 크기 <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>학습 데이터와 조금이라도 다른 성격의 데이터가 입력되어도 모델이 제대로 동작하는 상태 <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>samantha:작성,편집</name></author><category term="deepdive" /><category term="face recognition" /><category term="loss function" /><summary type="html"><![CDATA[얼굴 인식 모델의 성능을 높이려는 목적에서 다양한 손실 함수가 제안돼 왔습니다. 하지만 손실 함수 개선만으로는 성능 향상에 한계가 있습니다. 기존의 인식 모델 자체가 수천개 범주만을 구분할 수 있다보니, 수만 명의 인물을 구분해야 하는 태스크에는 적합하지 않은 거죠. 이에 카카오엔터프라이즈는 얼굴의 유사성을 그룹화해 표현하는 특징 벡터를 추출하는 새로운 모델 구조인 GroupFace를 고안했습니다.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://genius0928.github.io//assets/img/2020-07-23-200723/000.jpg" /><media:content medium="image" url="https://genius0928.github.io//assets/img/2020-07-23-200723/000.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">FRVT 1:1 검증 챌린지 참가 스토리</title><link href="https://genius0928.github.io//deepdive/200616" rel="alternate" type="text/html" title="FRVT 1:1 검증 챌린지 참가 스토리" /><published>2020-06-16T00:00:00+09:00</published><updated>2020-06-16T00:00:00+09:00</updated><id>https://genius0928.github.io//deepdive/200616</id><content type="html" xml:base="https://genius0928.github.io//deepdive/200616"><![CDATA[<p>얼굴 인식 기술은 지난 수십년 간 컴퓨터 비전의 주요 연구 분야 중 하나로 자리매김하고 있습니다. 카카오 또한 딥러닝을 이용한 관련 연구개발을 진행하고 있죠. 지난 2016년 말 출시된 ‘라이브픽’은 자사 얼굴 인식 기술을 사용한 대표적인 예입니다. 이 이미지 검색 서비스([그림 1])는 시사회, 시상식, 사인회, 공항 출입국과 같은 이벤트별로 모은 스타 사진을 시간순으로 보여줍니다.</p>

<p><img src="/assets/img/2020-06-16-200616/001.png" width="40%" align="" /></p>

<p><em class="center">[ 그림 1 ] 라이브픽 예시 화면</em></p>

<p>당시 개발한 모델이 주로 학습한 데이터셋에는 서양인 얼굴이 많이 포함돼 있어 동양인 얼굴을 제대로 인식하지 못하는 문제가 있었습니다. 동양인 얼굴 이미지 수집 및 레이블링, 모델을 개선하면 좋겠다는 논의를 바탕으로 새로운 얼굴 인식 모듈 개발에 성공한 카카오는 이전보다 훨씬 더 정확한 라이브픽 서비스를 제공할 수 있게 됐습니다.</p>

<p>여기서 더 나아가 카카오엔터프라이즈 AI Lab(이하 AI Lab)은 자사가 보유한 얼굴 인식 모델의 객관적인 성능을 측정하면 좋겠다고 판단했습니다. 그 결과, 팀(신종주, 김홍락, 김용현)을 이뤄 참가하게 된 안면인식 공급업체 테스트(FRVT<sup>face recognition vendor test</sup>)<sup><a href="#bb4d:fn:1" class="footnote" id="bb4d:fn-back:1">1</a></sup>의 경쟁 주제 중 하나인 1:1 검증<sup>verification</sup> 일반 이미지(wild) 부문에서 100여 팀 중 3등(2020년 5월 21일 기준)을 차지하는 성과를 달성했습니다.</p>

<p>이번 글에서는 AI Lab이 도전한 대회를 소개하고, AI Lab만의 문제 해결 방식, 성능 평가 척도 및 향후 연구 계획에 대해 다루고자 합니다. 이를 위해 AI Lab AI기술팀 멀티미디어처리파트의 신종주 연구원을 만나 그 자세한 이야기를 들어봤습니다.</p>

<p class="tech-ground">☛ Tech Ground 데모 페이지 바로 가기 : <b><a href="https://labs.kakaoi.ai/facedetection">얼굴 검출</a></b> 데모</p>

<p><br /></p>

<p class="dot-line">∙  ∙  ∙  ∙  ∙  ∙  ∙</p>

<p><br /></p>

<h1 id="frvt-11-검증-챌린지를-소개합니다">FRVT 1:1 검증 챌린지를 소개합니다</h1>

<p>미국국립표준기술연구소(NIST)가 주관하는 FRVT는 얼굴 정보 대조를 통한 출입국심사, 여권 불법 복제 탐지, 아동(미성년자) 성범죄 피해자 식별<sup><a href="#bb4d:fn:2" class="footnote" id="bb4d:fn-back:2">2</a></sup>처럼  민간∙사법∙국가 보안 영역에서 활용되는 자동화된 얼굴 인식 응용 프로그램의 성능을 측정하는 대회입니다. 대회가 열리는 2~3년의 기간 동안 참가자는 4개월에 한 번씩 새로운 알고리즘을 제출할 수 있습니다. 주최 측은 한 업체가 최근 제출한 2개의 알고리즘의 성능을 평가해 그 결과를 리포트 형태로 발표합니다.</p>

<p>대회 참가자는 검증(1:1 매칭), 식별(1:N 매칭), 모프(morph), 품질(quality) 등 총 4개의 경쟁 주제 중 원하는 분야를 선택해 참가할 수 있습니다. ‘검증’은 시스템에 입력된 두 이미지 속 인물의 동일인 여부를 판별하는 능력을 측정합니다. ‘식별’은 시스템에 입력된 얼굴 이미지가 내부 데이터베이스(DB)에 저장된 인물 중 누구와 가장 유사한지를 판단하는 데 활용됩니다. ‘모프’는 시스템에 입력된 얼굴 이미지를 인식해 그 합성 여부를 제대로 판단하는지를 평가합니다. ‘품질’에서는 화질, 얼굴 촬영 각도, 조명 등에 따른 입력 영상의 품질을 판별하는 능력을 측정합니다.</p>

<p>이 중에서도 AI Lab이 참가한 1:1 검증에 대해 더 자세히 설명해보겠습니다. 1:1 검증 참가자는 미국 입국자의 비자(visa) 사진과 범인 식별용 상반신(mugshot) 사진, 실생활 환경에서 촬영된 일반 얼굴 사진(wild), 공항 출입국심사대에서 촬영된 사진(visaboarder) 부문에서 주어진 두 얼굴 이미지쌍의 동일인 여부를 판별하는 실력을 겨룹니다([그림 2]). 새롭게 신설된 네번째 부문<sup><a href="#bb4d:fn:3" class="footnote" id="bb4d:fn-back:3">3</a></sup>에서의 관건은 고품질 영상의 비자 사진과 웹카메라로 촬영한 중저품질 영상 속 인물의 동일인 여부를 판별하는 데 있습니다.</p>

<p><img src="/assets/img/2020-06-16-200616/002.png" width="80%" align="" /></p>

<p><em class="center">[ 그림 2 ] FRVT 1:1 검증에서 얼굴 인식 알고리즘 성능 평가에 사용하는 사진 종류 (출처 : NIST)</em></p>

<p>NIST는 참가자로부터 받은 코드를 평가 플랫폼에서 직접 가동하는 방식을 채택했습니다. CPU 싱글스레드<sup><a href="#bb4d:fn:4" class="footnote" id="bb4d:fn-back:4">4</a></sup>를 이용한 C++ 추론 모델로 영상 하나당 1초 안에 정답(동일인입니다. 또는 비동일인입니다)을 도출해야 한다는 유의점을 사전 고지했습니다. NIST에서는 사용자가 제출한 리눅스<sup>Linux</sup> 라이브러리 파일을 직접 실행해 각 테스트 데이터셋 얼굴 이미지쌍을 상대로 얼굴 인식 알고리즘의 성능 정확도를 평가합니다.</p>

<p><br /></p>

<h1 id="ai-lab이-과제를-해결한-방법">AI Lab이 과제를 해결한 방법</h1>

<p>AI Lab은 얼굴 인식 분야에서 많이 사용하는 모델인 ResNet-100<sup><a href="#bb4d:fn:5" class="footnote" id="bb4d:fn-back:5">5</a></sup><a href="#bb4d:rf:1" class="reference" id="bb4d:rf-back:1">[1]</a>을 훈련했습니다. 학습에는 AI Lab이 자체적으로 확보한 10만 명의 인물로부터 획득한 1,000만 여장의 얼굴 이미지를 활용했으며, 검증에는 NIST에서 공개한 일반 사진 데이터셋(IJB<sup><a href="#bb4d:fn:6" class="footnote" id="bb4d:fn-back:6">6</a></sup>-B,C)과 FRVT에서 제공한 상반신 사진을 활용했습니다.</p>

<p>문제는 추론이었습니다. 주최 측의 설명대로라면, 단 1초 안에 [그림 3]에 기술된 1~4단계를 거쳐 추론 결과를 제시하는 제약이 걸려있기 때문입니다. 이에 AI Lab은 총 3차례에 걸친 도전을 통해 제시간 내로 더 정확한 추론 결과를 내는 전략을 고민했습니다.</p>

<p><img src="/assets/img/2020-06-16-200616/003.jpg" width="" align="" /></p>

<p><em class="center">[ 그림 3 ] 얼굴 인식 시스템의 추론 과정</em></p>

<h2 id="1적절한-컴퓨팅-환경-설정하기">1.적절한 컴퓨팅 환경 설정하기</h2>

<p>NIST에서는 서버 운영체제를 CentOS<sup><a href="#bb4d:fn:7" class="footnote" id="bb4d:fn-back:7">7</a></sup>로 제한했습니다. 이에 AI Lab은 운영체제 설치와 반납이 쉬운 사내 가상 서버에 CentOS를 설치했습니다. 하지만 NIST가 제약을 둔 동일한 컴퓨팅 환경이 아니라서 알고리즘의 추론 속도를 정확하게 산출하기가 쉽지 않은 문제가 있었습니다. 가상 서버에서의 CPU 동작 속도가 기대치보다 더 느렸기 때문이죠.</p>

<p>첫 번째 제출 이후 AI Lab은 개발 환경 개선 작업에 나섰습니다. 제출 당시 사용했던 가상 서버 환경이 NIST가 제시한 것보다 느렸던 점을 고려하며 상대적인 속도를 측정했습니다. 그 결과, 두 번째 제출부터는 NIST가 제시한 것과 유사한 컴퓨팅 환경을 구축할 수 있었습니다.</p>

<h2 id="2추론-플랫폼-통일하기">2.추론 플랫폼 통일하기</h2>

<p>AI Lab은 자사가 보유한 얼굴 검출 모듈과 얼굴 정렬 모듈은 파이토치<sup>Pytorch</sup>로, 얼굴 인식 모듈은 mxnet에서 학습돼 도커<sup>docker</sup><sup><a href="#bb4d:fn:8" class="footnote" id="bb4d:fn-back:8">8</a></sup>8로 각기 따로 서비스하고 있습니다. 문제는 앞서 설명한 대로 대회에 참가하기 위해서는 서로 의존성이 없는 세 모듈을 C++<sup><a href="#bb4d:fn:9" class="footnote" id="bb4d:fn-back:9">9</a></sup>로 동작할 수 있게 만들어야만 하는 데 있었습니다.</p>

<p>시간이 촉박했던 1차 제출에서는 AI Lab가 기존에 보유하던 3가지 모듈을 급하게 합치느라 코드를 충분하게 검토하지 못했습니다. 파이토치, mxnet을 비롯한 대다수의 딥러닝 플랫폼은 파이썬<sup>Python</sup> 뿐만 아니라 C++에서도 모델 파일을 읽어 들여서 추론하는 기능을 제공합니다. AI Lab은 효율적인 운영을 위해 딥러닝 플랫폼 단일화가 좋겠다고 판단했습니다. 그 구조가 복잡해 플랫폼 변경이 쉽지 않은 얼굴 검출 모듈에 사용된 파이토치를 기준 삼아, 얼굴 인식 모듈을 파이토치로 변환하기로 하였습니다.</p>

<p>이미 파이토치로 구현된 얼굴 검출 모듈과 얼굴 정렬 모듈은 C++에서 사용 가능한 torch script<sup><a href="#bb4d:fn:10" class="footnote" id="bb4d:fn-back:10">10</a></sup> 파일로 쉽게 변환할 수 있었습니다. 아쉽게도 1차에서는 MMdnn<sup><a href="#bb4d:fn:11" class="footnote" id="bb4d:fn-back:11">11</a></sup>를 이용해 mxnet으로 구현된 얼굴 인식 모듈을 파이토치 코드로 변환할 때, 배치분포 정규화<sup>batch normalization</sup><sup><a href="#bb4d:fn:12" class="footnote" id="bb4d:fn-back:12">12</a></sup> 층의 매개변수<sup>parameter</sup> 값을 그대로 복사하지 못하는 문제를 인지하지 못했습니다. 이렇게 되면 0.00000x 차이로도 출력 결과가 달라져 성능 저하에 큰 영향을 미치게 되죠. 2차 제출에서는 이 문제를 해결해 추론 정확도를 대폭 높였으나, 상위권에 오르기에는 다소 부족했습니다.</p>

<p>세 번째 제출에서 AI Lab은 세 모델을 torch script 대신, 인텔 CPU에 최적화되어 빠른 추론 속도를 제공하는 openvino 파일로 변환했습니다. 컴퓨터 비전 딥러닝 모델에서 주로 활용하는 컨볼루션<sup>convolution</sup><sup><a href="#bb4d:fn:13" class="footnote" id="bb4d:fn-back:13">13</a></sup>과 배치분포 정규화 층의 매개변수를 합쳐 한 번의 연산만 수행할 수 있어(layer fusion) 속도 향상을 기대할 수 있다고 내다봤기 때문입니다. AI Lab이 자사 서비스 개발에 openvino를 사용해본 경험도 이 결정에 영향을 미쳤습니다.</p>

<p>다만 openvino는 기본적으로 멀티스레드<sup>multi-thread</sup><sup><a href="#bb4d:fn:14" class="footnote" id="bb4d:fn-back:14">14</a></sup>로 동작합니다. 몇몇 매개변수를 조절해봐도 최소 3개의 쓰레드를 사용하도록 강제하죠. AI Lab은 openvino 내부 코드 중 필요가 없는 스레드를 제거해 싱글스레드로 동작할 수 있도록 했습니다.</p>

<h2 id="3추론-정확도-높이기">3.추론 정확도 높이기</h2>

<p>제아무리 숙련된 머신러닝 엔지니어라도 데이터에 적합한 네트워크 구조를 탐색하는 데에는 수개월 공들여야 합니다. 세 번째 제출일까지는 촬영 각도와 조명, 표정, 장신구 등에 따라 큰 편차를 보이는 데이터셋에 강건한(robust) 얼굴 인식 모델 개발은 거의 불가능한 상황이었죠. 이런 이유로 AI Lab은 알고리즘 개선 대신, 테스트 단계에서의 어그먼테이션(TTA<sup>test time augmentation</sup>) 도입을 고려했습니다.</p>

<p>하나의 이미지를 여러 관점에서 관찰할 수 있도록 이미지를 좌우로 뒤집거나(flipping) 자르는(cropping) 등 다양한 기법을 아우르는 어그먼테이션은 모델의 추론 정확도를 끌어올리는 주요 방법론 중 하나입니다. TTA는 각기 서로 다른 어그먼테이션 기법을 적용한 테스트 데이터를 최종 모델(훈련 데이터와 검증 데이터로 학습을 마친 모델)에 입력하는 방식으로 성능을 끌어올립니다. 마치 서로 다른 어그먼테이션 기법을 적용한 학습 모델을 앙상블<sup>ensemble</sup>한 것과 비슷한 성능 향상 효과를 기대할 수 있습니다.  테스트 데이터를 모델에 여러 차례 입력하기 때문에 정답을 추론하는 시간은 좀 더 길 수는 있습니다. 하지만 최종 모델의 매개변수를 변경하지 않고도 추론 성능을 높일 수 있어서 시간과 비용을 획기적으로 줄이는 데 도움이 됩니다.</p>

<p>AI Lab은 이미지 한 장의 좌우를 뒤집으면 그렇지 않을 때보다 전체 추론 시간이 2배 더 가까이 소모된다는 점에서 다양한 어그먼테이션 기법을 적용하기 어렵다고 판단했습니다. 이에 앞서 두 차례 제출에서 어그먼테이션 미적용시 추론 결과 도출까지 0.5초를 넘지 않았던 경험을 바탕으로, 테스트 단계에서 이미지를 좌우로 뒤집는 플립 기법<a href="#bb4d:rf:2" class="reference" id="bb4d:rf-back:2">[2]</a>을 적용해 코드를 구현했습니다.</p>

<p><br /></p>

<h1 id="성능-척도와-최종-결과">성능 척도와 최종 결과</h1>

<p>두 얼굴 이미지에서 특징 벡터를 추출한 얼굴 인식 시스템은 최종적으로 두 벡터 간 유사도를 비교해 점수를 산정합니다. 이 유사도 점수는 동일인 얼굴 쌍에서는 높으며, 비동일인 얼굴 이미지 쌍에서는 낮습니다. 따라서 동일인 얼굴 쌍과 비동일인 얼굴 쌍 각각에 대해 유사도 분포가 잘 분리돼 있을수록 시스템의 성능이 좋다고 할 수 있습니다.</p>

<p>문제는 알고리즘이 구분하기 어려운 얼굴 쌍으로 인해 두 분포가 겹치는 영역이 존재한다는 데 있습니다. 이런 이유로 얼굴 인식 알고리즘의 성능을 평가할 때는 특정 유사도 점수(T)를 기준으로 입력된 두 이미지를 동일인이라고 판단하거나 또는 비동일인이라고 판단합니다.</p>

<p>그렇다면 무엇을 기준으로 T를 결정할까요? 일반적으로는 비동일인 이미지쌍 데이터셋에서 동일인이라고 추론할 특정 확률(타인일치율, FMR)에 따라 결정합니다. 물론 기준이 되는 FMR 값은 평가하려는 대상에 따라 조금씩 다를 수 있습니다. 예를 들어, 출입 시스템의 성능을 평가할 때는 FMR이 낮아야 합니다. 특정인을 제대로 인식하지 못해 출입하지 못하는 쪽보다는, 허가받지 않은 특정인에게 출입 허가를 부여하는 쪽의 문제가 더 크기 때문입니다. 반면, 미아 찾기 시스템에서는 FMR이 다소 높아야 합니다. 오검출율이 늘어나더라도 최대한 많은 후보군을 뽑아낼 수 있기 때문입니다.</p>

<p><img src="/assets/img/2020-06-16-200616/004.png" width="" align="" /></p>

<p><em class="center">[ 그림 4 ] 동일인 얼굴 쌍과 비동일인 얼굴 쌍의 유사도 점수 분포상에서 T에 따른 FMR과 FNMR</em></p>

<p>NIST는 부문별로 서로 다른 FMR(비자 사진=1e-4/1e-6, 상반신 사진=1e-5, 일반 사진=1e-4, 공항 출입국 사진=1e-6)을 만족하는 T를 기준으로 동일인 이미지 쌍 데이터셋에서 비동일인이라 추론할 확률(본인불일치율, FNMR)의 수치를 비교했습니다.</p>

<p>주최 측은 [표 1]처럼 테스트 데이터셋을 구축했습니다. 비자 사진과 일반 얼굴 사진 부문에서의 비동일인 이미지 쌍에서는 성별과 나이와 같은 공변량<sup>covariate</sup>15을 따로 제어하지 않았습니다. 이는 이미지 속 인물에 관한 정보를 모르는 상태에서 DB에 저장된 모든 인물과의 동일인 여부를 비교해야 하는 1:N 매칭 태스크로의 확장까지 염두해서 데이터셋을 설계한 것으로 보입니다. 주최 측은 추후에는 난이도가 더 높은 공변량 제어 비동일인 이미지 쌍에 대해서도 알고리즘의 성능을 평가하는 실험을 설계한다는 계획입니다.</p>

<table>
  <tr class="key">
    <td colspan="2">테스트 데이터셋</td>
    <td>동일인 이미지쌍</td>
    <td>비동일인 이미지쌍</td>
    <td>인물 수</td>
  </tr>
  <tr>
    <td class="key">visa</td>
    <td>미국 입국자의 비자 사진</td>
    <td>10,000</td>
    <td>10,000,000,000</td>
    <td>100,000</td>
  </tr>
  <tr>
    <td class="key">mugshot</td>
    <td>범인 식별용 상반신 사진</td>
    <td>1,000,000</td>
    <td>100,000,000</td>
    <td>1,000,000</td>
  </tr>
  <tr>
    <td class="key">wild</td>
    <td>실생활에서 촬영한 일반 얼굴 사진</td>
    <td>1,000,000</td>
    <td>10,000,000</td>
    <td>10,000</td>
  </tr>
  <tr>
    <td class="key">visaboarder</td>
    <td>공항 출입국심사대에서 촬영한 사진</td>
    <td>1,000,000</td>
    <td>100,000,000</td>
    <td>1,000,000</td>
  </tr>
</table>
<p><em class="center">[ 표 1 ] NIST가 모델 성능 평가에 사용한 테스트 데이터</em></p>

<p>사진 부문의 비동일인 이미지쌍에서는 성별 이외의 다른 공변량은 따로 제어하지 않았습니다. 사진 촬영 시점에 이미 인물의 성별 정보를 알고 있다는 사실에 기인한 것으로 분석됩니다. 한편, 나이를 제한하지 않은 이유는 과거에 상반신 사진을 촬영한 범죄자가 출소 후 다른 범죄 사건에 또다시 연루될 가능성을 배제하지 않았다는 의미로 해석됩니다.</p>

<p>최종적으로 AI Lab은 일반 이미지 데이터셋 부문에서 FMR이 0.01%일 때 FNMR 2.74%를 달성, 1등(FNMR 2.71%), 2등(FNMR 2.73%)과 근소한 차이를 보이며 우수한 결과를 거뒀습니다([그림 5]). 다만 AI Lab은 인물마다 여러 장의 비자 사진과 상반신 사진을 구할 수 없어 해당 부문에서 중위권에 머무른 부분은 아쉽다고 자체 평가했습니다.</p>

<p><img src="/assets/img/2020-06-16-200616/005.png" width="" align="" /></p>

<p><em class="center">[ 그림 5 ] FRVT 1:1 검증 리포트의 순위 정보(2020년 5월 기준, 출처 : NIST)</em></p>

<h1 id="향후-계획">향후 계획</h1>

<p>AI Lab은 이번 대회 기간 중 1:1 검증 모든 데이터셋 부문에서의 상위권 진출을 목표로 자사 얼굴 인식 알고리즘 성능을 높인다는 계획입니다. 우선, 제한된 환경에서 측면 또는 정면을 촬영한 사진 데이터셋을 추가로 확보할 예정입니다. 아울러, 추론 속도 향상을 위해 알고리즘 고도화를 위한 추가 연구를 진행한다는 계획입니다. 신종주 연구원은 “최근 한 연구에서 얼굴 검출 모듈과 얼굴 정렬 모듈을 하나의 네트워크로 합쳐 좋은 성과를 냈다”며 “이 둘을 합치면 전체 추론 시간을 줄일 수 있어 추론 성능 향상에 필요한 다양한 추가 기법 도입을 고려해볼 수 있을 것으로 기대된다”고 말했습니다.</p>

<p><br /></p>

<h1 id="참고-문헌">참고 문헌</h1>

<p><a id="bb4d:rf:1" class="referencebody"><a href="#bb4d:rf-back:1" class="backlink">[1]</a>  ArcFace: Additive Angular Margin Loss for Deep Face Recognition (2019) by Jiankang Deng, Jia Guo, Niannan Xue, Stefanos Zafeiriou in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</a><br /></p>

<p><a id="bb4d:rf:2" class="referencebody"><a href="#bb4d:rf-back:2" class="backlink">[2]</a>  ImageNet Classification with Deep Convolutional Neural Networks (2012) by Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton in Advances in Neural Information Processing Systems 25 (NIPS)</a><br /></p>

<p><br /></p>

<h1 id="각주">각주</h1>

<ol class="footnotelist">
<li id="bb4d:fn:1" class="footnotebody" value="1"><p> 지난 대회는 2000년, 2002년, 2006년, 2010년, 2013년에 열렸다. 이를 보아 FRVT는 비정기적으로 열리는 대회인 것으로 추측된다.<a href="#bb4d:fn-back:1" class="backlink"> ↩</a></p></li>
<li id="bb4d:fn:2" class="footnotebody" value="2"><p> 미국 법무부에서는 성범죄 피해 아동을 찾는 데 얼굴 인식 프로그램을 적극적으로 활용하고 있다. 다만 대부분의 학습 데이터가 성인 얼굴만으로 구성돼 있다보니 아동 식별 정확도가 낮다고 알려져 있다.<a href="#bb4d:fn-back:2" class="backlink"> ↩</a></p></li>
<li id="bb4d:fn:3" class="footnotebody" value="3"><p> 주최 측은 서로 다른 방식으로 촬영된 영상물을 대조하는 과제도 언젠가는 선보인다는 계획을 발표한 바 있다.<a href="#bb4d:fn-back:3" class="backlink"> ↩</a></p></li>
<li id="bb4d:fn:4" class="footnotebody" value="4"><p> 연산을 하나의 CPU로 순차 처리하는 방식<a href="#bb4d:fn-back:4" class="backlink"> ↩</a></p></li>
<li id="bb4d:fn:5" class="footnotebody" value="5"><p> 지난 2015년 마이크로소프트 리서치(Microsoft Research)가 제출해 대용량 영상 분류 및 객체 검출을 위한 대회인 ILSVRC<sup>ImageNet Large Scale Visual Recognition Competition</sup>에서 우승을 거머쥔 딥러닝 모델을 일컫는다. 최근에는 이를 변형한 ResNet-100이 얼굴 인식 분야에서 좋은 성과를 내고 있다.<a href="#bb4d:fn-back:5" class="backlink"> ↩</a></p></li>
<li id="bb4d:fn:6" class="footnotebody" value="6"><p> NIST와 정보고등연구계획국(IARPA)이 함께 운영하는 얼굴 인식 평가용 데이터셋<a href="#bb4d:fn-back:6" class="backlink"> ↩</a></p></li>
<li id="bb4d:fn:7" class="footnotebody" value="7"><p> 레드햇(Red Hat)에서 무료로 배포하는 리눅스 운영체제<a href="#bb4d:fn-back:7" class="backlink"> ↩</a></p></li>
<li id="bb4d:fn:8" class="footnotebody" value="8"><p> 하나의 서버에 분리된 공간을 만들어 개발자가 원하는 프로그램을 설치하고 동작할 수 있게 만든다. 예를 들어, 도커를 이용하면 GPU 메모리를 나눠서 여러개의 서비스를 동시에 가동할 수 있다는 점에서 GPU 사용 효율성을 높일 수 있다.<a href="#bb4d:fn-back:8" class="backlink"> ↩</a></p></li>
<li id="bb4d:fn:9" class="footnotebody" value="9"><p> 프로그래밍 언어 중 하나<a href="#bb4d:fn-back:9" class="backlink"> ↩</a></p></li>
<li id="bb4d:fn:10" class="footnotebody" value="10"><p> C++에서는 torch script 파일을 읽어서 추론 결과를 내놓는다.<a href="#bb4d:fn-back:10" class="backlink"> ↩</a></p></li>
<li id="bb4d:fn:11" class="footnotebody" value="11"><p> 여러 딥러닝 플랫폼에서 만든 모델을 서로 변환하는 기능을 제공하는 프로그램<a href="#bb4d:fn-back:11" class="backlink"> ↩</a></p></li>
<li id="bb4d:fn:12" class="footnotebody" value="12"><p> 데이터의 분포가 평균 0, 분산 1 근처일 때 신경망 학습이 잘 되는 것으로 알려져 있다. 이 때문에 데이터의 분포를 미리 조정하는 일(normalization)은 신경망 학습의 가장 기초적인 전처리과정 중 하나다. 데이터가 깊은 신경망을 통과하면서 이 분포가 무너진다는 데 있다. 내재 분포 이동(internal covariate shift)이라 불리는 이 현상은 깊은 신경망의 느린 학습의 주범 중 하나였다. 층마다 다시 데이터의 분포를 표준화<sup>normalization</sup>하는 배치 분포 정규화를 통해 이 문제가 해결됐다.<a href="#bb4d:fn-back:12" class="backlink"> ↩</a></p></li>
<li id="bb4d:fn:13" class="footnotebody" value="13"><p> 이미지에서 숨은 특징을 추출하는 층<a href="#bb4d:fn-back:13" class="backlink"> ↩</a></p></li>
<li id="bb4d:fn:14" class="footnotebody" value="14"><p> 서버내 여러 CPU 자원을 동시에 사용해 연산 속도를 높이는 방식<a href="#bb4d:fn-back:14" class="backlink"> ↩</a></p></li>
<li id="bb4d:fn:15" class="footnotebody" value="15"><p> 독립변수(실험변수) 외 종속변수(결과변수)에 영향을 줄 수 있어서 통제해야하는 변수<a href="#bb4d:fn-back:15" class="backlink"> ↩</a></p></li>
</ol>]]></content><author><name>samantha:작성,편집</name></author><category term="deepdive" /><category term="face recognition" /><category term="FRVT" /><summary type="html"><![CDATA[사내 서비스 '라이브픽'에 쓰이는 얼굴 인식 모델을 개선하는 과정에서 객관적인 성능을 평가받기 위해 챌린지에 도전했습니다. 그 결과, 1차, 2차 그리고 3차에 연이은 도전 끝에 1등과 2등과 매우 근소한 차이를 보이며 3등을 차지했습니다. 현재 다양한 산업 영역에 응용되는 얼굴인식 기술은 요구되는 정확성이 높아 업체 간 경쟁이 치열해지고 있습니다. 매우 한정된 자원으로 거둔 유의미한 성과를 소개하고자 합니다.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://genius0928.github.io//assets/img/2020-06-16-200616/000.jpg" /><media:content medium="image" url="https://genius0928.github.io//assets/img/2020-06-16-200616/000.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">카카오 i 번역 성능 향상 실험 : 대규모 말뭉치를 활용한 사전학습</title><link href="https://genius0928.github.io//deepdive/200507" rel="alternate" type="text/html" title="카카오 i 번역 성능 향상 실험 : 대규모 말뭉치를 활용한 사전학습" /><published>2020-05-07T00:00:00+09:00</published><updated>2020-05-07T00:00:00+09:00</updated><id>https://genius0928.github.io//deepdive/200507</id><content type="html" xml:base="https://genius0928.github.io//deepdive/200507"><![CDATA[<p>사전학습<sup>pretraining</sup>은 데이터양이 절대적으로 적은 상황에서 적용하는 기법입니다. 문제(본 훈련)에서 제시되는 것과 유사한 형태의 데이터로 모델을 사전학습시키면 본 훈련에 효과적인 매개변수<sup>parameter</sup> 초기값 확보에 크게 도움이 되죠. 바로 이런 효과 덕분에 오늘날 대규모 말뭉치<sup>corpus</sup>를 사전학습한 언어 모델<sup>language model</sup><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>이 <a href="https://openai.com/blog/language-unsupervised/">자연어처리(NLP)에서 주류로 자리</a>하게 됐습니다.</p>

<p>하지만 대용량 데이터를 사전학습해 성능을 크게 끌어올린 최신 언어 모델이 모든 NLP 태스크를 잘 풀지는 못합니다. 단적인 예로, 하나의 언어로 구성된 문장에서 특징<sup>feature</sup>을 추출하는 데 주안을 둔 언어 모델은 여러 언어에서의 특징 추출이 중요한 번역 태스크에는 적합하지 않죠. 따라서 태스크에 적합한 구조의 언어 모델과 사전학습에 필요한 다량의 말뭉치를 선택하는 여러 기준이 필요합니다.</p>

<p>이에 카카오엔터프라이즈 AI Lab(이하 AI Lab)이 대규모 말뭉치를 이용해 영어를 한국어로 번역하는 모델 개발 과정을 공유하고자 합니다. 실제로 이 기법은 기존 카카오 i 번역 엔진에 사용된 모델과 비교했을 때 더 나은 성능을 냈습니다. AI Lab  AI기술팀 컨텍스트파트의 양기창 개발자를 만나 자세한 이야기를 들어봤습니다.</p>

<p><br /></p>

<p class="notice">※양기창 개발자는 지난 2019년 7월 카카오의 인턴십 프로그램을 통해 이 연구를 진행했습니다.</p>
<p class="notice">※본문에서는 AI Lab이 제안한 모델과 여타 다른 기본 언어 모델<sup>vanilla language model</sup>과의 혼동을 줄이고자, 카카오 i 번역 엔진에 사용된 transformer 구조에 기반한 모델을 '기존 모델'이라고 지칭했습니다.</p>

<p><br /></p>

<p><img src="/assets/img/2020-05-07-200507/001.png" width="" align="" /></p>

<p><em class="center">[ 그림 1 ] 대규모 말뭉치를 사전학습한 번역 모델과 기존 모델과 성능을 비교하는 과정을 도식화한 이미지</em></p>

<p><br /></p>

<p class="dot-line">∙  ∙  ∙  ∙  ∙  ∙  ∙</p>

<p><br /></p>

<h1 id="1-사전학습-모델-선택하기">1. 사전학습 모델 선택하기</h1>

<table>
<tbody>
<tr class="key">
  <td style="width:15%;">발표 날짜</td>
  <td style="width:10%;">언어 모델</td>
  <td style="width:40%;">발표 기관</td>
</tr>
<tr>
  <td>2018년 10월</td>
  <td>BERT</td>
  <td class="left">구글(Google)</td>
</tr>
<tr>
  <td>2019년 1월</td>
  <td>XLM</td>
  <td class="left">FAIR(Facebook AI Research)</td>
</tr>
<tr>
  <td>2019년 2월</td>
  <td>GPT-2</td>
  <td class="left">오픈AI(OpenAI)</td>
</tr>
<tr>
  <td>2019년 5월</td>
  <td>MASS</td>
  <td class="left">마이크로소프트 리서치(Microsoft Research)</td>
</tr>
<tr>
  <td>2020년 1월</td>
  <td>mBART</td>
  <td class="left">FAIR</td>
</tr>
</tbody>
</table>
<p><em class="center">[표 1] AI Lab이 영어→한국어 번역 과제 수행을 위해 검토한 대표적인 5가지 언어 모델</em></p>

<p>AI Lab은 전 세계 많은 연구자가 기준으로 삼는 대표적인 언어 모델인 <a href="https://arxiv.org/abs/1810.04805">BERT</a>와 <a href="https://arxiv.org/abs/1901.07291">XLM</a>, <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">GPT-2</a>, <a href="https://arxiv.org/pdf/1905.02450.pdf">MASS</a>, <a href="https://arxiv.org/abs/2001.08210">mBART</a>를 다음과 같은 기준으로 평가했습니다.</p>

<p>첫번째, 사용하는 문자<sup>character</sup> 또는 어족<sup>language family</sup><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>이 다른 언어간 번역 태스크에서의 성능을 비교했습니다. 학계에 보고되는 대부분의 번역 모델 태스크는 인도유럽어족<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>에 해당하는 언어 간 번역입니다. 인도유럽어족의 하나인 영어와 독일어 예문([그림 2])을 보면, 문장 구성 방식이나 사용 문자가 서로 비슷하다는 점을 볼 수 있습니다. 이처럼 한 어파에 해당되는 언어는 서로 유사한 언어적 특성을 갖추고 있어 번역 난이도가 상대적으로 더 수월하죠. 반면, 독자적인 문자인 한글을 사용하는 한국어는 일본어나 몽골어를 제외하고는 어족이 같은 언어가 거의 없습니다. 주 사용자층이 한국인인 카카오 i 번역 엔진에서는 한국어에서 다른 이질적인 언어 간 번역 성능에 높은 가중치를 둬야한다고 판단했습니다.</p>

<p><img src="/assets/img/2020-05-07-200507/002.png" width="" align="" /></p>

<p><em class="center">[ 그림 2 ] 같은 어파에 속하는 영어와 독일어 예문</em></p>

<p>두번째, transformer<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup> 모델 구조를 최대한 유지할 수 있는지를 점검했습니다. transformer에 기반한 기존 모델과 그 구조가 비슷해야 모델 간 성능을 보다 객관적으로 비교해볼 수 있다고 판단했습니다. 번역 모델의 배포와 서비스 적용 등을 고려했을 때, 코드 재사용성과 효율성 면에서 유리하다는 점도 이런 판단에 영향을 미쳤습니다.</p>

<p>세번째, 문장 단위 번역에 적합한지를 살펴봤습니다. 번역 모델은 데이터 입력 단위 단위인 문장 또는 문서에 각각 최적화돼 있습니다. 달리 말하면, 문장 단위 번역에 집중한 모델에 문서 단위 데이터를 입력하면 자연스러운 번역 결과를 얻지 못합니다. 카카오 i 번역 엔진에 문장 단위 데이터가 주로 입력되는 패턴에 맞춰 기존 모델은 문장 단위 번역에 최적화돼 있습니다. 이런 이유로 연구팀이 채택할 모델이 문장 단위 번역에서 우수한 성능을 내는지 비교했습니다.</p>

<p>GPT-2로는 다국어 학습을 시도한 연구가 거의 없었을 뿐만 아니라, 기존 모델과 구조가 달라 이를 서비스에 적용하기가 쉽지 않았습니다. 여러 언어를 한 모델로 훈련할 수가 없는 BERT 또한 번역 태스크에는 적합하지 않았습니다. BERT의 다국어 버전인 XLM은 인코더<sup>encoder</sup><sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>만 사용했습니다. 디코더<sup>decoder</sup><sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup> 구조까지 갖춘 기존 모델과는 구조가 상이한 XLM으로는 직접적인 성능 비교가 어려웠습니다. 가장 최근에 나온 문서 단위 번역모델인 mBART으로는 문장 단위 번역을 제공하는 우리 서비스에 적절하지 않았습니다. MASS는 transformer의 인코더와 디코더에 기반한 모델로, 문장 단위 번역 태스크에 최적화돼 있습니다. 아울러 영어→중국어처럼 <a href="https://paperswithcode.com/task/machine-translation">이질적인 언어 번역 태스크에서 좋은 성능</a>을 냈습니다. AI Lab은 MASS가 위 세 가지 기준을 만족하는 가장 합리적인 모델이라 최종 판단했습니다.</p>

<p><br /></p>

<h1 id="2-실험-설계-추가하기">2. 실험 설계 추가하기</h1>

<p>논문에서 설명한 모델과 실제 공식 구현체 간의 구현방식에 두 가지 차이가 있었습니다. 논문에서는 단일언어 말뭉치로만 사전학습을 진행하는 반면, 마이크로소프트 리서치가 이를 실제 개발한 <a href="https://github.com/microsoft/MASS">공식 구현체</a>에서는 [그림 3]처럼 두 언어 간 병렬 말뭉치를 이용한 사전학습도 함께 진행했습니다. 언어 정보를 나타내는 임베딩<sup>language embedding</sup>을 둔 일체형 인코더와 디코더 구조를 제안한 논문과는 달리, MASS 공식 구현체에서는 언어별로 인코더와 디코더를 두었습니다. 이에 AI Lab은 MASS 공식 구현체에서처럼 단일언어 말뭉치와 다중언어 말뭉치를 모두 활용해 사전학습을 진행했습니다. 아울러 언어별 인코더와 디코더를 만들어 실험을 진행키로 결정했습니다.</p>

<p><img src="/assets/img/2020-05-07-200507/003.png" width="" align="" /></p>

<p><em class="center">[ 그림 3 ] MASS 공식 구현체가 사전학습을 진행하는 방식</em></p>

<p><br /></p>

<h1 id="3-데이터-수집하기">3. 데이터 수집하기</h1>

<p>AI Lab은 기존 보유한 &lt;영어 문장, 한국어 문장&gt;으로 구성된 다중언어 말뭉치 외에도, 한국어 또는 영어 문장으로만 구성된 단일언어 말뭉치를 추가로 수집했습니다. 오픈소스 형태로 공개된 데이터셋과 카카오엔터프라이즈가 보유한 ‘백과사전’과 ‘뉴스’, ‘자막’ 데이터셋을 합쳐 기존 모델에 사용된 훈련 데이터셋보다 4배가량 더 큰 규모의 단일언어 말뭉치를 구축했습니다.</p>

<p>‘백과사전’ 예문에서는 여러 분야에 걸쳐 일반적인 지식을 습득하기가 수월합니다. 다만 위키피디아 한국어 예문은 영어에 비해 상대적으로 그 수가 적습니다. 이 숫자의 균형을 맞추고자 AI Lab은 카카오엔터프라이즈가 보유한 백과사전 예문도 추가했습니다. ‘뉴스’는 우리가 실생활에서 접하는 용어와 지식을 획득하기가 좋아 큰 비중을 두고 수집했습니다. ‘자막’에서는 구어체 표현과 관용구 등을 배우기가 좋습니다. AI Lab은 카카오엔터프라이즈가 보유한 것 중 번역 태스크에 사용되지 않은 데이터만 활용했습니다.</p>

<table>
<tr class="key">
  <td width="25%">사전학습 데이터</td>
  <td width="35%">외부</td>
  <td width="35%">내부</td>
</tr>
<tr>
<td>백과사전</td>
<td>위키피디아</td>
<td rowspan="3">카카오엔터프라이즈 내부 데이터</td>
</tr>
<tr>
<td>뉴스</td>
<td>WMT-News 크롤 데이터셋<sup id="a11">[11](#f11)</sup></td>
</tr>
<tr>
<td>자막</td>
<td>-</td>
</tr>
</table>

<p><em class="center">[ 표 2 ] AI Lab이 사전학습에 사용한 단일언어 말뭉치 종류</em></p>

<p><br /></p>

<h1 id="4-사전학습-후-미세조정하기">4. 사전학습 후 미세조정하기</h1>

<p><a href="https://arxiv.org/pdf/2001.08210.pdf">BART 논문</a>에 따르면, 사전학습 횟수가 높을수록 성능이 좋습니다. 하지만 실제 실험에서는 사전학습 횟수가 250,000회를 넘어가면서부터는 높은 성능을 기대할 수 없었습니다. 미세조정<sup>fine-tuning</sup> 횟수 대비 사전학습 횟수의 비율(사전학습 횟수/미세조정 횟수)로 비교해본 결과, 100%에서부터 성능 향상이 일어남을 확인하였습니다. 이 결과를 토대로 AI Lab은 자체 수집한 데이터를 가지고 160,000회의 사전학습을 진행했습니다.</p>

<p>이렇게 사전학습이 완료된 모델은 번역 태스크에 맞게 훈련됩니다(미세조정). 사전학습 단계에서 배운 내용을 잊어버리거나, 최악의 경우 사전학습을 하지 않은 모델보다 훨씬 낮은 번역 성능을 낼 수 있기에, 적절한 초매개변수<sup>hyperparameter</sup><sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup> 탐색이 이 단계의 핵심입니다. AI Lab은 훈련 데이터 규모를 1/20로 축소해 항목별 실험을 빠르게 진행했습니다.</p>

<h2 id="1-배치-크기">(1) 배치 크기</h2>

<p><img src="/assets/img/2020-05-07-200507/004.png" width="" align="" /></p>

<p><em class="center">[ 그림 4 ] 체코-&gt;영어 번역 태스크에서 배치 크기에 따른 Transformer 모델 성능 비교 (출처 : <a href="https://arxiv.org/pdf/2001.08210.pdf">논문</a>)</em></p>

<p><a href="https://arxiv.org/pdf/1804.00247.pdf">프라하 카렐 대학교(Charles University in Prague)의 연구</a>에 따르면, 배치 크기<sup>batch size</sup>가 클수록 언어 모델의 번역 성능이 좋습니다([[그림 4]]. 하지만 실제 실험에서는 사전학습에 적용된 큰 배치 크기를 그대로 사용하면 미세조정 초반에 손실<sup>loss</sup>이 폭발합니다. 배치 크기가 지나치게 큰 상황에서는 모델 최적화<sup>optimization</sup><sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup>와 일반화<sup>generalization</sup><sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>가 어렵기 때문입니다.</p>

<p>이런 이유로 AI Lab은 미세조정에 적절한 배치 크기를 선택하는 실험을 진행했습니다([그림 5]). 사전학습 때 사용한 배치 크기의 1배(빨간색)에서는 그래프가 0으로 수렴하지 않고 되려 폭증했습니다. 배치 크기가 지나치게 크다고 판단한 AI Lab은 이를 줄여가면서 모델의 성능이 가장 좋은 지점을 탐색하는 데 집중했습니다. 그 결과, 사전학습 때 사용한 배치 크기의 2/3배(파란색)<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>에서 가장 좋은 성능을 확인할 수 있었습니다. 양기창 개발자는 “배치 크기를 1/2배(분홍색)로도 실험을 진행했으나, 2/3배에서보다는 성능이 미세하게 낮았다”고 부연했습니다.</p>

<p><img src="/assets/img/2020-05-07-200507/005.png" width="" align="" /></p>

<p><em class="center">[ 그림 5 ] 배치 크기에 따른 번역 모델 성능 비교</em></p>

<h2 id="2-학습률">(2) 학습률</h2>

<p>학습률<sup>learning rate</sup>은 학습 한 번에 가중치를 얼마나 갱신할지를 정하는 매개변수입니다. 적절한 학습률 탐색은 딥러닝 모델 훈련에서 중요합니다. 학습률이 지나치게 낮으면 최종 성능에 이르는 데 걸리는 학습 시간이 굉장히 오래 걸릴 뿐만 아니라 최소값<sup>global minimum</sup><sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">11</a></sup>이 아닌 극소값<sup>local minimum</sup><sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">12</a></sup>에 빠지는 문제가 발생합니다. 반면, 학습률이 지나치게 높으면 입력층으로 갈수록 전파되는 오차가 되려 폭증하는 현상으로 인해 최소값을 지나칠 수 있습니다.</p>

<p>앞 사례처럼 학습 초반에 손실이 올라가는 현상을 방지하고자 AI Lab은 배치 크기에 따른 적절한 학습률을 찾는 실험도 진행했습니다. 그 결과, 사전학습에서의 학습률 대비 1/2배(하늘색)와 1배(파란색) 간 뚜렷한 성능 차이가 없었습니다([그림 6]). 이에 AI Lab은 학습률을 0.2 그대로 유지했습니다.</p>

<p><img src="/assets/img/2020-05-07-200507/006.png" width="" align="" /></p>

<p><em class="center">[ 그림 6 ] 학습률에 따른 번역 모델 성능 비교</em></p>

<h2 id="3-워밍업-횟수">(3) 워밍업 횟수</h2>

<p>워밍업<sup>warmup steps</sup>은 학습 초반에는 학습에 큰 영향을 주지 않을 정도로 작은 학습률을 사용하고, 그 이후에는 기울기 방향을 신뢰할 수준에 이를 때까지 학습률을 서서히 키우는 방식을 가리킵니다. 실제로 <a href="https://arxiv.org/pdf/1706.02677.pdf">페이스북(Facebook)</a>은 (큰 배치를 이용한) 학습 초반에는 학습에 일정한 영향을 주지 않을 정도로 작은 학습률을 사용하고 이를 서서히 키웠습니다. 학습에 일정한 영향을 미치는 수준에 이른 이후에는 배치 크기를 키운 만큼 학습률을 키웠습니다. 그 결과, 큰 배치 크기에서도 모델의 일반화 성능을 유지하는 데 성공했습니다.</p>

<p>AI Lab 또한 이와 비슷하게 사전학습 설정값대로 학습률을 0에서 0.2까지 8,000회에 나눠서 천천히 키웠습니다. 하지만 미세조정 초반에 손실이 올라가는 현상이 발생했습니다. 워밍업 횟수가 작아서 생긴 현상이라고 본 AI Lab은 워밍업 횟수를 2배로 늘려 학습률을 천천히 예열하면 성능 개선이 이뤄질 거라 가정했습니다. 그러나 학습률 워밍업 횟수가 8,000회(하늘색)일 때와 16,000회(초록색)에서의 오차는 큰 차이가 없었습니다([그림 7]). AI Lab은 사전학습 때처럼 학습률 워밍업 횟수를 8,000회로 설정했습니다.</p>

<p><img src="/assets/img/2020-05-07-200507/007.png" width="51%" align="" /></p>

<p><em class="center">[ 그림 7 ] 학습률 워밍업 횟수에 따른 번역 모델 성능 비교</em></p>

<p><br /></p>

<h1 id="5-성능-평가하기">5. 성능 평가하기</h1>

<p>일반적으로 번역 모델의 성능 평가 척도로는 주로 <a href="https://www.korean.go.kr/nkview/nklife/2017_4/27_0403.pdf">BLEU</a><sup>bilingual evaluation understudy</sup>를 활용합니다. IBM에서 개발한 BLEU는 기계가 번역한 문장(source)과 인간이 실제로 번역한 정답 문장(reference) 간의 정확도를 측정합니다. 숫자가 클수록 번역된 문장과 정답 문장과의 유사성이 높습니다. 물론 BLEU가 높더라도 사람의 번역 품질보다 훨씬 떨어질 수는 있습니다. 그럼에도 언어에 구애받지 않고 계산 속도가 빠르며 객관적인 평가법이라는 점에서 그 공신력을 인정받고 있습니다.</p>

<p>하지만 AI Lab은 BLEU로 기존 모델과 자체 개발한 모델의 성능을 직접 비교하기에는 무리가 있다고 판단했습니다. BLEU에서는 기계가 번역한 문장이 ‘나는 케빈입니다’로 동일하더라도, ‘나/는/ /케빈/ /입니/다’ 또는 ‘나는/ /케빈 입니다’처럼 문장을 분절하는 단위가 달라지면 점수 산정 방식이 달라집니다. 서로 다른 번역 모델이 동일한 토크나이저를 이용하더라도 토크나이저 훈련에 사용한 말뭉치 종류가 다르면 분절 단위가 바뀌기 때문입니다.</p>

<p>차선책으로 고려한 성능 평가 척도는 아마존웹서비스(AWS)가 지난 <a href="https://arxiv.org/abs/1804.08771">2018년 ACL 학회에 제출한 논문</a>에서 처음 제안한 sacre BLEU([그림 8])입니다. 논문에서는 통계적으로 추출한 서브워드<sup>subword</sup><sup id="fnref:13" role="doc-noteref"><a href="#fn:13" class="footnote" rel="footnote">13</a></sup> 기반으로 문장을 분절하는 최신 알고리즘인 BPE<sup>Byte-Pair Encoding</sup>를 사용합니다. 이렇게 하면 번역 모델과 모델 학습에 사용한 말뭉치 종류에 관계없이 기계가 번역한 문장을 동일한 단위로 분절할 수 있습니다. 보다 객관적인 척도라는 점을 인정 받은 sacre BLEU는 최근 여러 논문에서 BLEU에 함께 번역 모델의 성능을 평가하는 기준으로 널리 사용되고 있습니다.</p>

<p><img src="/assets/img/2020-05-07-200507/008.png" width="47%" align="" /></p>

<p><em class="center">[ 그림 8 ] sacre BLEU가 점수를 내는 과정 (출처 : <a href="https://arxiv.org/abs/1804.08771">논문</a>)</em></p>

<p>sacre BLEU를 평가 척도로 내세워 영어→한국어 번역 태스크에서 기존 모델과 AI Lab이 개발한 모델의 성능을 비교해본 결과([표 3]), 단일언어 말뭉치와 다중언어 말뭉치 모두를 사전학습 번역 모델이 기존 모델보다 약 5%P 더 나은 성능을 냄을 확인할 수 있었습니다. 반면, 단일언어 말뭉치 또는 다중언어 말뭉치만을 사용하면 되려 모델의 성능이 저하됐습니다. 이에 대해 양기창 개발자는 “사전학습에 사용한 말뭉치가 미세조정 단계에서 유의미한 영향을 미칠수록, 그리고 잡음<sup>noise</sup>를 상쇄할만한 방대한 데이터를 모을수록 모델의 성능 향상에 영향을 미침을 알 수 있었다”고 분석했습니다.</p>

<table>
<tr class="key">
  <td>모델</td>
  <td>사전학습에 사용한 말뭉치 종류</td>
  <td>sacre BLEU</td>
</tr>
<tr>
  <td class="key">기존 모델</td>
  <td>X</td>
  <td>0</td>
</tr>
<tr>
  <td class="key" rowspan="3">AI Lab이 제안한 모델</td>
  <td>단일언어 말뭉치((한국어), (영어))</td>
  <td>-1.358</td>
</tr>
  <tr>
  <td>다중언어 말뭉치((영어, 한국어)), Masked LM</td>
  <td>-5.30</td>
</tr>
<tr>
  <td>단일언어 말뭉치 + 다중언어 말뭉치, Masked LM</td>
  <td>+1.481</td>
</tr>
</table>

<p><em class="center">[표 3] 기존 모델과 AI Lab이 제안한 모델 성능 비교, Masked LM</em></p>

<p><br /></p>

<h1 id="향후-계획">향후 계획</h1>

<p>AI Lab은 정량적인 평가 이외에도 전문 번역가를 통해 모델을 정성적으로 평가하는 등 적절한 평가 방법론을 마련하는 추가 연구를 준비하고 있습니다. 영어→한국어 번역 태스크 외에도, 카카오의 주 사용자층인 한국인을 위해 한국어→영어, 한국어→중국어, 한국어→일본어 번역을 포함해 다양한 언어 간 번역 태스크에도 사전학습을 적용하는 실험을 진행할 계획입니다.</p>

<p><br /></p>

<hr />

<h3 id="각주">각주</h3>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>언어라는 현상을 모델링하고자 특정 단어 시퀀스(또는 문장)에 확률을 할당하는 모델 <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>같은 조상언어에서 갈라나온 거라 추정되는 여러 언어를 통틀어 일컫는 말 <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>유럽과 서아시아, 남아시아에 사는 사람이 사용하는 언어가 속하는 어족 <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>컨볼루션<sup>convolution</sup>이나 순환<sup>recurrence</sup> 기법 대신, 모든 단어가 현재 결과에 기여하는 정도를 반영할 수 있도록 각 입력 단어가 출력 상태에 연결하는 어텐션<sup>attention</sup> 신경망 구조를 활용한 seq2seq 모델이다. 어텐션을 이용해 거리가 먼 단어 간의 관계를 효과적으로 표현이 가능한 덕분에 학습 성능이 좋아졌다. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>입력된 토큰 시퀀스의 언어적 특성을 특징 벡터<sup>feature vector</sup>로 표현하는 부분 <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>특징 벡터를 이용해 새로운 토큰 시퀀스를 생성하는 부분 <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>개발자가 머신러닝 알고리즘에서 직접 조정하는 값. 필터 수나 모델의 층 수, 필터 크기 등이 여기에 해당한다. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>모든 학습 데이터에 대해 오차를 최소화하는 가중치 값을 찾은 상태 <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>학습 데이터와 조금이라도 다른 성격의 데이터가 입력되어도 모델이 제대로 동작하는 상태 <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>12,288토큰/GPU <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p>모든 점이 가지는 함수값 이하의 함수값 <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:12" role="doc-endnote">
      <p>주위의 모든 점이 가지는 함수값 이하의 함수값 <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:13" role="doc-endnote">
      <p>단어를 통계적인 관점에서 나눈 가장 작은 단위 <a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>samantha:작성,편집</name></author><category term="deepdive" /><category term="pre-training" /><category term="translation" /><summary type="html"><![CDATA[카카오가 보유한 대규모 말뭉치를 사전학습한 번역 모델은 기존 카카오 i 번역 엔진에 사용된 모델과 비교했을 때 더 나은 성능을 냈습니다. 이 글에서는 바로 이 사전학습된 번역 모델을 만드는 과정과 실험 결과에 대한 내용을 담았습니다.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://genius0928.github.io//assets/img/2020-05-07-200507/000.jpg" /><media:content medium="image" url="https://genius0928.github.io//assets/img/2020-05-07-200507/000.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>